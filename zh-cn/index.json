[{"categories":[],"content":"SimpleTrack: Understanding and Rethinking 3D Multi-object Tracking SimpleTrack：理解和重新思考 3D 多目标跟踪 近年来，3D 多目标跟踪 (MOT) 见证了许多新颖的基准和方法，尤其是那些在“检测跟踪”范式下的基准和方法。 尽管它们取得了进步和实用性，但尚未对其优缺点进行深入分析。在本文中，我们通过将当前的 3D MOT 方法分解为四个组成部分，将它们总结为一个统一的框架：检测预处理、关联、运动模型和生命周期管理。然后，我们将现有算法的失败案例归因于每个组件，并对其进行详细调查。基于分析，我们提出了相应的改进，从而形成了一个强大而简单的基线：SimpleTrack。 Waymo Open Dataset 和 nuScenes 的综合实验结果表明，我们的最终方法只需稍作修改即可获得最新的最新结果。 此外，我们采取了额外的步骤并重新思考当前的基准是否真实地反映了算法应对现实挑战的能力。我们深入研究了现有基准测试的细节并发现了一些有趣的事实。最后，我们分析了 SimpleTrack 中剩余故障的分布和原因，并提出了 3D MOT 的未来发展方向。我们的代码可在 https://github.com/TuSimple/SimpleTrack 获得 ","date":"2023-05-08","objectID":"/zh-cn/mot/:0:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"1.Introduction 多目标跟踪 (MOT) 是计算机视觉中的一项复合任务，结合了定位和识别两个方面。鉴于其复杂性，MOT 系统通常涉及许多相互关联的部分，例如检测的选择、数据关联、对象运动的建模等。这些模块中的每一个都有其特殊的处理，并且可以显着影响整个系统的性能.因此，我们想问一下3D MOT中哪些组件起着最重要的作用，我们如何改进它们？ 带着这样的目标，我们重新审视了当前的 3D MOT 算法 [3、10、28、37、43、44]。这些方法大多采用**“检测跟踪”范式**，它们直接从 3D 检测器中获取边界框并跨帧构建轨迹。我们首先将它们分解为四个单独的模块并检查每个模块：**输入检测的预处理、运动模型、关联和生命周期管理**。基于此模块化框架，我们将3D MOT的故障案例定位并归因于相应的组件，并发现了先前设计中被忽视的几个问题。 首先，我们发现不准确的输入检测可能会污染关联。然而，纯粹按分数阈值修剪它们会牺牲召回率。其次，我们发现需要仔细设计定义在两个 3D 边界框之间的相似性度量。基于距离和简单的 IoU 都效果不佳。第三，3D 空间中的对象运动比 2D 图像空间中的对象运动更可预测。因此，运动模型预测甚至不良观察（低分检测）之间的一致性可以很好地表明物体的存在。根据这些观察结果，我们提出了几个简单但重要的解决方案。对 Waymo Open Dataset [34] 和 nuScenes [8] 的评估表明，我们的最终方法“SimpleTrack”在 3D MOT 算法中具有竞争力（在表 6 和表 7 中）。 除了分析 3D MOT 算法外，我们还反思当前的基准。我们强调在评估中需要高频检测和正确处理输出轨迹。为了更好地理解我们方法的上限，我们根据 ID 开关和 MOTA 指标进一步分解剩余的错误。我们相信这些观察可以激发算法和基准的更好设计。 简而言之，我们的贡献如下： 我们分解了“检测跟踪”3D MOT 框架的管道，并分析了每个组件与故障案例之间的联系。 我们为每个模块提出相应的处理方法，并将它们组合成一个简单的基线。结果在 Waymo 开放数据集上具有竞争力。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:1:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"2. Related Work 由于检测器的强大功能，大多数 3D MOT 方法 [3、10、28、37、43、44] 采用“检测跟踪”框架。我们首先总结了具有代表性的 3D MOT 工作，然后强调了 3D 和 2D MOT 之间的联系和区别。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:2:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"2.1 3D MOT 许多 3D MOT 方法由手工制作的基于规则的组件组成。 AB3DMOT [37] 是使用 IoU 进行关联和使用卡尔曼滤波器作为运动模型的共同基线。其著名的追随者主要改进关联部分：Chiu 等人 [10] 和 CenterPoint [43] 用 Mahalanobis 和 L2 距离替换 IoU，这在 nuScenes [8] 上表现更好。其他一些人注意到生命周期管理的重要性，其中 CBMOT [3] 提出了一种基于分数的方法来取代“基于计数”的机制，Poschmann ¨ 等人 [28] 将 3D MOT 视为因子图上的优化问题。尽管这些改进非常有效，但仍非常需要对 3D MOT 方法进行系统研究，尤其是这些设计受到影响的地方以及如何进行进一步改进。为此，我们的论文力求达到预期。 与上述方法不同，许多其他方法试图以更少的手动设计来解决 3D MOT。 [2,9,15,38] 利用 RGB 图像的丰富特征进行关联和生命周期控制，Chiu 等人 [9]特别使用神经网络来处理特征融合、关联度量和轨迹初始化。最近，OGR3MOT [44] 遵循 Guillem 等人 [7] 并以端到端的方式使用图神经网络 (GNN) 解决 3D MOT，特别关注数据关联和活动轨迹的分类。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:2:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"2.2 2D MOT 2D MOT 与 3D MOT 共享数据关联的共同目标。一些值得注意的尝试包括概率方法 [1、16、30、32]、动态规划 [11]、二分匹配 [6]、最小成本流 [4、46]、凸优化 [27、35、36、45]、和条件随机场 [42]。 随着深度学习的快速发展，许多方法 [7, 12–14, 19, 40] 学习匹配机制，而其他方法 [17, 20, 21, 24, 26] 学习关联度量。 与 3D MOT 类似，许多 2D 跟踪器 [5,22,33,48] 也受益于增强的检测质量并采用“检测跟踪”范式。然而，由于尺度变化，RGB 图像上的对象具有不同的大小；因此，它们对于关联和运动模型来说更难。但是 2D MOT 可以轻松利用丰富的 RGB 信息并使用外观模型 [18、19、33、39]，这是基于 LiDAR 的 3D MOT 所不具备的。总之，MOT 方法的设计应适合每种模态的特征。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:2:2","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"3. 3D MOT Pipeline 在本节中，我们将 3D MOT 方法分解为以下四个部分。图中有一个例子。 **输入检测的预处理：**它预处理来自检测器的边界框并选择用于跟踪的边界框。一些范例操作包括选择分数高于某个阈值的边界框。 （在“预处理”图 1 中，一些多余的边界框被删除。） **运动模型：**它预测和更新对象的状态。大多数 3D MOT 方法 [3、10、37] 直接使用卡尔曼滤波器，而 CenterPoint [43] 使用检测器从多帧数据预测的速度。 （在“预测”和“运动模型更新”图 1 中。） **关联：**它将检测与轨迹相关联。关联模块包括两个步骤：相似度计算和匹配。相似度测量一对检测和轨迹之间的距离，而匹配步骤根据预先计算的相似度解决对应关系。 AB3DMOT [37] 提出使用匈牙利算法的 IoU 的基线，而 Chiu 等人 [10] 使用 Mahalanobis 距离和贪婪算法，CenterPoint [43] 采用 L2 距离。 （在“关联”图 1 中。） **生命周期管理：**它控制着“出生”、“死亡”和“输出”政策。 “Birth”决定一个检测边界框是否会被初始化为一个新的tracklet； “死亡”在认为已移出注意区域时移除轨迹； “Output”决定了一个tracklet是否会输出它的状态。大多数 MOT 算法采用简单的基于计数的规则 [10,37,43]，CBMOT [3] 通过修改 tracklet 置信度的逻辑来改进 birth and death。 （在“生命周期管理”图 1 中。） ","date":"2023-05-08","objectID":"/zh-cn/mot/:3:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4. Analyzing and Improving 3D MOT 在本节中，我们将分析和改进 3D MOT 流水线中的每个模块。为了更好地说明，我们通过将每个修改的影响从 SimpleTrack 的最终变体中删除来消除它。默认情况下，消融都在使用 CenterPoint [43] 检测的验证拆分上。我们还在4.5节提供了附加消融分析以及与第 1 节中其他方法的比较。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:4:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4.1 预处理 为了满足召回要求，当前的检测器通常输出大量边界框，其分数粗略地表明它们的质量。然而，如果这些框在 3D MOT 的关联步骤中被平等对待，质量低或重叠严重的边界框可能会偏离跟踪器以选择不准确的检测来扩展或形成轨迹（如图 2的“原始检测”所示）。检测和 MOT 任务之间的这种差距需要仔细处理。 图2.分数过滤和NMS的比较。要去除第 2 行的冗余边界框，分数过滤至少需要 0.24 的阈值，但这会消除第 1 行的检测。但是，NMS 可以通过去除第 2 行的重叠并保持第 1 行的召回率来很好地满足两者. 3D MOT 方法通常使用置信度分数来过滤掉低质量检测并提高 MOT 输入边界框的精度。然而，这种方法可能不利于召回，因为它们直接放弃了观察不佳的对象（图 2 中的第一行）。它对像 AMOTA 这样的指标也特别有害，它需要跟踪器使用低分边界框来满足召回要求。 为了在不显着降低召回率的情况下提高精度，我们的解决方案简单直接：我们对输入检测应用更严格的非最大抑制 (NMS)。如图 2 右侧所示，单独的 NMS 操作可以有效地消除重叠的低质量边界框，同时保持多样化的低质量观察，即使在稀疏点或遮挡等区域也是如此。因此，通过在预处理模块中加入NMS，我们可以大致保持召回率，但大大提高了精度和好处MOT 在 WOD 上，我们更严格的 NMS 操作移除了 51% 和 52% 的车辆和行人边界框，精度几乎翻了一番：车辆从 10.8% 到 21.1%，行人从 5.1% 到 9.9%。与此同时，车辆召回率从 78% 降至 74%，行人召回率从 83% 降至 79%。根据表 1 和表2，这在很大程度上有利于性能，尤其在行人（表 2 的右侧），目标检测任务更难。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:4:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4.2 运动模型 运动模型描述了轨迹的运动状态。它们主要用于预测下一帧中对象的候选状态，这些状态是后续关联步骤的建议。此外，像卡尔曼滤波器这样的运动模型也可以潜在地细化物体的状态。通常，3D MOT 有两种常用的运动模型：卡尔曼滤波器 (KF)，例如AB3DMOT [37]，以及具有来自检测器的预测速度的恒速模型（CV），例如中心点 [43]。KF 的优点是它可以利用来自多个帧的信息，并在面对低质量检测时提供更平滑的结果。同时，CV 通过其明确的速度预测更好地处理突然和不可预测的运动，但其在运动平滑方面的效果有限。在表 3 和表 4，我们在WOD和nuScenes上比较了他们两个，这为我们的说法提供了明确的证据。 一般来说，这两个运动模型表现出相似的性能。在 nuScenes 上，CV 略胜 KF，而在 WOD 上则相反。 KF 在 WOD 上的优势主要来自于对边界框的细化。为了验证这一点，我们实现了“KF-PD”变体，它仅使用 KF 来提供关联之前的运动预测，并且输出都是原始检测。最终，Tab 中“CV”和“KF-PD”之间的边际差距。 表3 支持我们的主张。在 nuScenes 上，由于 nuScenes (2Hz) 的帧速率较低，CV 运动模型稍微好一些。为了证明我们的猜想，我们在 nuScenes 1 上应用了更高频率 10Hz 设置下的 KF 和 CV，这次 KF 在 AMOTA 中略胜 CV 0.696 和 0.693。 总而言之，由于更可预测的运动，卡尔曼滤波器更适合高频率情况，而恒速模型对于具有显式速度预测的低频场景更稳健。由于推断速度对于检测器而言还不常见，因此我们在不失一般性的情况下为 SimpleTrack 采用卡尔曼滤波器。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:4:2","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4.3 关联 4.3.1 关联度量：3D GIoU 基于 IoU [37] 和基于距离的 [10,43] 关联度量是 3D MOT 中的两个普遍选择。如图 3 所示，它们具有典型但不同的故障模式。 IoU 计算边界框之间的重叠率，因此如果检测和运动预测之间的 IoU 全部为零，则它无法连接检测和运动预测，这在 tracklets 的开始或具有突然运动的物体上很常见（图 3 的左侧）。基于距离的度量的代表是 Mahalanobis [10] 和 L2 [43] 距离。使用较大的距离阈值，它们可以处理基于 IoU 的度量的失败情况，但它们可能对低质量的附近检测不够敏感。我们在图 3 的右侧解释了这种情况。在第 k 帧上，蓝色运动预测与绿色误报检测的 L2 距离较小，因此它被错误关联。通过这样的例子，我们得出结论，基于距离的度量缺乏方向区分，这正是基于 IOU 的度量的优势。 图3.关联指标的说明。左图：IoU 与 GIoU。右图：L2 距离与 GIoU。详情在4.3.1节. 为了充分利用两个体系，我们建议将“广义 IoU”（GIoU）[31] 推广到 3D 以进行关联。简而言之，对于任何一对 3D 边界框 B1、B2，它们的 3D GIoU 如下式（1）所示。其中I，U是B1和B2的交集和并集。C 是 U 的封闭凸包。 V 表示多边形的体积。我们将 GIoU \u003e -0.5 设置为每个类别对象的阈值在WOD和nuScenes上为这对关联进入后续的匹配步骤。 $$ \\begin{aligned} V_U \u0026 =V_{B_1}+V_{B_2}-V_I \\ \\operatorname{GIoU}\\left(B_1, B_2\\right) \u0026 =V_I / V_U-\\left(V_C-V_U\\right) / V_C . \\end{aligned} $$ 如图 3 所示，GIoU 指标可以处理两种故障模式。图 4 中的定量结果还显示了 GIoU 改善 WOD 和 nuScenes 关联的能力。 图4.WOD（左和中）和 nuScenes（右）的关联指标比较。 “M-Dis”是马氏距离的缩写。最佳方法最靠近右下角，具有最低的 ID 开关和最高的 MOTA/AMOTA。 一般来说，检测和轨迹之间的匹配有两种方法：1）将问题表述为二分匹配问题，然后使用匈牙利算法[37]求解。 2) 通过贪心算法迭代关联最近的对[10, 43]。 我们发现这两种方法与关联度量严重耦合：基于 IoU 的度量对两者都很好，而基于距离的度量更喜欢贪心算法。我们假设原因是基于距离的度量范围很大，因此优化全局最优解的方法，如匈牙利算法，可能会受到异常值的不利影响。在图 5 中，我们对 WOD 上的匹配策略和关联度量之间的所有组合进行了实验。正如所证明的，IoU 和 GIoU 对于这两种策略都很好，而 Mahalanobis 和 L2 距离需要贪心算法，这也与之前工作的结论一致 [10]。 图5.WOD上的匹配策略比较。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:4:3","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4.4 生命周期管理 我们分析了 WOD2 上的所有 ID-Switch，并将它们分为两组，如图 6 所示：错误关联和提前终止。与许多工作的主要重点不同，即关联，我们发现提前终止实际上是 ID-Switch的主要原因：车辆为 95%，行人为 91%。在早期终止中，许多是由点云稀疏性和空间遮挡引起的。为了缓解这个问题，我们利用免费但有效的信息：运动模型和低分检测之间的共识。这些边界框通常定位质量较低，但如果它们与运动预测一致，则它们是对象存在的有力指示。然后我们使用这些来延长 tracklet 的寿命。 图6.两种主要类型 ID-Switches的图示。 怀着这样的动机，我们提出了“两级关联”。具体来说，我们应用两轮具有不同分数阈值的关联：一个低 Tl 和一个高 Th（例如，WOD 上的行人为 0.1 和 0.5）。在第一阶段，我们使用与大多数当前算法 [10、37、43] 相同的过程：只有得分高于 Th 的边界框用于关联。在第二阶段，我们专注于与第一阶段检测不匹配的轨迹，并且放宽他们的匹配条件：具有大于 Tl 的分数的检测对于匹配来说是足够的。如果 tracklet 在第二阶段成功与一个边界框相关联，它仍将保持活动状态。然而，由于低分检测通常质量较差，我们不输出它们以避免误报，它们也不用于更新运动模型。相反，我们使用运动预测作为最新的轨迹状态，取代低质量检测。 我们在图 7 中直观地解释了我们的“Twostage Association”和传统的“One-stage Association”之间的区别。假设 T = 0.5 是过滤检测边界框的原始分数阈值，那么跟踪器将忽略分数为 0.4 和0.2 在第 3 帧和第 4 帧上，由于连续帧中缺少匹配项而死亡，这最终会导致最终的 ID 切换。相比之下，我们的两阶段关联可以保持 tracklet 的活动状态。 图7.“单阶段”和“两阶段”关联与假设示例的比较。 “Extend”的意思是“延长生命周期”，“Predict”的意思是“由于没有关联而使用运动预测”。假设 Th = 0.5 和 Tl = 0.1 是分数阈值，“one-stage”方法会因为连续缺少关联而提前终止 tracklet。第4.4节的详细信息。 在表 5中，我们的方法在不伤害 MOTA 的情况下大大减少了 ID-Switches。这证明SimpleTrack通过更灵活地使用检测来有效地延长生命周期。与我们的工作并行，类似的方法也被证明对 2D MOT [47] 有用。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:4:4","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4.5 SimpleTrack 的集成 在本节中，我们将上述技术集成到统一的 SimpleTrack 中，并演示它们如何逐步提高性能。 在图 8 中，我们说明了 3D MOT 跟踪器的性能如何从基线提高。在WOD上，虽然车辆和行人的属性有很大不同，但每种技术都适用。在 nuScenes 上，每项改进建议都对 AMOTA 和 ID-Switch 都有效。 图8.SimpleTrack 在 WOD（左和中）和 nuScenes（右）上的改进。我们在 WOD 上使用 AB3DMOT [37]，在 nuScenes 上使用 Chiu 等人 [10] 的公共基线。对于 nuScenes，“10Hz-Two”（使用 10Hz 检测和两级关联）的改进在5.1节中。“Pred”（输出运动模型预测）在第5.2节。修改的名称位于 x 轴上。更好的 MOTA 和 ID-Switch 值在 y 轴上更高，以实现更清晰的可视化。 我们还报告了测试集的性能，并与其他 3D MOT 方法进行了比较。将我们的技术结合起来会产生新的最先进的结果（在表 6 和表 7 中）。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:4:5","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"5. Rethinking nuScenes 除了上述技术外，我们还深入研究了基准测试的设计。这些基准极大地促进了研究的发展并指导了算法的设计。对比WOD和nuScenes，我们有以下发现：1）nuScenes的帧率为2Hz，而WOD为10Hz。如此低的频率给 3D MOT（第5.1 节）增加了不必要的困难。 2）nuScenes的评估需要高召回率和低分数阈值。它还通过插值预处理轨迹，这鼓励跟踪器输出反映整个轨迹质量的置信度分数，而不是帧质量（第 5.2 节）。我们希望这两项发现能够激发社区重新思考 3D 跟踪的基准和评估协议。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:5:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"5.1 检测频率 跟踪通常受益于更高的帧速率，因为运动在短时间间隔内更容易预测。我们在表8中比较两个基准的点云、注释和常见 MOT 帧速率的频率。在 nuScenes 上，它有 20Hz 点云但只有 2Hz 注释。这导致大多数常见的检测器和 3D MOT 算法在 2Hz 下工作，即使它们实际上利用了所有 20Hz LiDAR 数据并且运行速度快于 2Hz。因此，我们研究高频数据的影响如下。 尽管高频 (HF) 帧的信息更丰富，但合并它们并非易事，因为 nuScenes 仅对低频帧进行评估，我们称之为“评估帧”。在表9中，简单地使用所有 10Hz 帧并不能提高性能。这是因为 HF 帧上的低质量检测可能会偏离跟踪器并损害采样评估帧的性能。为了克服这个问题，我们首先在 HF 帧上应用“单阶段关联”进行探索，其中仅考虑得分大于 Th = 0.5 的边界框并将其用于运动模型更新。然后，我们采用“双阶段关联”（在第 4.4 节中描述），使用得分大于 Tl = 0.1 的框来扩展轨迹。就像在表9中一样，我们的方法显着改善了 AMOTA 和 ID 开关。我们甚至尝试将帧速率提高到 20Hz，但由于偏差问题，这几乎无法带来进一步的改进。所以 SimpleTrack 在我们最终提交给测试集时使用了 10Hz 设置。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:5:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"5.2.轨迹插值 nuScenes 中使用的 AMOTA 指标计算不同召回阈值下的平均 MOTAR [37]，这需要跟踪器输出所有分数段的框。为了进一步提高召回率，我们在没有关联检测边界框的情况下输出帧和轨迹的运动模型预测，并根据经验为它们分配比任何其他检测更低的分数。在我们的例子中，它们的分数是 0.01 × SP，其中 SP 是前一帧中轨迹的置信度分数。如表10所示。这个简单的技巧提高了整体召回率和AMOTA。 然而，我们发现提高召回率并不是这种改进的唯一原因。除了边界框，运动模型预测的分数也做出了重要贡献。这从 nuScenes 上的评估协议开始，他们在其中插入输入轨迹以填充缺失的帧并使用其轨迹平均分数更改所有分数，如图 9 所示。在这种情况下，我们的方法可以明确地惩罚低-quality tracklets，通常包含更多由运动模型预测替换的缺失框。 总而言之，nuScenes 上的这种插值鼓励跟踪器整体处理轨迹质量并输出校准的质量感知分数。然而，即使对于相同的 tracklet，boxes的质量也可能在帧之间有很大差异，因此我们建议仅用一个分数来描述 tracklet 的质量是不完美的。此外，在这个插值步骤中还引入了未来信息，它改变了 tracklet 结果。这也可能引发人们对评估设置是否仍然是完全在线的担忧。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:5:2","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"6. Error Analyses 在本节中，我们对 SimpleTrack 剩余的失败案例进行分析，并提出改进“检测跟踪”范式的潜在未来方向。不失一般性，我们以WOD为例。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:6:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"6.1.上限实验设置 为了定量评估失败案例的原因，我们将 SimpleTrack 与两种不同的预言机变体进行了对比。结果总结在表11中。 GT Output 消除了由“输出”策略引起的错误。我们在“输出”阶段计算来自 SimpleTrack 的边界框与 GT 框之间的 IoU，然后使用 IoU 来决定是否应该输出一个框而不是检测分数。 GT All 是 CenterPoint 框跟踪性能的上限。我们贪婪地匹配从 CenterPoint 到 GT 框的检测，保持真阳性并为它们分配地面真实 ID。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:6:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"6.2. “检测跟踪”分析 **ID-Switches：**我们分解了ID-Switches的原因，如图 6 所示。尽管通过“两阶段关联”，车辆提前终止的比例大大降低了 86%，行人提前终止的比例降低了 70%，但它仍然占 88%，车辆和行人的 SimpleTrack 中其余ID-Switches的故障案例分别为 72%。我们检查这些案例并发现它们中的大多数是由于长期遮挡或暂时看不见的物体返回。因此，除了改进关联之外，未来的潜在工作还可以开发像 2D MOT [18、19、33、39] 中的外观模型，或者在它们返回后静默维护它们的状态以重新识别这些对象。 **FP and FN：**选项卡中的“GT All”。图 11 显示了使用 CenterPoint [43] 检测的 MOT 上限，我们以车辆类别为例进行分析。即使使用“GT All”，false negatives 仍然是 0.215，这是 detection FN，在“tracking by detection”框架下很难修复。比较“GT All”和SimpleTrack，我们发现跟踪算法本身引入了0.119个漏报。我们将它们进一步分解如下。具体来说，“GT Output”和“GT ALL”之间的差异表明 0.043 个假阴性是由 NMS 和预处理中的分数阈值产生的未初始化轨迹引起的。其他来自生命周期管理。 “Initialization”在输出tracklet之前需要两帧的积累，这与AB3DMOT [37]相同。这会产生边际 0.005 的假阴性。我们的“输出”逻辑使用检测分数来决定输出与否，占误报数 0.076。基于这些分析，我们可以得出结论，差距主要是由分数和检测质量之间的不一致造成的。通过使用历史信息，与单帧检测器相比，3D MOT 可能会提供更好的分数，这最近已经引起了一些关注 [3, 44]。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:6:2","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"7. Conclusions and Future Work 在本文中，我们将“检测跟踪”3D MOT 算法解耦为几个组件，并分析了它们的典型故障。有了这样的见解，我们提出了使用 NMS、GIoU 和两阶段关联的相应增强，这导致了我们的 SimpleTrack。此外，我们还重新考虑了 nuScenes 中的帧率和插值预处理。我们最终指出了“通过检测跟踪”3D MOT 的几个可能的未来方向。 然而，除了“检测跟踪”范式之外，还有一些具有巨大潜力的分支。为了获得更好的边界框质量，3D MOT 可以使用长期信息 [25、29、41] 对其进行细化，这被证明优于仅基于局部帧的检测。未来的工作还可以将当前基于手动规则的方法转化为基于学习，例如使用基于学习的帧内机制代替NMS，使用帧间推理代替3D GIoU和生命周期管理等。 **致谢：**我们要感谢 Tianwei Yin 在我们将 CenterPoint 检测应用于 3D 多目标跟踪过程中的热心帮助。 Center-based 3D Object Detection and Tracking 基于中心的 3D 对象检测和跟踪 **摘要：**三维对象通常表示为点云中的 3D 框。这种表示模仿了经过充分研究的基于图像的 2D 边界框检测，但带来了额外的挑战。 3D 世界中的对象不遵循任何特定方向，基于框的检测器难以枚举所有方向或将轴对齐的边界框拟合到旋转的对象。在本文中，我们建议将 3D 对象表示、检测和跟踪为点。我们的框架 CenterPoint 首先使用关键点检测器检测对象的中心，然后回归到其他属性，包括 3D 大小、3D 方向和速度。在第二阶段，它使用对象上的附加点特征来改进这些估计。在 CenterPoint 中，3D 对象跟踪简化为贪心最近点匹配。由此产生的检测和跟踪算法简单、高效且有效。 CenterPoint 在 nuScenes 基准测试中实现了 3D 检测和跟踪的一流性能，单个模型的 NDS 为 65.5，AMOTA 为 63.8。在 Waymo Open Dataset 上，CenterPoint 的表现大大优于之前所有的单一模型方法，在所有仅 Lidar-only 的提交中排名第一。代码和预训练模型可在 https://github.com/tianweiy/CenterPoint 获得。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:7:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"1. Introduction 强大的 3D 感知是许多最先进的驾驶系统的核心要素 [1, 48]。与研究充分的 2D 检测问题相比，点云上的 3D 检测提出了一系列有趣的挑战：首先，点云是稀疏的，3D 对象的大部分都没有测量 [22]。其次，生成的输出是一个三维框，通常不能很好地与任何全局坐标系对齐。第三，3D 物体的尺寸、形状和纵横比范围很广，例如，在交通领域，自行车接近平面，公共汽车和豪华轿车被拉长，行人很高。2D和3D检测之间的这些显著差异使得两个领域之间的思想转移变得更加困难[43,45,58]。轴对齐的 2D 框 [16, 17] 是自由形式 3D 对象的不良代理。一种解决方案可能是为每个对象方向 [56、57] 分类不同的模板（锚点），但这不必要地增加了计算负担，并可能引入大量潜在的误报检测。我们认为，连接 2D 和 3D 域的主要潜在挑战在于对象的这种表示。 在本文中，我们展示了如何将物体表示为点（图1）大大简化了3D识别。我们的两阶段3D检测器CenterPoint使用关键点检测器[62]查找对象的中心及其属性，第二阶段改进所有估计。==具体来说，CenterPoint 使用标准的基于激光雷达的主干网络，即 VoxelNet [54、64] 或 PointPillars [27]，来构建输入点云的表示。==然后它将这种表示扁平化为俯视图，并使用标准的基于图像的关键点检测器来查找对象中心 [62]。对于每个检测到的中心，它会从中心位置的点特征回归到所有其他对象属性，例如 3D 大小、方向和速度。此外，我们使用轻量级第二阶段来细化对象位置。==第二阶段在估计对象 3D 边界框的每个面的 3D 中心提取点特征。它恢复了由于跨步和有限的感受野而丢失的局部几何信息，并以较小的成本带来了不错的性能提升。== 基于中心的表示有几个关键优势：首先，与边界框不同，点没有固有方向。这极大地减少了对象检测器的搜索空间，并允许主干学习对象的旋转不变性和等价性。其次，基于中心的表示简化了下游任务，例如跟踪。如果对象是点，那么轨迹就是空间和时间中的路径。 CenterPoint 预测连续帧之间对象的相对偏移（速度）并贪婪地链接对象。第三，基于点的特征提取使我们能够设计一个有效的两阶段细化模块，它比以前的方法快得多 [42-44]。 我们在两个流行的大型数据集上测试我们的模型：Waymo Open [46] 和 nuScenes [6]。我们表明，从框表示到基于中心的表示的简单切换会在不同骨干网下的 3D 检测中产生 3-4 mAP 的增加 [27、54、64、65]。两阶段细化进一步带来了额外的 2 mAP 提升，计算开销很小 (\u003c 10%)。我们最好的单一模型在 Waymo 上实现了 71.8 和 66.4 的 2 级 mAPH 车辆和行人检测，在 nuScenes 上实现了 58.0 mAP 和 65.5 NDS，在这两个数据集上的性能优于所有已发布的方法。值得注意的是，在 NeurIPS 2020 nuScenes 3D 检测挑战中，CenterPoint 构成了前 4 名获奖作品中的 3 个的基础。对于 3D 跟踪，我们的模型在 nuScenes 上的性能为 63.8 AMOTA，比之前的最新技术高出 8.8 AMOTA。在 Waymo 3D 跟踪基准测试中，我们的模型在车辆和行人跟踪方面分别达到了 59.4 和 56.6 的 2 级 MOTA，比以前的方法高出 50%。我们的端到端 3D 检测和跟踪系统几乎实时运行，在 Waymo 上为 11 FPS，在 nuScenes 上为 16 FPS。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:8:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"2. Related work 二维对象检测从图像输入中预测轴对齐边界框。 RCNN 家族 [16,17,20,41] 找到类别不可知的边界框候选者，然后对其进行分类和改进。 YOLO [40]、SSD [32] 和 RetinaNet [31]直接找到一个特定类别的框候选，回避后面的分类和细化。基于中心的检测器，例如CenterNet [62] 或 CenterTrack [61]，直接检测隐式对象中心点而不需要候选框。许多 3D 检测器 [19、43、45、58] 从这些 2D 检测器演变而来。我们表明基于中心的表示 [61、62] 非常适合 3D 应用。 3D 对象检测旨在预测三维旋转边界框 [11,15,27,30,37,54,58,59]。它们不同于输入编码器上的二维检测器。 Vote3Deep [12] 利用以特征为中心的投票 [49] 来有效地处理等间距 3D 体素上的稀疏 3D 点云。 VoxelNet [64] 在每个体素内使用 PointNet [38] 来生成统一的特征表示，具有 3D 稀疏卷积 [18] 和 2D 卷积的头部从中产生检测。 SECOND [54] 简化了 VoxelNet 并加速了稀疏 3D 卷积。 PIXOR [55] 将所有点投影到具有 3D 占用和点强度信息的 2D 特征图上，以去除昂贵的 3D 卷积。PointPillars [27] 用支柱表示代替所有体素计算，每个地图位置一个高大的细长体素，提高了主干效率。 MVF [63] 和 Pillar-od [50] 结合多视图特征来学习更有效的支柱表示。我们的贡献集中在输出表示上，并且与任何 3D 编码器兼容，并且可以改进它们。 VoteNet [36] 通过使用点特征采样和分组的投票聚类来检测对象。相比之下，我们直接通过中心点的特征回归到 3D 边界框而不进行投票。 Wong 等人 [53] 和 Chen 等人 [8] 在对象中心区域（即点锚点）使用类似的多点表示并回归到其他属性。我们为每个对象使用一个正单元格，并使用关键点估计损失。 **两阶段 3D 对象检测：**最近的工作考虑将 RCNN 样式的 2D 检测器直接应用于 3D 域 [9, 42–44, 59]。他们中的大多数应用 RoIPool [41] 或 RoIAlign [20] 在 3D 空间中聚合特定于 RoI 的特征，使用基于 PointNet 的点 [43] 或体素 [42] 特征提取器。这些方法从 3D 激光雷达测量（点和体素）中提取区域特征，由于大量的点导致运行时间过长。相反，我们从中间特征图中提取 5 个表面中心点的稀疏特征。这使得我们的第二阶段非常高效并保持有效。 **3D 对象跟踪。**许多 2D 跟踪算法 [2、4、26、52] 很容易开箱即用地跟踪 3D 对象。然而，基于 3D 卡尔曼滤波器 [10、51] 的专用 3D 跟踪器仍然具有优势，因为它们可以更好地利用场景中的三维运动。在这里，我们采用了一种遵循 CenterTrack [61] 的更简单的方法。我们使用速度估计和基于点的检测来通过多个帧跟踪对象的中心。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:9:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"3. Preliminaries 2D CenterNet [62] 将目标检测改写为关键点估计。它采用输入图像并为 K 类中的每一个预测 w×h 热图 $\\hat{Y} \\in[0,1]^{w \\times h \\times K}$。输出热图中的每个局部最大值（即，其值大于其八个邻居的像素）对应于检测到的对象的中心。为了检索 2D 框，CenterNet 回归到在所有类别之间共享的大小映射$\\hat{Y} \\in$ ^S ∈ R w×h×2。对于每个检测对象，尺寸图将其宽度和高度存储在中心位置。 CenterNet 架构使用标准的全卷积图像主干，并在顶部添加了一个密集的预测头。在训练期间，CenterNet 学习在每个类 ci ∈ {1 的每个注释对象中心 qi 处使用渲染高斯核预测热图。 . . K}，并回归到带注释的边界框中心的对象大小 S。为了弥补骨干架构跨步引入的量化误差，CenterNet 还回归到局部偏移 ^O。 在测试时，检测器会生成 K 个热图和密集的类不可知回归图。热图中的每个局部最大值（峰值）对应一个对象，置信度与峰值处的热图值成正比。对于每个检测到的对象，检测器从相应峰值位置的回归图中检索所有回归值。根据应用领域，可能需要非极大值抑制 (NMS)。 3D 检测设 P = {(x, y, z, r)i} 是 3D 位置 (x, y, z) 和反射率 r 测量的无序点云。 3D 目标检测旨在从该点云预测鸟瞰图中的一组 3D 目标边界框 B = {bk}。每个边界框 b = (u, v, d, w, l, h, α) 由相对于物体地平面的中心位置 (u, v, d) 和 3D 大小 (w, l, h) 组成, 和由偏航 α 表示的旋转。在不失一般性的情况下，我们使用以自我为中心的坐标系，传感器位于 (0, 0, 0) 且偏航 = 0。 现代 3D 对象检测器 [19、27、54、64] 使用 3D 编码器将点云量化为常规 bin。基于点的网络 [38] 然后提取 bin 内所有点的特征。然后 3D 编码器将这些特征合并到它的主要特征表示中。大多数计算发生在主干网络中，主干网络仅对这些量化和合并的特征表示进行操作。主干网络的输出是一个地图视图特征图 M ∈ RW×L×F，宽度为 W，长度为 L，在地图视图参考框架中有 F 个通道。宽度和高度都与单个体素箱的分辨率和骨干网络的步幅直接相关。常见的主干包括 VoxelNet [54、64] 和 PointPillars [27]。 使用地图视图特征图 M 检测头，最常见的是单级 [31] 或两级 [41] 边界框检测器，然后从锚定在该开销特征图上的一些预定义边界框生成对象检测。由于 3D 边界框具有各种尺寸和方向，基于锚点的 3D 检测器很难将轴对齐的 2D 框拟合到 3D 对象。此外，在训练过程中，之前基于锚点的 3D 检测器依赖 2D Box IoU 进行目标分配 [42、54]，这给为不同类别或不同数据集选择正/负阈值带来了不必要的负担。在下一节中，我们将展示如何基于点表示构建原理性 3D 对象检测和跟踪模型。我们引入了一种新颖的基于中心的检测头，但依赖于现有的 3D 主干（VoxelNet 或 PointPillars）。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:10:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4. CenterPoint 图 2 显示了 CenterPoint 模型的总体框架。令 M ∈ RW×H×F 为 3D backbone 的输出。 CenterPoint 的第一阶段预测特定类别的热图、对象大小、亚体素位置细化、旋转和速度。所有输出都是密集预测。 中心热图头： center-head 的目标是在任何检测到的对象的中心位置产生一个热图峰值。这个头产生一个 K 通道热图 ^Y ，每个 K 类一个通道。在训练期间，它的目标是通过将带注释的边界框的 3D 中心投影到地图视图中而生成的 2D 高斯分布。我们使用焦点损失 [28, 62]。自上而下的地图视图中的对象比图像中的对象稀疏。在地图视图中，距离是绝对的，而图像视图通过透视扭曲了它们。考虑一个道路场景，在 mapview 中车辆占据的区域很小，但在 image-view 中，一些大物体可能占据了大部分屏幕。此外，透视投影中深度维度的压缩自然会使对象中心在图像视图中彼此更靠近。遵循 CenterNet [62] 的标准监督会产生非常稀疏的监督信号，其中大多数位置都被视为背景。为了抵消这一点，我们通过扩大在每个地面实况对象中心呈现的高斯峰来增加对目标热图 Y 的正监督。具体来说，我们将高斯半径设置为 σ = max(f(wl), τ )，其中 τ = 2 是允许的最小高斯半径，f 是 CornerNet [28] 中定义的半径函数。这样，CenterPoint 保持了基于中心的目标分配的简单性；该模型从附近的像素得到更密集的监督。 **回归头：**我们在对象的中心特征处存储了几个对象属性：子体素位置细化 o ∈ R 2 、地上高度 hg ∈ R、3D 大小 s ∈ R 3 和偏航角 (sin(α) , cos(α)) ∈ [−1, 1]2 。子体素位置细化 o 减少了骨干网络的体素化和跨步的量化误差。地上高度 hg 有助于在 3D 中定位对象并添加地图视图投影删除的缺失高程信息。方向预测使用偏航角的正弦和余弦作为连续回归目标。结合框大小，这些回归头提供了 3D 边界框的完整状态信息。每个输出都使用自己的头部。我们在地面实况中心位置使用 L1 损失来训练所有输出。我们回归到对数大小以更好地处理各种形状的box。在推理时，我们通过在每个对象的峰值位置索引到密集回归头输出来提取所有属性。 **速度头和跟踪：**为了通过时间跟踪对象，我们学习为每个检测到的对象预测二维速度估计 v ∈ R 2 作为附加回归输出。速度估计需要时间点云序列 [6]。在我们的实现中，我们将先前帧中的点转换并合并到当前参考帧中，并预测当前帧和过去帧之间的对象位置差异，这些差异由时间差（速度）归一化。与其他回归目标一样，速度估计也在当前时间步长的地面实况对象位置使用 L1 损失进行监督。 在推理时，我们使用此偏移量以贪婪的方式将当前检测与过去的检测相关联。具体来说，我们通过应用负速度估计将当前帧中的对象中心投影回前一帧，然后通过最近距离匹配将它们与跟踪对象匹配。按照 SORT [4]，我们在删除它们之前将不匹配的轨道保留最多 T = 3 帧。我们用最后已知的速度估计更新每个不匹配的轨道。详细的跟踪算法图见补充。 CenterPoint 将所有热图和回归损失结合到一个共同的目标中，并联合优化它们。它简化并改进了之前基于锚点的 3D 检测器（参见实验）。然而，目前所有对象属性都是从对象的中心特征推断出来的，它可能不包含足够的信息来进行准确的对象定位。例如，在自动驾驶中，传感器往往只能看到物体的侧面，而看不到它的中心。接下来，我们通过使用带有轻量级点特征提取器的第二个细化阶段来改进 CenterPoint。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:11:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4.1.两阶段中心点 我们使用不变的中心点作为第一阶段。第二阶段从主干的输出中提取额外的点特征。我们从预测边界框的每个面的 3D 中心提取一个点特征。请注意，边界框中心、顶面和底面中心都投影到地图视图中的同一点。因此，我们只考虑四个朝外的盒子面以及预测的对象中心。对于每个点，我们使用双线性插值从骨干地图视图输出 M 中提取特征。接下来，我们连接提取的点特征并将它们传递给 MLP。第二阶段在一阶段 CenterPoint 的预测结果之上预测类别不可知的置信度分数和框细化。 对于类别不可知的置信度分数预测，我们遵循 [25, 29, 42, 44] 并使用由框的 3D IoU 和相应的地面实况边界框引导的分数目标 ： $$ I=\\min \\left(1, \\max \\left(0,2 \\times I o U_t-0.5\\right)\\right) $$ 其中 $I o U_t$是第 t 个建议框与真实值之间的 IoU。我们使用二元交叉熵损失进行训练： $$ L_{\\text {score }}=-I_t \\log \\left(\\hat{I}_t\\right)-\\left(1-I_t\\right) \\log \\left(1-\\hat{I}_t\\right) $$ 其中$\\hat{I}_t$是预测的置信度分数。在推理过程中，我们直接使用单阶段的中心点类别预测并计算最终的置信分数$\\hat{Q}_t=\\sqrt{\\hat{Y}_t * \\hat{I}_t}$为两个分数的几何平均值，其中Qt为对象t的最终预测置信度，$\\hat{Y}_t=\\max _{0 \\leq k \\leq K} \\hat{Y}_{p, k}$ 和 $\\hat{I}_t$分别为对象t的第一阶段和第二阶段置信度。 对于盒回归，模型在第一阶段建议的基础上预测一个改进，我们用L1损失来训练模型。我们的两级CenterPoint简化并加速了之前使用昂贵的基于pointnet的特征提取器和RoIAlign操作的两级3D探测器[42,43]。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:11:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4.2. Architecture 所有第一阶段输出共享第一个3 × 3卷积层、批处理归一化[24]和ReLU。然后，每个输出使用由批处理范数和ReLU分隔的两个3×3卷积的自己的分支。我们的第二阶段使用了一个共享的两层MLP，其中包括批规范、ReLU和Dropout[21]，下降率为0.3，然后使用单独的三层MLP进行置信度预测和箱回归。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:11:2","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"5.Experiments **Waymo开放数据集：**Waymo开放数据集[46]包含798个车辆和行人的训练序列和202个验证序列。点云包含64道激光雷达，每0.1s对应180k个点。官方的三维检测评价指标包括三维边界盒平均精度(mAP)和mAP加权航向精度(mAPH)。mAP和mAPH是基于IoU的阈值，车辆为0.7，行人为0.5。对于3D跟踪，官方指标是多目标跟踪精度(MOTA)和多目标跟踪精度(MOTP)[3]。官方评估工具包还提供了两个难度级别的性能细分:1级用于具有超过五个激光雷达点的盒子，2级用于具有至少一个激光雷达点的盒子。 我们的Waymo模型对X轴和Y轴的检测范围为[- 75.2,75.2]，对Z轴的检测范围为[- 2m, 4m]。CenterPoint-Voxel使用(0.1m, 0.1m, 0.15m)体素大小遵循PV-RCNN[42]，而CenterPoint-Pillar使用(0.32m, 0.32m)网格大小。 **nuScenes数据集：**nuScenes[6]包含1000个驱动序列，分别有700、150、150个序列用于训练、验证和测试。每个序列大约20秒长，激光雷达频率为20fps。该数据集为每个激光雷达帧提供校准的车辆姿态信息，但每10帧(0.5s)才提供框注释。nuScenes使用32车道激光雷达，每帧产生大约30k个点。总共有28k、6k、6k个带注释的框架分别用于训练、验证和测试。注释包括10个具有长尾分布的类。官方的评估指标是班级的平均水平。对于3D检测，主要指标是平均平均精度(mAP)[13]和nuScenes检测分数(NDS)。mAP采用鸟瞰图中心距离\u003c 0.5m, 1m, 2m, 4m，而不是标准的盒重叠。NDS是mAP和其他属性度量的加权平均值，包括平移、比例、方向、速度和其他方框属性[6]。在我们提交测试集之后，nuScenes团队添加了一个新的神经规划度量(PKL)[35]。PKL度量基于规划者路线(使用3D检测)和地面真实轨迹的KL散度来衡量3D目标检测对下行自动驾驶任务的影响。因此，我们还报告了在测试集上评估的所有方法的PKL度量。 对于3D跟踪，nuScenes使用AMOTA[51]，它惩罚ID开关，假阳性和假阴性在各种召回阈值上的平均值。 对于nuScenes的实验，我们将X轴和Y轴的检测距离设置为[- 51.2m, 51.2m]， Z轴的检测距离设置为[- 5m, 3m]。CenterPoint-Voxel使用(0.1m, 0.1m, 0.2m)体素大小，CenterPoint-Pillars使用(0.2m, 0.2m)网格。 **训练和推理：**我们使用与先前工作相同的网络设计和训练时间表[42,65]。详细的超参数请参见附录。在两阶段CenterPoint的训练过程中，我们从第一阶段预测中随机抽取128个正负比为1:1的盒子[41]。如果提案与至少0.55 IoU的基础真值注释重叠，则提案是积极的[42]。在推理过程中，我们对非最大值抑制(NMS)之后的前500个预测运行第二阶段。推理时间是在Intel酷睿i7 CPU和Titan RTX GPU上测量的。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:12:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"5.1. 主要结果 **3D检测：**我们首先在Waymo和nuScenes的测试集上展示我们的3D检测结果。这两个结果都使用单个CenterPoint-Voxel模型。表1和表2总结了我们的结果。在Waymo的测试集上，我们的模型在车辆检测方面达到了71.8的2级mAPH，在行人检测方面达到了66.4的2级mAPH，比以前的方法高出了7.1%的车辆mAPH和10.6%的行人mAPH。在nuScenes上(表2)，我们的模型在多尺度输入和多模型集成的情况下比去年的挑战赛冠军CBGS[65]高出5.2%的mAP和2.2%的NDS。我们的模型也快得多，如后面所示。补充材料中有分类说明。我们的模型在所有类别中都显示出一致的性能改进，并且在小类别(交通锥的+5.6 mAP)和极端纵横比类别(自行车的+6.4 mAP和施工车辆的+7.0 mAP)中显示出更显著的收益。。更重要的是，我们的模型在神经平面指标(PKL)下的表现明显优于其他所有提交的作品，PKL是组织者在我们提交排行榜后评估的隐藏指标。这突出了我们框架的泛化能力。 表1.Waymo测试集上最先进的3D检测比较。我们展示了第1级和第2级基准测试的mAP和mAPH。 表2.nuScenes测试集上3D检测的最新比较。我们展示了nuScenes检测分数(NDS)和平均平均精度(mAP)。 **3D跟踪：**表3显示了CenterPoint在Waymo测试集上的跟踪性能。我们在第4节中描述的基于速度的最接近距离匹配明显优于Waymo论文[46]中的官方跟踪基线，后者使用基于卡尔曼滤波的跟踪器[51]。我们观察到车辆和行人跟踪的MOTA分别提高了19.4和18.9。在nuScenes(表4)上，我们的框架优于上次挑战的获胜者Chiu etal。[10]由8.8 AMOTA。值得注意的是，我们的跟踪不需要单独的运动模型，并且在可以忽略不计的时间内运行，比检测时间多1ms。 表3.Waymo测试集上最先进的3D跟踪比较。我们展示了MOTA和MOTP。↑越高越好，↓越低越好。 表4.在nuScenes测试集上最先进的3D跟踪比较。我们展示了AMOTA、假阳性(FP)、假阴性(FN)、id开关(IDS)的数量和每个类别的AMOTA。↑越高越好，↓越低越好。 5.2. 消融实验 **基于中心vs基于锚点：**我们首先将基于中心的一级检测器与基于锚点的一级检测器进行比较[27,54,65]。在Waymo上，我们遵循最先进的PV-RCNN[42]来设置锚超参数:我们在每个位置使用两个锚，分别为0°和90°，车辆的正/负IoU阈值为0.55/0.4，行人的正/负IoU阈值为0.5/0.35。在nuScenes上，我们遵循上次挑战获胜者CBGS的主播分配策略[65]。我们也与VoteNet[36]、PointRCNN[43]和PIXOR[55]中使用的基于网格点的表示进行了比较，后者将地面真值框内的所有点都赋值为正。对于这个实验，我们保持所有其他参数与我们的CenterPoint模型相同。 如表5所示，在Waymo数据集上，简单地从锚点切换到我们的中心，VoxelNet和PointPillars编码器的mAPH分别提高了4.3和4.5。在nuScenes上(表6)，CenterPoint在不同骨干网上提高了基于锚点的对应3.8-4.1 mAP和1.1-1.8 NDS。与基于网格点的表示(3.2-3.3 mAP和1.4-2.0 NDS改进)相比，结果相似。为了理解这种改进的来源，我们进一步展示了基于Waymo验证集上的对象大小和方向角度在不同子集上的性能分解。 表5.基于锚点和基于中心的Waymo验证3D检测方法比较。我们显示了每个班级和平均LEVEL 2 mAPH。 表6.基于锚点、基于网格点和基于中心的nuScenes验证3D检测方法比较我们给出了平均精度(mAP)和nuScenes检测分数(NDS)。 我们首先根据它们的航向角度将ground truth实例分为三个bin: 0°到15°、15°到30°和30°到45°。这种划分测试了探测器检测严重旋转箱子的性能，这对自动驾驶的安全部署至关重要。我们还将数据集分为三个部分:小、中、大，每个部分包含13个整体的地面真值框。 表7和表8总结了结果。当盒子旋转或偏离平均盒子尺寸时，我们的基于中心的检测器比基于锚点的基线表现得更好，这证明了模型在检测物体时捕捉旋转和尺寸不变性的能力。这些结果令人信服地突出了使用基于点的3D对象表示的优势。 表7.基于锚点和基于中心的不同航向角目标检测方法的比较。第2行和第3行列出了旋转角度的范围及其对应的对象部分。我们在Waymo验证中展示了两种方法的LEVEL 2 mAPH。 表8.对象大小对基于锚点和基于中心方法性能的影响。我们展示了不同大小范围对象的每个类LEVEL 2 mAPH:小33%，中33%和大33%。 **一阶段vs两阶段：**在表9中，我们展示了在Waymo验证中使用二维CNN特征的单阶段和两阶段CenterPoint模型的比较。具有多个中心功能的两阶段细化为两个3D编码器提供了小开销(6ms-7ms)的大精度提升。我们还与RoIAlign进行了比较，后者在RoI中密集采样6 × 6个点[42,44]，我们基于中心的特征聚合取得了相当的性能，但更快更简单。体素量化限制了两阶段CenterPoint对PointPillars行人检测的改进，因为行人通常只存在于模型输入的1个像素中。在我们的实验中，两阶段的细化并没有带来nuScenes上单阶段中心点模型的改进。这在一定程度上是由于nuScenes中的点云更稀疏。nuScenes使用32车道激光雷达，每帧产生约30k个激光雷达点，约为Waymo数据集中点数的16个。这限制了两阶段细化的可用信息和潜在改进。在PointRCNN[43]和PV-RCNN[42]等之前的两阶段方法中也观察到了类似的结果。 表9.在Waymo验证中，比较VoxelNet和PointPillars编码器的3D LEVEL 2 mAPH使用单阶段、两阶段具有3D中心特征，以及两阶段具有3D中心和表面中心特征。 **不同特征组件的影响：**在我们的两阶段CenterPoint模型中，我们只使用来自2D CNN特征图的特征。然而，以前的方法也提出利用体素特征进行第二阶段的细化[42,44]。在这里，我们比较了两种体素特征提取基线: **Voxel-Set提取：**PV-RCNN[42]提出了体素集抽象(voxel - set Abstraction, VSA)模块，该模块扩展了pointnet++[39]的集合抽象层，在一个固定半径的球中聚合体素特征。 **径向基函数(RBF)插值：**PointNet++[39]和SA-SSD[19]使用径向基函数从最近的三个非空三维特征体中聚合网格点特征。 对于这两个基线，我们使用它们的官方实现将鸟瞰特征与体素特征结合起来。表10总结了结果。它表明鸟瞰特征足以获得良好的性能，并且与文献中使用的体素特征相比效率更高[19,39,42]。 为了与之前没有对Waymo测试进行评估的工作进行比较，我们还在表11中报告了Waymo验证分割的结果。我们的模型在很大程度上优于所有已发布的方法，特别是对于具有挑战性的2级数据集的行人类别(+18.6 mAPH)，其中框只包含一个激光雷达点。 **3D跟踪：**表12为nuScenes验证上3D跟踪的消融实验。我们与去年的挑战赛冠军Chiu等人[10]进行了比较，他们使用基于马氏距离的卡尔曼滤波来关联CBGS的检测结果[65]。我们将评估分解为检测器和跟踪器，以进行严格的比较。给定相同的检测对象，使用我们简单的基于速度的最近点距离匹配比基于卡尔曼滤波的Mahalanobis距离匹配[10]高出3.7 AMOTA(行1 vs.行3，行2 vs.行4)。改进有两个来源:1)我们用学习到的点速度来建模物体运动，而不是用卡尔曼滤波器来建模三维边界框;2)我们通过中心点距离来匹配物体，而不是盒态的马氏距离或三维边界盒IoU。更重要的是，我们的跟踪是一个简单的最近邻匹配，没有任何隐藏状态计算。这节省了3D卡尔曼滤波器的计算开销(73ms vs. 1ms)。 **结论：**提出了一种基于中心的激光雷达点云三维目标同步检测与跟踪框架。我们的方法使用一个标准的3D点云编码器，在头部有几个卷积层，以产生鸟瞰热图和其他密集的回归输出。检测是一个简单的局部峰值提取与细化，跟踪是一个最接近的距离匹配。CenterPoint简单，接近实时，在Waymo和nuScenes基准测试中实现了最先进的性能。 VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection VoxelNet：基于点云的 3D 对象检测的端到端学习 准确检测 3D 点云中的对象是许多应用程序的核心问题，例如自主导航、家政机器人和增强/虚拟现实。为了将高度稀疏的 LiDAR 点云与区域建议网络 (RPN) 连接起来，大多数现有工作都集中在手工制作的特征表示上，例如，鸟瞰图投影。在这项工作中，我们消除了对 3D 点云进行手动特征工程的需要，并提出了 VoxelNet，这是一种通用的 3D 检测网络，它将特征提取和边界框预测统一到一个阶段、端到端的可训练深度网络中。==具体来说，VoxelNet 将点云划分为等间距的 3D 体素，并通过新引入的体素特征编码 (VFE) 层将每个体素内的一组点转换为统一的特征表示。通过这种方式，点云被","date":"2023-05-08","objectID":"/zh-cn/mot/:12:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"1. Introduction 基于点云的 3D 对象检测是各种现实世界应用的重要组成部分，例如自主导航 [11、14]、家政机器人 [28] 和增强/虚拟现实 [29]。与基于图像的检测相比，LiDAR 提供可靠的深度信息，可用于准确定位对象并表征其形状 [21、5]。然而，与图像不同的是，由于 3D 空间的非均匀采样、传感器的有效范围、遮挡和相对位姿等因素，LiDAR 点云是稀疏的并且具有高度可变的点密度。为了应对这些挑战，许多方法手动为点云制作特征表示，并对其进行3D对象检测。几种方法将点云投影到透视图中并应用基于图像的特征提取技术 [30、15、22]。其他方法将点云光栅化为 3D 体素网格，并使用手工制作的特征对每个体素进行编码 [43、9、39、40、21、5]。然而，这些手动设计选择引入了信息瓶颈，阻碍了这些方法有效地利用 3D 形状信息和检测任务所需的不变性。图像识别 [20] 和检测 [13] 任务的重大突破是由于从手工制作的特征转向机器学习的特征。 最近，Qi 等人[31] 提出了 PointNet，这是一种端到端的深度神经网络，可以直接从点云中学习逐点特征。这种方法在 3D 对象识别、3D 对象部分分割和逐点语义分割任务上展示了令人印象深刻的结果。在 [32] 中，引入了 PointNet 的改进版本，使网络能够学习不同尺度的局部结构。为了取得令人满意的结果，这两种方法在所有输入点（~1k 点）上训练特征变换器网络。由于使用激光雷达获得的典型点云包含约100k个点，因此像[31,32]中那样训练架构会导致高计算和内存需求。将 3D 特征学习网络扩展到更多点和 3D 检测任务是我们在本文中解决的主要挑战。 区域提议网络 (RPN) [34] 是一种高度优化的高效目标检测算法 [17、5、33、24]。然而，这种方法要求数据密集并以张量结构（例如图像、视频）组织，而典型的 LiDAR 点云并非如此。在本文中，我们缩小了点集特征学习和 RPN 之间在 3D 检测任务中的差距。 我们展示了 VoxelNet，这是一种通用的 3D 检测框架，它以端到端的方式同时从点云中学习判别特征表示并预测准确的 3D 边界框，如图 2 所示。我们设计了一种新颖的体素特征编码（VFE ）层，通过将逐点特征与局部聚合特征相结合，实现体素内的点间交互。堆叠多个 VFE 层允许学习复杂的特征以表征局部 3D 形状信息。具体来说，VoxelNet 将点云划分为等间距的 3D 体素，通过堆叠的 VFE 层对每个体素进行编码，然后 3D 卷积进一步聚合局部体素特征，将点云转换为高维体积表示。最后，RPN 使用体积表示并产生检测结果。这种高效的算法受益于稀疏点结构和体素网格上的高效并行处理。 我们评估了VoxelNet在鸟瞰图检测上的性能以及由KITTI提供的完整3D探测任务基准[11]。实验结果表明，VoxelNet在很大程度上优于最先进的基于激光雷达的3D检测方法。我们还证明了VoxelNet在从LiDAR点云检测行人和骑自行车的人方面取得了非常令人鼓舞的结果。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:13:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"1.1 Related Work 3D 传感器技术的快速发展促使研究人员开发有效的表示来检测和定位点云中的对象。一些较早的特征表示方法是 [41, 8, 7, 19, 42, 35, 6, 27, 1, 36, 2, 25, 26]。当丰富和详细的 3D 形状信息可用时，这些手工制作的特征会产生令人满意的结果。然而，它们无法适应更复杂的形状和场景，也无法从数据中学习所需的不变性，导致在自主导航等不受控制的场景中取得的成功有限。 鉴于图像提供了详细的纹理信息，许多算法从 2D 图像推断出 3D 边界框 [4、3、44、45、46、38]。然而，基于图像的 3D 检测方法的准确性受深度估计准确性的限制。 几种基于激光雷达的 3D 对象检测技术利用体素网格表示。 [43, 9] 使用从体素中包含的所有点派生的 6 个统计量对每个非空体素进行编码。 [39]融合了多个局部统计数据来表示每个体素。 [40] 计算体素网格上的截断符号距离。[21] 对 3D 体素网格使用二进制编码。 [5] 通过计算鸟瞰视图中的多通道特征图和正面视图中的圆柱坐标，引入了 LiDAR 点云的多视图表示。其他几项研究将点云投影到透视图上，然后使用基于图像的特征编码方案 [30、15、22]。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:13:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"1.2. Contributions 我们提出了一种用于基于点云的3D 检测的新型端到端可训练深度架构VoxelNet，它直接在稀疏3D 点上运行并避免手动特征工程引入的信息瓶颈。 我们提出了一种实现VoxelNet 的有效方法，它受益于稀疏点结构和体素网格上的高效并行处理。 我们在 KITTI 基准测试中进行实验，表明 VoxelNet 在基于 LiDAR 的汽车、行人和骑车人检测基准测试中产生了最先进的结果。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:13:2","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"2. VoxelNet 在本节中，我们将解释 VoxelNet 的架构、用于训练的损失函数以及实现网络的高效算法。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:14:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"2.1 VoxelNet架构 所提出的 VoxelNet 由三个功能块组成：(1) 特征学习网络，(2) 卷积中间层，以及 (3) 区域提议网络 [34]，如图 2 所示。我们在下面详细介绍了 VoxelNet部分。 图2.体素网络架构。特征学习网络将原始点云作为输入，将空间划分为体素，并将每个体素内的点转换为表征形状信息的向量表示。该空间表示为稀疏的 4D 张量。卷积中间层处理 4D 张量以聚合空间上下文。最后，RPN 生成 3D 检测。 2.1.1 特征学习网络 **体素分区：**给定一个点云，我们将 3D 空间细分为等间距的体素，如图 2 所示。假设点云包含范围分别为 D、H、W 沿 Z、Y、X 轴的 3D 空间。我们相应地定义大小为 vD、vH 和 vW 的每个体素。生成的 3D 体素网格的大小为 D′ = D/vD，H′ = H/vH，W′ = W/vW。这里，为简单起见，我们假设 D、H、W 是 vD、vH、vW 的倍数。 **分组：**我们根据点所在的体素对点进行分组。由于距离、遮挡、物体相对位姿、非均匀采样等因素。雷达点云是稀疏的，并且在整个空间中具有高度可变的点密度。因此，分组后，体素将包含可变数量的点。如图 2 所示，其中 Voxel-1 的点明显多于 Voxel-2 和 Voxel-4，而 Voxel-3 不包含任何点。 **随机采样：**通常高清 LiDAR 点云由约 100k 个点组成。直接处理所有点不仅会增加计算平台的内存/效率负担，而且整个空间中高度可变的点密度可能会使检测产生偏差。为此，我们从包含超过 T 个点的体素中随机抽取固定数量 T 个点。这种采样策略有两个目的，(1) 节省计算量（详见第 2.3 节）； (2) 减少体素之间点的不平衡，从而减少采样偏差，并为训练增加更多变化。 **堆叠体素特征编码：**关键创新是 VFE 层链。为简单起见，图 2 说明了一个体素的分层特征编码过程。不失一般性，我们使用 VFE Layer-1 在以下段落中描述细节。图 3 显示了 VFE 第 1 层的架构。 图3.体素特征编码层。 将 V = {pi = [xi , yi , zi , ri ] T ∈ R 4}i=1…t 表示为包含 t ≤ T LiDAR 点的非空体素，其中 pi 包含第 i 个的 XYZ 坐标点，ri 是接收到的反射率。我们首先计算局部均值作为 V 中所有点的质心，表示为 (vx, vy, vz)。然后我们用相对偏移 w.r.t 增加每个点 pi。质心并获得输入特征集 Vin = {p^i = [xi , yi , zi , ri , xi −vx, yi −vy, zi −vz] T ∈ R 7}i=1…t。接下来，每个 p^i 通过全连接网络 (FCN) 转换为特征空间，我们可以在其中聚合来自点特征 fi ∈ R m 的信息，以编码体素内包含的表面形状。 FCN 由线性层、批量归一化 (BN) 层和修正线性单元 (ReLU) 层组成。在获得逐点特征表示后，我们在与 V 相关联的所有 fi 上使用逐元素最大池化来获得 V 的局部聚合特征 ~f ∈ R m。最后，我们增加每个 fi 与 ∼f 形成逐点连接的特征 f out i = [f T i , ∼f T ] T ∈ R 2m。这样我们就得到了输出特征集Vout = {f out i }i…t。所有非空体素都以相同的方式编码，并且它们在 FCN 中共享相同的参数集。 我们使用 VFE-i(cin, cout) 来表示将维度 cin 的输入特征转换为维度 cout 的输出特征的第 i 个 VFE 层。线性层学习大小为 cin ×(cout/2) 的矩阵，逐点级联得到维度 cout 的输出 由于输出特征结合了逐点特征和局部聚合特征，堆叠 VFE 层对体素内的点交互进行编码，并使最终特征表示能够学习描述性形状信息。 voxel-wise 特征是通过 FCN 将 VFE-n 的输出转换为 R C 并应用 element-wise Maxpool 获得的，其中 C 是 voxel-wise 特征的维度，如图 2 所示。 **稀疏张量表示：**通过仅处理非空体素，我们获得了体素特征列表，每个特征都唯一地关联到特定非空体素的空间坐标。获得的体素特征列表可以表示为一个稀疏的 4D 张量，大小为 C × D' × H' × W'，如图 2 所示。虽然点云包含约 100k 个点，但超过 90% 的体素通常是空的。将非空体素特征表示为稀疏张量大大减少了反向传播过程中的内存使用和计算成本，这是我们高效实施的关键步骤。 2.1.2 卷积中间层 我们用ConvMD(cin, cout, k, s, p)表示一个M维卷积算子，其中cin和cout是输入输出通道数，k, s, p是核大小对应的M维向量，分别是步幅大小和填充大小。当 M 维度的大小相同时，我们使用标量来表示大小，例如k 为 k = (k, k, k)。 每个卷积中间层应用 3D 卷积，BN层，ReLU层依次。卷积中间层在逐渐扩大的感受野内聚合体素特征，为形状描述添加更多上下文。卷积中间层中过滤器的详细尺寸在第 3 节中解释。 2.1.3 区域建议网络 最近，区域建议网络 [34] 已成为性能最佳的对象检测框架 [40、5、23] 的重要组成部分。在这项工作中，我们对 [34] 中提出的 RPN 架构进行了几个关键修改，并将其与特征学习网络和卷积中间层相结合，形成端到端的可训练管道。 我们的 RPN 的输入是由卷积中间层提供的特征图。该网络的架构如图 4 所示。该网络具有三个全卷积层块。每个块的第一层通过步长为 2 的卷积将特征图下采样一半，然后是一系列步长为 1 的卷积（×q 表示滤波器的 q 次应用）。在每个卷积层之后，应用 BN 和 ReLU 操作。然后我们将每个块的输出上采样到固定大小并连接以构建高分辨率特征图。最后，这个特征图被映射到所需的学习目标：(1) 概率得分图和 (2) 回归图。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:14:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"2.2.损失函数 设 {a pos i }i=1…Npos 是 Npos 正锚的集合，{a neg j }j=1…Nneg 是 Nneg 负锚的集合。我们将一个 3D ground truth box 参数化为 (x g c , yg c , zg c , lg , wg , hg , θg )，其中 x g c , yg c , zg c 表示中心位置，l g , wg , hg 分别是长、宽、高盒子的角度，θ g 是绕 Z 轴的偏航角。为了从参数化为 (x a c , ya c , za c , la , wa , ha , θa ) 的匹配正锚中检索地面真值框，我们定义了包含对应中心位置Δx, Δy, Δz、尺寸mensions和旋转角∆θ， 7 个回归目标的残差向量 u ∗ ∈ R 7 ，它们被定义为： $$ \\begin{aligned} \u0026 \\Delta x=\\frac{x_c^g-x_c^a}{d^a}, \\Delta y=\\frac{y_c^g-y_c^a}{d^a}, \\Delta z=\\frac{z_c^g-z_c^a}{h^a} \\ \u0026 \\Delta l=\\log \\left(\\frac{l^g}{l^a}\\right), \\Delta w=\\log \\left(\\frac{w^g}{w^a}\\right), \\Delta h=\\log \\left(\\frac{h^g}{h^a}\\right), \\ \u0026 \\Delta \\theta=\\theta^g-\\theta^a \\end{aligned} $$ 其中$d^a=\\sqrt{\\left(l^a\\right)^2+\\left(w^a\\right)^2}$是锚框底部的对角线。在这里，我们的目标是直接估计定向 3D 框并使用对角线 d a 均匀地归一化 Δx 和 Δy，这不同于 [34, 40, 22, 21, 4, 3, 5]。我们定义损失函数如下： $$ \\begin{aligned} L \u0026 =\\alpha \\frac{1}{N_{\\mathrm{pos}}} \\sum_i L_{\\mathrm{cls}}\\left(p_i^{\\mathrm{pos}}, 1\\right)+\\beta \\frac{1}{N_{\\mathrm{neg}}} \\sum_j L_{\\mathrm{cls}}\\left(p_j^{\\mathrm{neg}}, 0\\right) \\ \u0026 +\\frac{1}{N_{\\mathrm{pos}}} \\sum_i L_{\\mathrm{reg}}\\left(\\mathbf{u}_i, \\mathbf{u}_i^*\\right) \\end{aligned} $$ 其中 p pos i 和 p neg j 分别表示正锚点 a pos i 和负锚点 a neg j 的 softmax 输出，而 ui ∈ R 7 和 u ∗ i ∈ R 7 是正锚点 a pos 的回归输出和ground truth我 。前两项是 {a pos i }i=1…Npos 和 {a neg j }j=1…Nneg 的归一化分类损失，其中 Lcls 代表二元交叉熵损失，α、β 是平衡相对重要性的正常数。最后一项 Lreg 是回归损失，我们在这里使用 SmoothL1 函数 [12, 34]。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:14:2","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"2.3.高效实施 GPU 针对处理密集张量结构进行了优化。直接使用点云的问题是点在空间中分布稀疏，每个体素都有可变数量的点。我们设计了一种将点云转换为密集张量结构的方法，其中可以跨点和体素并行处理堆叠的 VFE 操作。 该方法总结在图 5 中。我们初始化一个 K × T × 7 维张量结构来存储体素输入特征缓冲区，其中 K 是非空体素的最大数量，T 是每个体素的最大点数，7是每个点的输入编码维度。这些点在处理之前是随机的。对于点云中的每个点，我们检查相应的体素是否已经存在。此查找操作使用哈希表在 O(1) 中高效完成，其中体素坐标用作哈希键。如果体素已经初始化，如果点少于 T，我们将点插入到体素位置，否则忽略该点。如果体素未初始化，我们初始化一个新的体素，将其坐标存储在体素坐标缓冲区中，并将点插入到该体素位置。体素输入特征和坐标缓冲区可以通过单次遍历点列表来构建，因此其复杂度为 O(n)。为了进一步提高内存/计算效率，可以仅存储有限数量的体素 (K) 并忽略来自具有少量点的体素的点。 构建体素输入缓冲区后，堆叠式 VFE 仅涉及点级和体素级密集操作，可以在 GPU 上并行计算。请注意，在 VFE 中进行串联操作后，我们将对应于空点的特征重置为零，这样它们就不会影响计算出的体素特征。最后，使用存储的坐标缓冲区，我们将计算出的稀疏体素结构重组为密集体素网格。以下卷积中间层和 RPN 操作在密集的体素网格上工作，可以在 GPU 上有效地实现。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:14:3","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"3. Training Details 在本节中，我们将解释 VoxelNet 的实现细节和训练过程。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:15:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"3.1.网络详情 我们的实验设置基于 KITTI 数据集 [11] 的 LiDAR 规范。 汽车检测: 对于此任务，我们分别考虑沿 Z、Y、X 轴 [−3, 1] × [−40, 40] × [0, 70.4] 米范围内的点云。投影到图像边界之外的点被删除 [5]。我们选择 vD = 0.4，vH = 0.2，vW = 0.2 米的体素大小，这导致 D' = 10，H' = 400，W' = 352。我们将 T = 35 设置为随机采样点的最大数量在每个非空体素中。我们使用两个 VFE 层 VFE-1(7, 32) 和 VFE-2(32, 128)。最终的 FCN 将 VFE-2 输出映射到 $\\mathbb{R}^{128}$。因此，我们的特征学习网络生成了一个形状为 128 × 10 × 400 × 352 的稀疏张量。为了聚合体素特征，我们依次使用卷积中间层，分别为 Conv3DD(128, 64, 3,(2,1,1), (1,1,1))， Conv3D(64, 64, 3, (1,1,1), (0,1,1))和Conv3D(64, 64, 3, (2,1,1), (1,1,1))，产生大小为 64 × 2 × 400 × 352 的 4D 张量。整形后，RPN 的输入是一个大小为 128×400×352 的特征图，其中维度对应于 3D 张量的通道、高度和宽度。图 4 说明了此任务的详细网络架构。与 [5] 不同，我们仅使用一个锚尺寸，la = 3.9，wa = 1.6，ha = 1.56 米，以 z a c = −1.0 米为中心，有两个旋转，0 度和 90 度。我们的 anchor 匹配标准如下：如果一个 anchor 与 ground truth 具有最高的 Intersection over Union (IoU) 或者其与 ground truth 的 IoU 高于 0.6（在鸟瞰图中），则该锚被认为是正的。如果 anchor 与所有 ground truth boxes 之间的 IoU 小于 0.45，则该 anchor 被认为是负的。我们将锚点视为不关心它们是否具有 0.45 ≤ IoU ≤ 0.6 与任何基本事实。我们在等式中设置 α = 1.5 和 β = 1.2。 **行人和骑车人检测：**输入范围 1 分别为 [−3, 1] × [−20, 20] × [0, 48] 米，沿 Z、Y、X 轴。我们使用与汽车检测相同的体素大小，得到 D = 10，H = 200，W = 240。我们设置 T = 45 以获得更多的 LiDAR 点以更好地捕获形状信息。特征学习网络和卷积中间层与汽车检测任务中使用的网络相同。对于 RPN，我们通过将第一个 2D 卷积中的步幅大小从 2 更改为 1 对图 4 中的块 1 进行了修改。这允许在锚点匹配中获得更精细的分辨率，这对于检测行人和骑自行车的人是必要的。我们使用锚尺寸 l a = 0.8, wa = 0.6, ha = 1.73 米，以 z a c = −0.6 米为中心，旋转 0 度和 90 度进行行人检测，并使用锚尺寸 l a = 1.76, wa = 0.6, ha = 1.73 米，以 z a c = −0.6 米为中心z a c = −0.6，旋转 0 度和 90 度以检测骑车人。具体的anchor匹配标准如下：如果anchor与ground truth的IoU最高，或者其与ground truth的IoU大于0.5，我们将其指定为positive。如果一个锚点与每个基本事实的 IoU 小于 0.35，则该锚点被认为是负的。对于具有 0.35 ≤ IoU ≤ 0.5 且具有任何基本事实的锚点，我们将它们视为无关紧要。 在训练期间，我们在前 150 个时期使用学习率为 0.01 的随机梯度下降 (SGD)，在最后 10 个时期将学习率降低至 0.001。我们使用 16 个点云的批量大小。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:15:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"3.2. 数据增强 如果训练点云少于 4000 个，从头开始训练我们的网络将不可避免地出现过度拟合。为了减少这个问题，我们引入了三种不同形式的数据增强。增强训练数据是即时生成的，无需存储在磁盘上 [20]。 定义集合 M = {pi = [xi , yi , zi , ri ] T ∈ R 4}i=1,…,N 为整个点云，由 N 个点组成。我们将一个 3D 边界框 bi 参数化为 (xc, yc, zc, l, w, h, θ)，其中 xc, yc, zc 是中心位置，l, w, h 是长、宽、高，θ 是绕 Z 轴的偏航旋转。我们定义 Ωi = {p|x ∈ [xc − l/2, xc + l/2], y ∈ [yc − w/2, yc + w/2], z ∈ [zc − h/2, zc + h/2], p ∈ M} 作为包含 bi 内所有 LiDAR 点的集合，其中 p = [x, y, z, r] 表示整个集合 M 中的特定 LiDAR 点。 第一种数据增强形式独立地将扰动应用于每个地面实况 3D 边界框以及框中的那些 LiDAR 点。具体来说，我们围绕 Z 轴旋转 bi 和相关的 Ωi 相对于 (xc, yc, zc) 通过均匀分布的随机变量 ∆θ ∈ [−π/10, +π/10]。然后我们将平移 (Δx, Δy, Δz) 添加到 bi 的 XYZ 分量和 Ωi 中的每个点，其中 Δx, Δy, Δz 独立于具有均值零和标准差的高斯分布绘制1.0。为了避免物理上不可能的结果，我们在扰动后对任意两个框进行碰撞测试，如果检测到碰撞则恢复到原始状态。由于扰动独立应用于每个地面真值框和相关的 LiDAR 点，因此网络能够从比原始训练数据更多的变化中学习。 其次，我们对所有地面真值框 bi 和整个点云 M 应用全局缩放。具体来说，我们将 XYZ 坐标和每个 bi 的三个维度以及 M 中所有点的 XYZ 坐标与从中提取的随机变量相乘均匀分布 [0.95, 1.05]。如基于图像的分类 [37、18] 和检测任务 [12、17] 所示，引入全局尺度增强提高了网络检测具有各种大小和距离的对象的鲁棒性。 最后，我们对所有地面真值框 bi 和整个点云 M 应用全局旋转。旋转沿 Z 轴和围绕 (0, 0, 0) 应用。全局旋转偏移由均匀分布 [−π/4, +π/4] 采样确定。通过旋转整个点云，我们模拟车辆转弯。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:15:2","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4. Experiments 我们在 KITTI 3D 对象检测基准 [11] 上评估 VoxelNet，其中包含 7,481 个训练图像/点云和 7,518 个测试图像/点云，涵盖三个类别：汽车、行人和骑自行车的人。对于每个类别，检测结果根据三个难度级别进行评估：简单、中等和困难，这三个级别根据对象大小、遮挡状态和截断级别确定。由于测试集的基本事实不可用并且对测试服务器的访问受到限制，我们使用 [4, 3, 5] 中描述的协议进行综合评估，并将训练数据细分为训练集和验证集，这将产生 3,712 个用于训练的数据样本和 3,769 个用于验证的数据样本。拆分避免了来自同一序列的样本同时包含在训练和验证集中 [3]。最后我们还展示了使用 KITTI 服务器的测试结果。 对于汽车类别，我们将所提出的方法与几种性能最佳的算法进行比较，包括基于图像的方法：Mono3D [3] 和 3DOP [4]；基于 LiDAR 的方法：VeloFCN [22] 和 3D-FCN [21]；和多模式方法 MV [5]。 Mono3D [3]、3DOP [4] 和 MV [5] 使用预训练模型进行初始化，而我们仅使用 KITTI 中提供的 LiDAR 数据从头开始训练 VoxelNet。 为了分析端到端学习的重要性，我们实施了一个强大的基线，该基线源自 VoxelNet 架构，但使用手工制作的特征而不是建议的特征学习网络。我们称这个模型为手工制作的基线（HC-baseline）。 HC-baseline 使用 [5] 中描述的鸟瞰图特征，这些特征是在 0.1m 分辨率下计算的。与 [5] 不同的是，我们将高度通道的数量从 4 个增加到 16 个，以捕获更详细的形状信息——进一步增加高度通道的数量并没有带来性能提升。我们用相似大小的二维卷积层替换 VoxelNet 的中间卷积层，分别是 Conv2D(16, 32, 3, 1, 1), Conv2D(32, 64, 3, 2, 1), Conv2D(64, 128, 3) , 1, 1).最后，RPN 在 VoxelNet 和 HC-baseline 中是相同的。 HC-baseline 和 VoxelNet 中的参数总数非常相似。我们使用第 3 节中描述的相同训练过程和数据增强来训练 HC 基线。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:16:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4.1. KITTI 验证集评估 **指标：**我们遵循官方的 KITTI 评估协议，其中汽车类的 IoU 阈值为 0.7，行人和自行车类的 IoU 阈值为 0.5。鸟瞰图和全 3D 评估的 IoU 阈值相同。我们使用平均精度 (AP) 指标比较这些方法。 **鸟瞰图评估：**评估结果如表 1 所示。VoxelNet 在所有三个难度级别上始终优于所有竞争方法。与最先进的 [5] 相比，HC-baseline 也取得了令人满意的性能，这表明我们的基区域建议网络 (RPN) 是有效的。对于鸟瞰图中的行人和骑车人检测任务，我们将提出的 VoxelNet 与 HC-baseline 进行了比较。对于这些更具挑战性的类别，VoxelNet 产生的 AP 远高于 HC 基线，这表明端到端学习对于基于点云的检测至关重要。 我们要注意的是，[21] 报告的简单、中等和困难级别分别为 88.9%、77.3% 和 72.7%，但这些结果是基于 6,000 个训练框架和～1,500 个验证框架的不同拆分获得的，并且它们不能直接与表 1 中的算法进行比较。因此，我们不将这些结果包含在表中。 **3D 评估：**与只需要在 2D 平面中精确定位物体的鸟瞰图检测相比，3D 检测是一项更具挑战性的任务，因为它需要在 3D 空间中更精细地定位形状。表 2 总结了比较。对于汽车类，VoxelNet 在所有难度级别上都明显优于 AP 中的所有其他方法。具体来说，仅使用 LiDAR，VoxelNet 明显优于最先进的方法 MV (BV+FV+RGB) [5]，基于LiDAR+RGB，在简单、中等和困难级别分别提高了 10.68%、2.78% 和 6.29%。 HC-baseline 达到了与 MV [5] 方法相似的精度。 与鸟瞰图评估一样，我们还将 VoxelNet 与 HC-baseline 在 3D 行人和骑车人检测方面进行了比较。由于 3D 姿势和形状的高度变化，成功检测这两个类别需要更好的 3D 形状表示。如表 2 所示，针对更具挑战性的 3D 检测任务强调了 VoxelNet 的改进性能（从鸟瞰图提高约 8% 到 3D 检测提高约 12%），这表明 VoxelNet 在捕获 3D 形状信息方面比手工制作的功能。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:16:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4.2. KITTI 测试集评估 我们通过将检测结果提交给官方服务器，在 KITTI 测试集上对 VoxelNet 进行了评估。结果总结在表 3 中。VoxelNet 在所有任务（鸟瞰图和 3D 检测）和所有困难方面都明显优于先前发布的最新技术 [5]。我们要注意的是，KITTI 基准测试中列出的许多其他主要方法同时使用 RGB 图像和 LiDAR 点云，而 VoxelNet 仅使用 LiDAR。 我们在图 6 中展示了几个 3D 检测示例。为了更好地可视化，将使用 LiDAR 检测到的 3D 框投影到 RGB 图像上。如图所示，VoxelNet 在所有类别中都提供了高度准确的 3D 边界框。 VoxelNet 的推理时间为 33 毫秒：体素输入特征计算需要 5 毫秒，特征学习网络需要 16 毫秒，卷积中间层需要 1 毫秒，区域建议网络在 TitanX GPU 和1.7GHz 中央处理器上需要 11 毫秒。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:16:2","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"5. Conclusion 基于 LiDAR 的 3D 检测中的大多数现有方法都依赖于手工制作的特征表示，例如，鸟瞰图投影。在本文中，我们消除了手动特征工程的瓶颈，并提出了 VoxelNet，这是一种用于基于点云的 3D 检测的新型端到端可训练深度架构。我们的方法可以直接在稀疏 3D 点上操作并有效地捕获 3D 形状信息。我们还展示了一种有效的 VoxelNet 实现，它受益于点云稀疏性和体素网格上的并行处理。我们在 KITTI 汽车检测任务上的实验表明，VoxelNet 大大优于最先进的基于 LiDAR 的 3D 检测方法。在更具挑战性的任务中，例如行人和骑自行车者的 3D 检测，VoxelNet 还展示了令人鼓舞的结果，表明它提供了更好的 3D 表示。未来的工作包括扩展 VoxelNet 以用于联合 LiDAR 和基于图像的端到端 3D 检测，以进一步提高检测和定位精度。 VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking VoxelNeXt:用于3D对象检测和跟踪的完全稀疏的VoxelNet **摘要：**3D物体检测器通常依赖于手工制作的代理，例如锚点或中心，并将经过充分研究的2D框架转换为3D。因此，稀疏体素特征需要通过密集预测头进行密集化和处理，这不可避免地需要额外的计算成本。在本文中，我们提出了VoxelNext来进行完全稀疏的3D物体检测。我们的核心见解是直接基于稀疏体素特征来预测对象，而不依赖于手工制作的代理。我们强大的稀疏卷积网络VoxelNeXt完全通过体素特征检测和跟踪3D物体。它是一个优雅而高效的框架，不需要稀疏到密集的转换或NMS后处理。我们的方法在nuScenes数据集上实现了比其他主机检测器更好的速度-精度权衡。我们首次证明了一个完全稀疏的基于体素的表示可以很好地用于LIDAR 3D目标检测和跟踪。在nuScenes、Waymo和Argoverse2基准测试上进行的大量实验验证了我们方法的有效性。我们的模型在nuScenes跟踪测试基准上优于所有现有的LIDAR方法。代码和模型可在github.com/dvlab-research/VoxelNeXt上获得。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:17:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"1. Introduction 3D感知是自动驾驶系统的基本组成部分。三维检测网络以稀疏点云或体素为输入，对三维物体进行定位和分类。最近的3D目标检测器[12,41,57]由于其效率，通常采用稀疏卷积网络(sparse cnn)[53]进行特征提取。受2D目标检测框架[14,39]的启发，锚点[12,53]或中心[57]，即CenterPoint[57]中的密集点锚点，通常用于预测。它们都是手工制作的，并作为3D对象的中间代理。 锚点和中心首先是为规则和网格结构的图像数据设计的，没有考虑3D数据的稀疏性和不规则性。为了使用这些代理表示，主流的检测器[12,41,57]将三维稀疏特征转换为二维密集特征，从而为有序锚点或中心构建密集检测头。尽管有用，但这种密集的封头传统会导致其他限制，包括效率低下和复杂的管道，如下所述。 在图1中，我们将CenterPoint中的热图可视化[57]。很明显，很大一部分空间的预测分数几乎为零。由于固有的稀疏性和背景点多，只有少数点有响应，即nuScenes验证集中Car类的平均响应不到1%。然而，密集预测头根据密集卷积计算的要求对特征映射中的所有位置进行计算。它们不仅浪费了大量的计算，而且由于冗余的预测而使检测管道复杂化。它需要使用非最大抑制(NMS)，如后处理来删除重复检测，从而使检测器不美观。这些限制促使我们寻求替代的稀疏检测解决方案。 在本文中，我们提出了VoxelNeXt。它是一个简单，高效，无后处理的3D物体检测器。我们设计的核心是一个体素到对象的方案，它通过一个强大的全稀疏卷积网络，直接从体素特征预测3D对象。关键的优点是我们的方法可以摆脱锚代理，稀疏到密集的转换，区域建议网络，和其他复杂组件。我们在图2中说明了主流3D探测器和我们的管道。 高推理效率是由于我们的体素到目标方案避免了密集的特征映射。它只对稀疏和必要的位置进行预测，如表1所示，与CenterPoint[57]进行比较。这种表示也使得VoxelNeXt很容易扩展到使用离线跟踪器的3D跟踪。先前的工作[57]只跟踪预测的对象中心，这可能涉及到对其位置的预测偏差。在VoxelNeXt中，查询体素，即用于框预测的体素，也可以被跟踪以进行关联。 最近，FSD[16]利用了全稀疏框架。在VoteNet[37]的激励下，它对对象中心进行投票，并采用迭代优化。由于3D稀疏数据通常分散在物体表面，因此这种投票过程不可避免地会引入偏见或错误。因此，需要细化，例如迭代组校正，以确保最终的准确性。该系统由于过于相信物体中心而变得复杂。FSD[16]在大范围Argoverse2上很有前景，但其效率不如我们，如图3所示。 为了证明VoxelNeXt的有效性，我们在nuScenes[3]、Waymo[45]、Argoverse2[52]数据集的三个大规模基准上评估了我们的模型。在这两个基准测试中，VoxelNeXt在3D物体检测方面都实现了高效的领先性能。它还产生了最先进的3D跟踪性能。在nuScenes跟踪测试分裂中，它在所有LIDAR-only条目中排名第一[3]。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:18:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"2. Related Work **雷达检测器：**3D探测器的工作原理通常与2D探测器相似，如R-CNN系列[12,34,41,54]和CenterPoint系列[14,57,60]。三维检测由于数据分布的稀疏性而区别于二维任务。但是许多方法[12,53,57,61]仍然寻求二维密集卷积头作为解决方案。 VoxelNet[61]使用PointNet[38]进行体素特征编码，然后使用密集区域建议网络和头进行预测。SECOND[53]通过使用密集的锚定头进行高效的稀疏卷积来改进VoxelNet。其他最先进的方法，包括PV-RCNN[41]、Voxel R-CNN[12]和VoTr[35]仍然保留了稀疏到密集的方案来扩大感受野。 在2D CenterNet[14]的激励下，CenterPoint[57]被应用于三维检测和跟踪。它将骨干网络的稀疏输出转换为地图-视图密集特征图，并基于密集特征预测物体中心位置的密集热图。这种基于密集中心的预测已被几种密集头方法所采用[30,33]。在本文中，我们采取了一个新的方向，并令人惊讶地证明了一个简单而强的稀疏CNN足以进行直接预测。值得注意的发现是，密集的head并不总是必要的。 稀疏检测器：[16,46,47]的方法避免了密集的检测头，而是引入了其他复杂的管道。RSN[47]对距离图像进行前景分割，然后在剩余的稀疏数据上检测3D物体。SWFormer[46]提出了一种带有精细窗口分割和带有特征金字塔的多个头的稀疏变压器。在VoteNet[37]的激励下，FSD[16]利用点聚类和群校正来解决中心特征缺失问题。这些探测器进行稀疏的预测，但以不同的方式使检测管道复杂化。在我们的工作中，这个中心缺失问题也可以简单地跳过具有大接受域的稀疏网络。我们对常用的稀疏cnn做最小的调整来实现完全稀疏的检测器。 稀疏卷积网络：稀疏卷积网络因其高效性成为3D深度学习的主干网络[10,11,23,41]。人们普遍认为，它的表示能力在预测方面是有限的。为了解决这个问题，[12,41,49,53]的3D检测器依赖于密集卷积头进行特征增强。最近的方法[6,32]对稀疏cnn进行卷积修改。[21,35]的方法甚至用变压器代替大的感受野。与所有这些解决方案相反，我们证明可以通过额外的下采样层简单地解决接收场不足的瓶颈，而无需任何其他复杂的设计。 **3D对象跟踪：**基于多帧激光雷达的三维目标跟踪模型。以往的大多数方法[2,9,51]对检测结果直接使用卡尔曼滤波，如AB3DMOT[51]。在CenterTrack[60]之后，CenterPoint[57]预测了通过多帧关联对象中心的速度。本文引入查询体素进行关联，有效缓解了目标中心的预测偏差。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:19:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"3. Fully Sparse Voxel-based Network 点云或体素是不规则分布的，通常分散在3D物体的表面，而不是在中心或内部。这促使我们沿着一个新的方向研究，直接基于体素来预测3D盒子，而不是手工制作的锚点或中心。 为此，我们的目标是进行最小的修改，使普通的3D稀疏CNN网络适应直接体素预测。在下面，我们将介绍骨干自适应(第3.1节)，稀疏头部设计(第3.2节)，以及扩展到3D目标跟踪(第3.3节)。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:20:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"3.1. 稀疏CNN主干自适应 **额外的下采样：**为了确保对稀疏体素特征进行直接和正确的预测，必须具有足够的接受域的强特征表示。尽管朴素的稀疏CNN骨干网已被广泛应用于3D目标检测器中[12,41,57]，但最近的研究发现了它的弱点，并提出了各种方法来增强稀疏骨干网，如精心设计的卷积[7]、大核[8]和变压器[25,26,35]。 与所有这些方法不同，我们尽可能少地修改来实现这一点，只使用额外的下采样层。缺省情况下，朴素稀疏CNN骨干网有4个阶段，特征步长为{1,2,4,8}。我们将输出的稀疏特征分别命名为{F1, F2, F3, F4}。这种设置不能直接预测，特别是对于大型物体。为了增强其能力，我们简单地包括两个额外的下采样层，以获得{F5, F6}的步长为{16,32}的特征。 这个小小的变化直接对感受野的扩大产生了显著的影响。我们将最后三个阶段{F4, F5, F6}的稀疏特征组合到Fc。它们的空间分辨率都与F4对齐。对于阶段i, Fi是一组单独的特征fp。p∈Pi是三维空间中的一个位置，坐标为(xp, yp, zp)。该过程如图4所示。值得注意的是，这种简单的稀疏连接不需要其他参数化层。稀疏特征Fc及其位置Pc为： $$ \\begin{aligned} \u0026 F_c=F_4 \\cup\\left(F_5 \\cup F_6\\right), \\ \u0026 P_6^{\\prime}=\\left{\\left(x_p \\times 2^2, y_p \\times 2^2, z_p \\times 2^2\\right) \\mid p \\in P_6\\right} \\ \u0026 P_5^{\\prime}=\\left{\\left(x_p \\times 2^1, y_p \\times 2^1, z_p \\times 2^1\\right) \\mid p \\in P_5\\right} \\ \u0026 P_c=P_4 \\cup\\left(P_5^{\\prime} \\cup P_6^{\\prime}\\right) . \\end{aligned} $$ 我们在图5中可视化了有效的感受野(erf)。有了额外的下采样层，erf更大，预测框更准确。它足够有效，并且花费很少的额外计算，如表2所示。因此，我们采用这种简单的设计作为骨干网。 稀疏高度压缩：[12,41,57]的3D目标检测器通过将稀疏特征转换为密集特征，然后将深度(沿z轴)合并为通道维度，将3D体素特征压缩为密集的2D地图。这些操作需要占用内存和计算。 在VoxelNet中，我们发现二维稀疏特征对于预测是有效的。高度压缩在VoxelNeXt是完全稀疏的。我们简单地把所有体素放在地面上，并在相同的位置总结特征。它的花费不超过1毫秒。我们发现，利用压缩后的二维稀疏特征进行预测的成本要低于利用三维稀疏特征进行预测的成本，如表5所示。压缩后的稀疏特征¯Fc及其位置¯Pc为: $$ \\begin{aligned} \\bar{P}c \u0026 =\\left{\\left(x_p, y_p\\right) \\mid p \\in P_c\\right} \\ \\bar{F}c \u0026 =\\left{\\sum{p \\in S{\\bar{p}}} f_p, \\mid \\bar{p} \\in \\bar{P}_c\\right} \\end{aligned} $$ 其中Sp¯= {p | xp = xp¯，yp = yp¯，p∈Pc}，包含放置在相同2D位置p。 **空间体素修剪：**我们的网络完全基于体素。3D场景通常包含大量冗余的背景点，这些背景点对预测几乎没有好处。我们沿着下采样层逐渐修剪不相关的体素。根据SPS-Conv[32]，我们抑制了小特征量体素的扩张，如图6所示。将抑制比设为0.5，我们只对特征幅度|fp|(在通道维度上的平均值)排在所有体素的前一半的体素进行放大。体素修剪在不影响性能的情况下大大节省了计算量，如表3所示。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:20:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"3.2. 稀疏预测头 **体素选择：**VoxelNeXt模型的详细框架如图4所示。我们不依赖于密集特征映射M，而是直接基于3D CNN骨干网络V∈R N×F的稀疏输出进行对象预测。我们首先预测K类的体素分数，s∈R N×K。在训练过程中，我们将最接近每个标注的边界框中心的体素分配为正样本。我们使用焦点丢失[31]进行监督。我们注意到，在推理查询过程中，体素通常不在对象中心。它们甚至不一定在边界框内，例如图9中行人的边界框。我们统计了nuScenes验证集上查询体素的分布，见表7。 在推理过程中，我们通过使用稀疏最大池化来避免NMS后处理，因为特征足够稀疏。类似于子流形稀疏卷积[19]，它只作用于非空位置。这是基于预测分数，并针对每个类单独进行的。我们采用稀疏最大池化来选择具有空间局部最大值的体素。移除的体素将被排除在框预测中，从而节省了头部的计算。 **框回归：**Bounding boxes直接从正的或选择的稀疏体素特征v∈Rn×F中回归。按照 CenterPoint [57] 中的协议，我们回归位置 (Δx, Δy) ∈ R 2 、高度 h ∈ R、3D 尺寸 s ∈ R 3 和旋转角度 (sin(α), cos(α)) ∈ R 2 。对于 nuScenes 数据集或跟踪，我们根据任务定义对速度 v ∈ R 2 进行回归。这些预测在训练过程中受到 L1 损失函数的监督。对于 Waymo 数据集，我们还预测 IoU 并使用 IoU 损失进行训练以提高性能 [22]。我们简单地使用全连接层或核大小为 3 的 3×3 子流形稀疏卷积层进行预测，没有其他复杂的设计。我们发现 3 × 3 稀疏卷积产生比全连接层更好的结果，负担有限，如表6. ","date":"2023-05-08","objectID":"/zh-cn/mot/:20:2","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"3.3. 3D追踪 我们的框架很自然地扩展到 3D 跟踪。CenterPoint [57] 通过二维速度 v ∈ R 2 跟踪预测的对象中心，该速度也受 L1 损失的监督。我们将此设计扩展到 VoxelNeXt。我们的方法是使用体素关联来包含更多与查询体素位置匹配的轨迹。 如图 8 所示，我们记录了用于预测每个框的体素的位置。与中心关联类似，我们计算用于匹配的 L2 距离。查询位置是通过将它们的索引回溯到原始输入体素而不是 stride-8 位置来选择的。跟踪体素存在于输入数据中，其偏差小于预测中心。此外，相邻帧之间的查询体素与框共享相似的相对位置。我们凭经验表明，体素关联改善了 Tab 中的跟踪。 11. ","date":"2023-05-08","objectID":"/zh-cn/mot/:20:3","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4. Experiments ","date":"2023-05-08","objectID":"/zh-cn/mot/:21:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4.1.消融研究 **额外的下采样层：**我们消除了 VoxelNeXt 中下采样层的影响。我们将其扩展到变体 Ds。 s 表示下采样的次数。例如，D3 与基础模型具有相同的网络步幅（3 倍）。我们的修改不会改变检测头的分辨率。这些模型的结果显示在表2中。 没有密集的头部，D3 性能下降严重，尤其是在 Truck 和 Bus 的大型物体上。从D3到D5，性能逐渐提升。额外的下采样层补偿感受野。额外的下采样层补偿感受野。为了验证这一点，我们添加了一个变体，D 5×5×5 3 ，这将所有阶段的稀疏卷积的内核大小增加到 5 × 5 × 5。大内核在一定程度上提高了性能但降低了效率。因此，我们使用额外的下采样作为简单的解决方案。 空间体素修剪： VoxelNeXt 根据特征大小逐渐删除冗余体素。我们在表3中取消了此设置。我们控制掉落比例从0.1到0.9，间隔0.2。当比率不大于 0.5 时，性能几乎不会衰减。因此，我们在实验中将下降率设置为 0.5 作为默认设置。我们还在表4中消融了体素修剪的阶段。我们默认在前3个阶段使用它。 **稀疏高度压缩：**我们在 2D 和 3D 的稀疏 CNN 类型上进行消融，在 VoxelNeXt 的骨干和头部，在表5中，naive 设计是backbone 和head 都应用了3D sparse CNN，导致高延迟。通过稀疏高度压缩，我们结合了 3D 骨干和 2D 稀疏预测头。它以良好的性能实现了更高的效率。我们将其用作 VoxelNeXt 的默认设置。当我们使用 2D 稀疏 CNN 作为骨干网络时，它具有与 3D 相同的层数和双通道。它实现了最佳效率，但性能有所下降。由于其高效率，我们将其命名为 VoxelNeXt-2D。 **稀疏预测头中的层类型：**我们使用全连接层或子流形稀疏卷积消除效果，以预测稀疏头中的框，如表6所示。全连接 (FC) 头的性能不如 3 × 3 稀疏卷积对应物，但效率更高。我们在 VoxelNeXt 中用 K3 表示后者。 **体素和预测框之间的相对位置：**在 VoxelNeXt 中，框预测的体素不需要在框内，更不用说像 表7中的中心了。我们计算它们生成的 3D 边界框内的体素的相关性。根据体素与框的相对位置，我们将体素分为近中心、近边界和外框 3 种区域类型。平均而言，大多数框是从内部的体素预测的，可能不在中心附近。统计上，只有少数框（总共不到 10%）是基于对象中心附近的体素预测的。这一发现表明边界体素也有资格进行预测，而对象中心并不总是必要的。 另一个观察结果是不同类别的比率之间存在很大差距。对于汽车和拖车，大多数框都是在内部体素上预测的。相比之下，对于卡车、交通锥和行人，大约一半的框是从外部体素预测的。我们在图 9 中说明了示例对。由于不同类别中的对象大小不同和空间稀疏性，预测体素符合数据分布，而不是像锚点或中心这样的代理。 **误差分析中与 CenterPoint 的比较：**我们将 VoxelNeXt 与表8中具有代表性的密集头方法 CenterPoint [57] 进行了比较。 在 1/4 nuScenes 训练集上训练并在完整的验证拆分上进行评估，VoxelNeXt 实现了 0.9% mAP 和 1.0% NDS 改进。在进一步分析中，CenterPoint 和 VoxelNeXt 在位置、大小和速度方面存在类似的错误。但是，在其他错误类型中存在较大差距，尤其是在定向方面。值得注意的是，VoxelNext 的方向误差比 CenterPoint 少 4.9%。我们假设这是由于稀疏的体素特征可能对方向差异更敏感。 **骨干网络效率统计：**我们在表9中计算了稀疏 CNN 主干网络的效率相关统计数据。由于将最后 3 个阶段的特征相加进行高度压缩，因此它们共享相同的通道号 128。由于第 5-6 阶段的高下采样率，它们的体素数与前几个阶段相比要小得多。因此，阶段 5-6 中引入的计算成本限制为 6 和 3 毫秒内的 6.1G 和 2.8G FLOP。它不超过整个骨干网的1/3，却对性能提升有显着影响。 **稀疏最大池化：**我们消除了表10中稀疏最大池化和 NMS 的影响。与常用的 NMS 相比，max-pool 的 mAP 相当，56.0% v.s.56.2%。 VoxelNeXt 可以灵活地与 NMS 或稀疏最大池化一起使用。 Max-pool 是一个优雅的解决方案，避免了一些不必要的预测计算。 **3D 跟踪体素关联：**表11 显示了 nuScenes 验证中 3D 跟踪的消融。除了跟踪预测框中心外，我们还包括预测匹配框的体素。体素关联引入了 1.1% AMOTA 的显着改进。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:21:1","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"4.2.主要结果 **3D 对象检测：**表12中，我们在测试拆分上评估我们的检测模型，并将它们与 nuScenes 测试集上的其他基于 LIDAR 的方法进行比较。表示为 † [13,28,40] 的结果报告了双翻转测试增强 [57]。两条线的结果都比之前的要好。我们在 Tab 中的 Waymo 验证拆分上将 VoxelNeXt 与其他 3D 对象检测器进行了比较。 15 和表中的 Argoverse2 [52]。 16. 我们在 Tab 中展示了延迟比较。如图 12 和图 3 所示。VoxelNeXt 在这些方法中以高效率实现了领先的性能。 **3D 多目标跟踪：**表13 和选项卡。 14，我们将 VoxelNeXt 的跟踪性能与 nuScenes 测试和验证拆分中的其他方法进行了比较。 VoxelNeXt 在所有基于激光雷达的方法中实现了最好的 AMOTA。此外，结合表中的双翻转测试结果。 12，在表中表示为†。 13日，VoxelNeXt进一步达到71.0%的AMOTA，在nuScenes 3D激光雷达跟踪基准测试中排名第一。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:21:2","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"5. Conclusion and Discussion 在本文中，我们提出了一个用于 3D 对象检测和跟踪的完全稀疏和基于体素的框架。它采用简单的技术，运行速度快，没有太多额外成本，并且以优雅的方式工作，无需 NMS 后处理。我们首次表明基于直接体素的预测是可行且有效的。因此，基于规则的方案，例如锚点或中心，以及密集的头部在我们的方案中变得不必要。 VoxelNeXt 在包括 nuScenes [3]、Waymo [45] 和 Argoverse2 [52] 在内的大规模数据集上展示了有希望的结果。凭借高效率，它在 3D 对象检测方面取得了领先的性能，并在 nuScenes 3D 跟踪 LIDAR 基准测试中排名第一。 **局限性：**理论 FLOP 和实际推理速度之间存在差距。与 CenterPoint [57] 的 186.6G 相比，VoxelNeXt 的 FLOPs 小得多，只有 38.7G。实际延迟减少很明显，但没有表1中的 FLOPs 大，因为它高度依赖于实现和设备。 ","date":"2023-05-08","objectID":"/zh-cn/mot/:22:0","tags":[],"title":"MOT","uri":"/zh-cn/mot/"},{"categories":[],"content":"ERASOR：基于自我中心比率的伪占用的动态对象移除静态3D点云地图构建 摘要：城市环境的扫描数据通常包括动态对象，例如车辆、行人等。然而，在构建具有顺序累积扫描数据的 3D 点云地图时，动态对象通常会在地图中留下不需要的痕迹。这些动态物体的痕迹作为障碍物，阻碍移动车辆实现良好的定位和导航。为了解决这个问题，本文提出了一种新的静态地图构建方法，称为 ERASOR，基于自我中心比率的伪占用的动态对象移除，它对运动模糊性快速且鲁棒。我们的方法将注意力集中在城市环境中大多数动态物体不可避免地与地面接触的性质上。因此，我们提出了一种称为伪占用的新概念来表达单位空间的占用，然后区分不同占用的空间。最后，采用 Region-wise Ground Plane Fitting (R-GPF) 来区分候选箱中可能包含动态点的静态点和动态点。正如在 SemanticKITTI 上进行的实验验证的那样，我们提出的方法相对于最先进的方法产生了有希望的性能，克服了现有的基于光线追踪和基于可见性的方法的局限性。 索引词——建图；距离感应 ","date":"2022-07-08","objectID":"/zh-cn/erasor/:0:0","tags":[],"title":"ERASOR-译","uri":"/zh-cn/erasor/"},{"categories":[],"content":"I. 介绍与相关工作 对于大多数移动平台，如无人地面车辆 (UGV)、无人机 (UAVs) 或自动驾驶汽车，恢复点云地图对于实现长期自主性至关重要 [1]-[10]。机器人可以利用地图提供的相邻几何结构的静态信息来实现成功的定位或导航。该地图可以用许多不同的形式表示，例如基于特征图的表示 [11]、基于占用的表示 [10]、[12] 或结构方差稳健的地点表示 [3]。本文将地图的范围限制为 3D 点云地图，该地图是由三维 LiDAR 传感器获得的激光扫描的累积生成的。 不幸的是，由于扫描数据呈现了周围环境的快照，因此城市环境中的扫描数据不可避免地包括动态对象的表示，例如车辆、行人等 [13]-[16]。此外，由于 3D 点云图是扫描数据顺序积累的产物，因此可能存在动态物体的痕迹，或鬼影痕迹效应 [8]、[13]，如左图 1 所示。这些动态对象的痕迹在地图中充当障碍物，从而阻碍移动机器人进行良好的定位和导航。 解决这个问题的常规方法主要分为两类：a）在地图生成时拒绝动态对象[16]，[17]；b）使用完整的生成地图集拒绝动态对象[7 ]、[8]、[15]。本文提出的方法侧重于后一种情况。因此，后一类可以进一步分为三种主要方法：a）基于分割的方法，b）基于光线追踪的方法，以及 c）基于可见性的方法。 首先，基于分割的方法通常基于聚类[16]、[18]、[19]。 Litomisky 和 Bhanu [18] 通过使用视点特征直方图 (VFH) ，以及 Yin 等人在静态集群中区分动态集群。 [19]利用特征匹配来检查错误的对应关系，然后将它们作为种子从动态对象中提取集群。尹恩等人[16]提出了一种基于区域增长的方法。 此外，基于深度学习的语义分割或检测已经导致性能的额外显着提高[20]。给定具有动态标签的正确分割点，可以通过排除预测的动态点 [15]、[21] 直接构建地图。然而，现有的基于分割的方法目前只关注动态点拒绝，而不是显示最终的静态地图。此外，这些都严重依赖于监督标签；因此，它们容易受到标记错误或未标记类的动态对象的影响[22]。为了缓解这个问题，有必要考虑扫描到地图的关系。 Occupancy grid [17] 和 Octomap [12] 是使用光线追踪的典型方法，它们计算 gridmap 空间中扫描的命中和未命中并决定空间的占用。通过扩展，Schauer 和 Nüchter [14] 提出通过遍历体素占用网格和 Pagad 等人来移除动态点。 [13] 提出了对象检测和 Octomap 的组合。然而，基于光线追踪的方法计算成本很高，这导致引入了基于可见性的方法来降低计算成本[8]、[10]、[15]。然而，这种假设存在入射角模糊性；随着从地面的距离测量变得更长，一个点的入射角变得更加模糊，因此相邻的地面点可能被错误地视为动态物体（图2（a））。为了解决这个问题，Pomerleau 等人。 [8] 利用贝叶斯方法，使用法线和入射角来表示每个点的状态。通过扩展，Kim 和 Kim [15] 提出了一种像素到窗口的比较方法，以考虑入射角模糊性。然而，之前的所有工作都在努力解决遮挡问题（图 2（b）），一旦在动态点后面没有观察到静态点，就不可能去除巨大动态对象的动态点（图 2（c）） . 在本文中，我们提出了一种新的静态地图构建方法，称为基于 pSeudo Occupancy 的以自我为中心的比率动态对象移除 (ERASOR) ，以克服上述方法的局限性。我们的方法是一种无可见性的方法，并利用了我们提出的在垂直列中组织的点的表示，称为伪占用。本文的贡献有四个： 我们提出了一种称为扫描比测试 (SRT) 的快速且稳健的方法，以根据城市环境中大多数动态对象（例如陆地车辆和行人）不可避免地与地面接触的性质来获取包含动态点的箱。 提取箱后，我们应用区域级地平面拟合 (R-GPF)，这是一种具有低计算负载的新型静态点检索方法。 R-GPF 克服了基于光线追踪的方法和基于可见性的方法的潜在局限性，详细信息如图 2 所示。 对其他最先进的方法进行了验证。据我们所知，这是在 SemanticKITTI [5] 上与其他最先进的方法进行定量比较的第一次尝试。特别是，我们指出了精度/召回指标的局限性，并提出了适用于静态地图构建任务的替代指标：保留率和拒绝率。 我们提出的方法显示出优于最先进方法的有希望的性能。特别是 ERASOR 以最少的静态点损失去除动态点，并且比最先进的方法至少快十倍。 本文的其余部分组织如下：第二节解释了伪占用的概念和我们提出的方法。第三节描述了实验和新的误差度量，第四节讨论了实验结果。最后，第五节总结了我们的贡献并描述了未来的工作。 ","date":"2022-07-08","objectID":"/zh-cn/erasor/:1:0","tags":[],"title":"ERASOR-译","uri":"/zh-cn/erasor/"},{"categories":[],"content":"II. 通过 ERASOR 构建静态地图 我们提出的静态地图构建方法的示意图如图 3 所示。以下段落重点介绍了 ERASOR 每个模块背后的问题定义和推理。请注意，本文主要关注静态地图构建的实例级动态对象拒绝。也就是说，城市环境的大规模变化，例如建筑物的重建或修复，超出了这项工作的范围 ","date":"2022-07-08","objectID":"/zh-cn/erasor/:2:0","tags":[],"title":"ERASOR-译","uri":"/zh-cn/erasor/"},{"categories":[],"content":"A. 问题定义 令 $\\mathcal{P_t} = \\lbrace\\mathbf{p}_1, \\mathbf{p}_2, . . . , \\mathbf{p}_n\\rbrace$ 是在时间步 $t$ 包含 $n$ 个点的一组点云，其中每个点在笛卡尔坐标中表示为 $\\mathbf{p}_k = \\lbrace x_k, y_k, z_k\\rbrace$。令 $\\mathcal{P}_t^Q$为查询帧上的查询点云，${ }_{Q}^{W} \\mathbf{T}_{t}$为与$\\mathcal{P}_t^Q$相关联的 $SE(3)$ 位姿。在本文中，假设姿势集是通过配准优化或校正后给出的。通过让${ }_{Q}^{W} \\mathbf{T}_{t} * \\mathcal{P}_t^Q$成为 $\\mathcal{P}_t^Q$ 在世界框架 $W $中的变换表示，使用一组原始 LiDAR 扫描构建的先验地图 $\\mathcal{M}$ 可以表示如下： $$ \\mathcal{M}=\\bigcup_{t \\in[T]}{ }_{Q}^{W} \\mathbf{T}_{t} * \\mathcal{P}_{t}^{Q} $$ 其中 $T$ 是总时间步长，$[T ]$ 等于 $\\lbrace1, 2, . . . , T - 1, T\\rbrace$。请注意，$\\mathcal{M}$ 在世界框架上，包含所有测量的动态点。 接下来，设 $\\mathcal{P}_t^M$ 为 $\\mathcal{M}$ 的子图相对于查询点云坐标表示，这等于 $\\mathcal{M}$ 的子图通过${ }_{W}^{Q} \\mathbf{T}_{t} = { }_{Q}^{W} \\mathbf{T}_{t} ^{-1}$变换。在本文中，上标 $Q$ 和 $\\mathcal{M}$ 分别用于表示查询和地图。通过定义 $\\hat{\\mathcal{M}}$ 是估计的静态图，我们感兴趣的问题定义如下： $$ \\hat{\\mathcal{M}}=\\mathcal{M} - \\bigcup_{t \\in[T]} \\hat{\\mathcal{M}}_{dyn,t} $$ 其中 $\\hat{\\mathcal{M}}_{dyn,t}$ 是指由 $\\mathcal{P}_t^Q$ 和 $\\mathcal{P}_t^M$ 之间的差异确定的估计动态点。 在介绍我们的无可见性方法之前，必须对单位空间的各个方面进行解释。我们的方法将注意力集中在城市环境中大多数动态物体的性质上，例如地面车辆和行人，它们不可避免地与地面接触。基于这个假设，我们可以构建四种可能的情况： 在$\\mathcal{P}_t^M$中有物体在地面上，而在$\\mathcal{P}_t^Q$中同一位置的地面没有物体。 在$\\mathcal{P}_t^M$中地面上没有物体，而在$\\mathcal{P}_t^Q$中相同位置的地面上有物体。 在$\\mathcal{P}_t^M$和$\\mathcal{P}_t^Q$中都有一个物体在地面上。 在$\\mathcal{P}_t^M$和$\\mathcal{P}_t^Q$中地面上没有物体。 在这些情况中，表示动态对象的点将被分类为第一种情况和第二种情况。但是，我们的目标是细化地图云，因此忽略了第二种情况。为了清楚起见，我们定义了两个术语；第一种情况可能是动态情况，而第三和第四种情况肯定是静态情况。 因此，我们的目标可以解释为检测 $\\mathcal{P}_t^M$ 中满足潜在动态情况的单位空间，并检索包含动态点的区域（参见第 II.D 节）。然后可以通过删除动态部分来细化单位空间集（参见第 II.E 节）。 ","date":"2022-07-08","objectID":"/zh-cn/erasor/:2:1","tags":[],"title":"ERASOR-译","uri":"/zh-cn/erasor/"},{"categories":[],"content":"B. 感兴趣的空间 在 $\\mathcal{P}_t^M$和 $\\mathcal{P}_t^Q$ 被分离到单位空间之前，点空间的物理有效域或感兴趣体积 $ (VOI, \\mathcal{V}{t})$ 定义和表述如下： $$ \\mathcal{V}{t}=\\left{\\mathbf{p}{k} \\mid \\mathbf{p}{k} \\in \\mathcal{P}{t}, \\rho{k}\u003cL_{\\max }, h_{\\min }\u003cz_{k}\u003ch_{\\max }\\right} $$ 其中 $ρ_k = px2k + y2k$； $Lmax$、$hmin$ 和 $hmax$ 是常数参数，分别表示最大径向边界、VOI 的最小高度和 VOI 相对于地面的最大高度。我们感兴趣的动态对象（例如车辆或行人）的高度通常落在合理的范围内 [6]、[7]。因此，我们可以设置 hmax = 3.0m，hmin = -1.0m 和 Lmax = 80.0m。请注意，最小值hmin = -1.0m，低于地面，即h = 0m，设置为覆盖倾斜区域以及检测到的地面高度的不确定性。 请注意，$VMt$ 是通过邻居搜索从 M 中提取的，即 K-D 树 [23]，关于查询帧的位置，然后通过 $WQ Tt -1$ 进行变换以降低计算成本。这是因为 M 主要由超过一百万个点组成；因此，将所有点转换为查询帧然后提取 $VMt$ 是不划算的。 ","date":"2022-07-08","objectID":"/zh-cn/erasor/:2:2","tags":[],"title":"ERASOR-译","uri":"/zh-cn/erasor/"},{"categories":[],"content":"C. 按区域划分的伪占用描述符 对于区域级动态对象移除，VOI 由称为区域级伪占用描述符 (R-POD) 的以自我为中心的空间占用描述符封装。它的设计受到感兴趣细胞 (COI) [7] 和扫描上下文 [6] 的启发。 金等人。 [7] 提出 COI 将点云中的垂直信息编码为二进制位。然而，COI 使用 RGB 格式并且不适合 LiDAR [3]。另一方面，Scan Context [6]、[24] 从每个 bin 中获取最大高度，并以自我为中心的方式将它们排列成 2D 矩阵。检查扫描到扫描的关系是有效的，但是编码z向信息的绝对最大值来检查扫描到地图的关系可能是有风险的。这是因为来自 LiDAR 里程计或 LiDAR SLAM 的姿势 z 方向信息通常比 x 和 y 方向信息更不确定 [25]。 为此，我们的 R-POD 通过以自我为中心的方式定义一个表示占用率与垂直信息的边界差异或伪占用率的垂直箱，从而融合了两种方法的优点。与 Scan Context 类似，RPOD 采用 Vt 并将体积划分为方位角和径向方向的规则间隔，即扇区和环。令 Nr 和 Nθ 为环数和扇区数。则表示为 St 的 R-POD 可以表示如下： $$ \\mathcal{S}{t}=\\bigcup{i \\in\\left[N_{r}\\right], j \\in\\left[N_{\\theta}\\right]} \\mathcal{S}_{(i, j), t} $$ 其中 S(i,j),t 表示 R-POD 在时间步 t 的第 (i, j) 个 bin。令 θ = arctan 2(y, x)。然后，每个 S(i,j),t 由满足以下条件的浊点组成。 $$ \\begin{aligned} \\mathcal{S}_{(i, j), t}=\\left{\\mathbf{p}_{k} \\mid \\mathbf{p}_{k} \\in \\mathcal{V}_{t}, \\frac{(i-1) \\cdot L_{\\max }}{N_{r}} \\leq \\rho_{k}\u003c\\frac{i \\cdot L_{\\max }}{N_{r}}\\right.\\ \\left.\\frac{(j-1) \\cdot 2 \\pi}{N_{\\theta}}-\\pi \\leq \\theta_{k}\u003c\\frac{j \\cdot 2 \\pi}{N_{\\theta}}-\\pi\\right} \\end{aligned} $$ 此后，单位空间，即每个 bin，分配一个实数值来描述伪占用，Δh(i,j),t。令 Z(i,j),t = {zk ∈ pk|pk ∈ S(i,j),t}。然后，每个bin的伪占用编码如下： $$ \\Delta h_{(i, j), t}=\\sup \\left{Z_{(i, j), t}\\right}-\\inf \\left{Z_{(i, j), t}\\right} $$ 其中 sup 和 inf 分别表示最高和最低。 ","date":"2022-07-08","objectID":"/zh-cn/erasor/:2:3","tags":[],"title":"ERASOR-译","uri":"/zh-cn/erasor/"},{"categories":[],"content":"D. 扫描率测试 提出了扫描比率测试（SRT）来检查给定一对查询和地图点云的 R-POD，即 SQ t 和 SMt 是否出现伪占用差异。 SRT 的灵感来自于尺度不变特征变换 (SIFT) [26] 中的 Lowe 比率测试。与基于全局阈值的方法[26]相比，基于比率的方法被证明对场景变化更稳健。由于这一优势及其普遍性，我们可以在比较伪入住率时将比率测试的概念应用到我们的方法中。 让术语扫描比率是 SQ t 和 SMt 中每对 bin 之间的伪占用率，即 ΔhQ (i,j),t 和 ΔhM(i,j),t 之间的比率。然后使用扫描比率将 bin 分类为上述情况，并选择属于潜在动态情况的 bin，其中任一 bin 都包含我们感兴趣的动态点。直截了当，如果两个 bin 都没有变化，即绝对静态情况，则扫描比率必须接近 1，而属于潜在动态情况的扫描比率必须比1小得多，如第 II.A 节所述，这是由地面上存在动态物体引起的。 因此，我们选择扫描比率小于比率阈值的 bin 作为可能包含动态对象或潜在动态 bin 的 bin。图 4(a) 说明了扫描比率的概率分布函数 (PDF)，图 4(b) 描述了使用 SemanticKITTI 数据集的浊点数量的直方图（参见第 III.A 节）。一组包含查询或地图中的动态点的 bin 往往具有较小的扫描率。相比之下，一组只有静态点的 bin 显示的扫描率接近 1。 基于这些观察，比率阈值设置为 0.2，根据经验确定该阈值足够严格。图 3 说明了从 SRT 检索到的结果。 SRT 上的绿色 bin 是潜在的动态 bin，蓝绿色 bin 是扫描比率超过 0.2 的 bin。红色区域是确定性静态区域或已清理区域，即 ΔhM(i,j),t/ΔhQ (i,j),t \u003c 0.2。请注意，如果地图或查询箱包含少量点，则跳过 SRT，在图 3 中的 SRT 上这些点以蓝色显示。 ","date":"2022-07-08","objectID":"/zh-cn/erasor/:2:4","tags":[],"title":"ERASOR-译","uri":"/zh-cn/erasor/"},{"categories":[],"content":"E. 区域级地平面拟合 根据第 II.A 节中的假设，可以观察到许多潜在的动态 bin 仅由地面点和动态点组成。因此，我们可能会想到计算要求不高的静态点检索方法。 地平面的 Bin-wise 估计优于对整个地图的单平面模型的估计，因为许多城市地区包括不形成完美平面的路缘或倾斜区域的地面 [27]。出于这个原因，采用了bin-wise地平面拟合，这是由bin-wise line拟合来寻找地面点的[28]。因为每个 bin 相对于整个地图来说都很小，所以可以安全地假设 bin 内的地面是平面的。因此，R-GPF 可以在非平面的城市特定区域上稳健地选择更多静态点并将其还原回地图（参见第 IV.B 节）。 假设第 $l$ 个 bin，即 SMl,t，是在整个 SRT 中的 Nr × Nθ 个 bin 中选择总 L 个潜在动态 bin 的候选者。因为每个 bin 中高度最低的点最有可能属于地表[28]，最低高度的种子点从 SMl,t 中选择，令 ¯z 为所选种子点的平均 z 值。然后，得到初始估计的地面点集 0Il,t 如下： $$ { }^{0} \\mathcal{I}{l, t}=\\left{\\mathbf{p}{k} \\mid \\mathbf{p}{k} \\in \\mathcal{S}{l, t}^{M}, z\\left(\\mathbf{p}{k}\\right)\u003c\\bar{z}+\\tau{\\text {seed }}\\right} $$ 其中 z(·) 表示一个点的 z 值，τseed 表示高度边距。因为我们的方法是迭代的，设第 i 个内点为 iIl,t，则 iIl,t 的协方差矩阵 iCl,t 计算如下： $$ { }^{i} C_{l, t}=\\sum_{j=1:\\left|{ }^{i} \\mathcal{I}_{l, t}\\right|}\\left(\\mathbf{p}_{j}-{ }^{i} \\overline{\\mathbf{p}}_{l, t}\\right)\\left(\\mathbf{p}_{j}-{ }^{i} \\overline{\\mathbf{p}}_{l, t}\\right)^{T} $$ 在哪里 | · |和 i¯pl,t 分别表示集合的大小和 iIl,t 的平均位置。 接下来，使用主成分分析（PCA），计算三个特征值和对应的三个特征向量，即 iCl,t~ vm = λm~ vm 其中 m = 1, 2, 3。然后，具有最小特征值的特征向量最大可能代表地面的法线向量。让特征向量为 inl,t = [ial,t ibl,t icl,t]T 。那么，平面系数可以计算为 idl,t = −inTl,ti¯pl,t，其平面方程为 ial,tx+ ibl,ty + icl,tz + idl,t = 0。最后，我们的目标是提取平面下方的潜在静态点如下： $$ { }^{i+1} \\mathcal{I}{l, t}=\\left{\\mathbf{p}{k} \\mid \\mathbf{p}{k} \\in \\mathcal{S}{l, t}^{M},{ }^{i} d_{l, t}-{ }^{i} \\hat{d}_{k}\u003c\\tau_{g}\\right} $$ 其中 i ^dk = −inTl,tpk 和 τg 表示平面的距离余量。该过程重复 3 次，之后将最后的 3Il,t 恢复到拒绝动态点的地图中。因此，我们方法拒绝的动态点可以直接检索如下： $$ \\hat{\\mathcal{M}}_{d y n, t}=\\bigcup_{l \\in[L]}\\left(\\mathcal{S}_{l, t}^{M}-{ }^{3} \\mathcal{I}_{l, t}\\right) $$ 图 3 简要介绍了 R-GPF 的过程。 R-GPF 上的红点代表动态点，绿点代表恢复到地图云中的地面点。 ","date":"2022-07-08","objectID":"/zh-cn/erasor/:2:5","tags":[],"title":"ERASOR-译","uri":"/zh-cn/erasor/"},{"categories":["论文"],"content":"SVo论文翻译","date":"2021-11-16","objectID":"/zh-cn/svo/","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"SVO：快速的半直接单目视觉里程计 摘要：我们提出了一种半直接单目视觉里程计算法，它比当前最先进的方法精确、稳健且速度更快。半直接法消除了运动估计中昂贵的特征提取和匹配技术的需要，直接在像素级上操作，做到了在高帧率下的亚像素精度（subpixel precision），以及采用概率建图（probabilistic mapping）方法对异常测量值进行显式建模，从而得到较少的异常值和更可靠的点。精确的高帧率运动估计在纹理较少、重复性和高频的场景中具有更强的鲁棒性。该算法应用于GPS无效环境下的无人机状态估计，在嵌入式计算机上以每秒55帧的速度运行，在消费型笔记本电脑上以每秒300帧以上的速度运行。我们将我们的方法称为SVO（半直接视觉里程计）。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:0:0","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"I. 引言  微型飞行器 (MAV) 很快将在灾害管理、工业检测和环境保护方面发挥重要作用。对于此类操作，仅基于 GPS 信息进行导航是不够的。 精确的完全自主操作需要 MAV 依赖替代定位系统。 因此，为了最小的重量和功耗，[1]-[5]建议仅使用一个向下看的相机与惯性测量单元结合使用。这种设置允许在室外区域完全自主的航点跟踪[1]-[3]以及 MAV 和地面机器人之间的协作[4] [5]。  据我们所知，所有用于 MAV 的单目视觉里程计 (VO) 系统[1] [2] [6] [7]都是基于特征的。然而，在 RGB-D 和基于立体的 SLAM 系统中，基于光度误差最小化的直接方法 [8]-[11]正变得越来越流行。  在这项工作中，我们提出了一种半直接 VO，它将基于特征的方法（跟踪许多特征、并行跟踪和映射、关键帧选择）的成功因素与直接方法的准确性和速度相结合。 MAV 的高帧率 VO 有望提高稳健性和更快的飞行机动。  这项工作的开源实现和视频可在以下网址获得：http://rpg.ifi.uzh.ch/software。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:1:0","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"A. 视觉运动估计方法的分类  同时从视频中恢复相机姿态和场景结构的方法可以分为两类:  a) 特征法：标准方法是在每个图像中提取一组稀疏的显着图像特征（例如点、线）；使用不变特征描述符在连续帧中匹配它们；使用对极几何稳健地恢复相机运动和结构；最后，通过重投影误差最小化来细化姿势和结构。大多数 VO 算法[12]遵循此程序，独立于应用的优化框架。这些方法成功的一个原因是强大的特征检测器和描述符的可用性，即使在较大的帧间移动时，它们也允许图像之间的匹配。基于特征的方法的缺点是依赖检测和匹配阈值，需要稳健的估计技术来处理错误的对应关系，以及大多数特征检测器针对速度而不是精度进行优化的事实，这样运动估计的漂移 必须通过对许多特征测量进行平均来补偿。  a) 直接法：直接方法[13]直接从图像中的光强值估计结构和运动。与仅考虑到某些特征位置的距离的基于特征，局部强度梯度幅度和方向用于优化的方法相比。直接方法利用图像中所有信息，即使来自梯度很小的区域，且已被证明在具有很少纹理的场景[14]或在相机散焦和运动模糊的情况下的鲁棒性方面优于基于特征的方法[15]。光度误差的计算比重投影误差更密集，因为它涉及扭曲和整合大图像区域。 然而，由于直接方法直接对图像的强度值进行操作，因此可以节省特征检测和不变描述符计算的时间。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:1:1","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"B. 相关工作  与[1] [2] [7]相比，大多数用于 MAV 的单目VO算法都依赖于PTAM[16]。PTAM是一种基于特征的SLAM算法，通过跟踪和映射许多（数百）特征来实现鲁棒性。同时，它通过并行运动估计和映射任务，并依靠高效的基于关键帧的束调整（BA）实时运行[17]。然而，PTAM是为小型桌面场景中的增强现实应用而设计的，需要进行多次修改（例如，限制关键帧的数量），以允许在大规模室外环境中操作[2]。  早期的直接单目SLAM方法跟踪并绘制了少数有时手动选择的平面面片[18]–[21]。第一种方法[18] [19]使用滤波算法来估计结构和运动，而后来的方法[20]–[22]使用非线性最小二乘优化。所有这些方法都是估计补丁的表面法线，这允许在广泛的视点范围内跟踪面片，从而大大减少估计中的漂移。文章[19]–[21]的作者实现了实时性能，但是，只有少数选定的平面区域和小数据集具有实时性能。文章[22]中提出了一种用于车载全向摄像机的VO算法。在[8]中，放松了局部平面性假设，并提出了针对立体相机计算的任意三维结构的直接跟踪。在[9]-[11]中，同样的方法也适用于RGB-D传感器。在DTAM[15]中，引入了一种新的直接方法，该方法通过最小化全局空间正则化能量泛函数来计算每个关键帧的密集深度图。通过使用深度贴图直接对齐整个图像，可以找到相机的姿势。这种方法的计算强度非常高，只有通过GPU的大量并行才能实现。为了减少计算需求，文章[23]中描述的方法，在本工作的审查过程中发表，只使用强梯度的像素。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:1:2","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"C. 贡献与概要  我们提出的半直接视觉里程计（SVO）算法采用特征对应，然而特征响应是直接运动估计的隐式结果，而不是显式特征提取和匹配的结果。因此只有在选择关键帧初始化新的3D点时，才需要进行特征提取(见图1)。其优点是通过减少每帧的特征提取而提高了速度，通过亚像素特征对应提高了精度。与以前的直接方法相比，我们使用许多(数百)小块而不是少数(数十)大平面块[18]-[21]。使用许多小面片可以增强鲁棒性，并允许忽略面片法线。提出的用于运动估计的基于稀疏模型的图像对齐算法与基于模型的稠密图像对齐相关[8]–[10] [24]。然而，我们证明了稀疏的深度信息足以获得运动的粗略估计并找到特征对应。一旦建立了特征对应和摄像机姿态的初始估计，该算法将继续只使用点特征，因此名称为“半直接”。这种转换允许我们依赖快速和建立的框架来调整BA（例如[25]）。  使用显式建模离群测量值的贝叶斯过滤器来估计特征位置处的深度。仅当相应的深度过滤器已收敛时，才在地图中插入3D点，这需要多次测量。结果是一张地图，只有很少的异常值以及很多可以可靠跟踪的点。  本文的贡献是: 一种新的半直接VO方法，它比目前最先进的 MAV 更快、更准确; 集成了一种概率映射方法，对异常值测量具有鲁棒性。  第二节概述了方法，第三节介绍了一些必要的符号，第四节和第五节解释了提出的运动估计和映射算法，第七节提供了实验结果和比较。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:1:3","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"II. 系统概述  图1 提供了 SVO 的概述。该算法使用两个并行线程（如[16]），一个用于估计相机的运动，另一个用于在探索环境时进行映射。这种分离允许在一个线程中进行快速和恒定的时间跟踪，而第二个线程扩展映射，与硬实时约束解耦。  运动估计线程实现了所提出的相对姿态估计的半直接方法。第一步是通过基于 parse 模型的图像对齐进行姿势初始化：通过最小化与相同3D点的投影位置对应的像素之间的光度误差来找到相对于前一帧的相机姿态（见图2）。下一步通过对齐对应的特征片来细化重投影点对应的2D坐标（见图3）。运动估计通过最小化前面特征对齐步骤中引入的重投影误差来细化姿态和结构。  在建图线程中，为每个要估计相应3D点的2D特征初始化一个概率深度过滤器。只要在图像中很少发现3D到2D对应的区域中选择了新的关键帧，就会初始化新的深度过滤器。过滤器在深度上具有很大的不确定性。在随后的每一帧，深度估计都以贝叶斯方式更新（见图5）。当深度过滤器的不确定性足够小时，将在地图中插入一个新的3D点，并立即用于运动估计。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:2:0","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"III. 标记  在详细介绍算法之前，我们简要定义了本文中使用的符号。  在时间步长 $k$ 采集的强度图像用 $I_k$ 表示：$\\Omega \\subset \\mathbb{R}^2 \\mapsto \\mathbb{R}$，在这里 $\\Omega$ 是图像域。任意三维点$\\mathbf{p}=(x, y, z)^{\\top} \\in \\mathcal{S}$ 在可见场景曲面上 $\\mathcal{S}$ 通过摄像机投影模型$\\pi ：\\mathbb{R}^3 \\mapsto \\mathbb{R}^2$ 映射到图像坐标 $\\mathbf{u}=(u,v)^{\\top} \\in \\Omega$ ： $$ \\mathbf{u}=\\pi\\left({ }_{k} \\mathbf{p}\\right) {\\tag 1} $$ 其中 $k$ 表示点坐标是相机参考帧 $k$ 中的。投影 $\\pi$ 由通过校准已知的固有摄像机参数确定。给定逆投影函数 $\\pi^{-1}$ 和深度 $d_\\mathbf{u} \\in \\mathcal{R}$，可以恢复与图像坐标对应的3D点: $$ {}_k\\mathbf{p} = \\pi^{-1}(\\mathbf{u},d_\\mathbf{u}) {\\tag 2} $$ 其中 $\\mathcal{R} \\subseteq \\Omega$ 域是深度已知的。  在时间 $k$ 相机的位置与方向使用刚体变换 $T_{k,w} \\in SE(3)$ 表示。它允许我们将一个3D点从世界坐标系映射到相机坐标系：${}_k\\mathbf{p} = T_{k,w} · {}_w\\mathbf{p}$ 。两个连续帧之间的相对变换可用$T_{k,k−1}=T_{k,w}·T^{−1}_{k−1,w}$ 来计算。在优化过程中，我们需要一个最小的变换表示，因此，在恒等式处使用对应于切空间 $SE(3)$ 的李代数 $\\mathfrak{s e}(3)$。我们用 $\\xi =(ω，ν)^T \\in \\mathbb{R}^6$ 表示代数元素，也称为扭转坐标，式中ω称为角速度，ν称为线速度。扭转坐标 $\\xi$ 通过指数映射[26]映射到 $SE(3)$： $$ \\mathbf T(\\xi) = \\exp (\\hat{\\xi}) {\\tag 3} $$ ","date":"2021-11-16","objectID":"/zh-cn/svo/:3:0","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"IV. 运动估计  SVO使用直接方法计算相对摄像机运动和特征对应的初始猜测，并基于特征的非线性重投影误差细化。下面将详细介绍每个步骤，并在 图2 到 图4 中进行说明。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:4:0","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"A. 基于稀疏模型的图像对齐  两次连续的相机的位姿变换 $\\mathbf T_{k,k-1}$ 的最大似然估计是最小化强度残差的负对数似然： $$ \\mathbf{T}_{k, k-1}=\\arg \\min _{\\mathbf{T}} \\iint_{\\overline{\\mathcal{R}}} \\rho[\\delta I(\\mathbf{T}, \\mathbf{u})] d \\mathbf{u} {\\tag 4} $$ 定义强度残差 $\\delta I$ 为对同一3D点的像素之间光度差。它可以通过从之前的图像 $I_{k-1}$ 一个2D点反向投影，然后将其投影到当前的相机视图来计算： $$ \\delta I(\\mathbf{T}, \\mathbf{u})=I_{k}\\left(\\pi\\left(\\mathbf{T} \\cdot \\pi^{-1}\\left(\\mathbf{u}, d_{\\mathbf{u}}\\right)\\right)\\right)-I_{k-1}(\\mathbf{u}) \\quad \\forall \\mathbf{u} \\in \\overline{\\mathcal{R}} {\\tag 5} $$ 其中 $\\mathcal{R}$ 是指其深度 $d_{\\mathbf u}$ 在 $k-1$ 时间已知，且投影点在当前图像可见的图像域： $$ \\overline{\\mathcal{R}}=\\lbrace\\mathbf{u} \\mid \\mathbf{u} \\in \\mathcal{R}_{k-1} \\wedge \\pi\\lparen\\mathbf{T} \\cdot \\pi^{-1}\\lparen\\mathbf{u}, d_{\\mathbf{u}}\\rparen \\rparen \\in \\Omega_k \\rbrace {\\tag 6} $$ 为了简单起见，我们在下面假设强度残差是正态分布的，且具有单位方差。负对数似然极小值对应于最小二乘问题：$\\rho[.] \\hat{=} \\frac{1}{2}|\\cdot|^{2}$ 。实际上，由于遮挡，分布具有较重的尾部，因此必须应用稳健的成本函数[10]。  与以前的工作不同，在以前的工作中，图像中的大区域的深度是已知的[8]–[10] [24]，而我们这里只知道稀疏特征位置 $\\mathbf u_i$ 的深度 $d_\\mathbf {u_i}$。我们用矢量 $\\mathbf I(\\mathbf u_i)$ 表示特征点周围4×4像素的小块。我们试图找到使所有面片的光度误差最小化的相机位姿（见图2）： $$ \\mathbf{T}_{k, k-1}=\\arg \\min _{\\mathbf{T}_{k, k-1}} \\frac{1}{2} \\sum_{i \\in \\overline{\\mathcal{R}}}\\left|\\delta \\mathbf{I}\\left(\\mathbf{T}_{k, k-1}, \\mathbf{u}_{i}\\right)\\right|^{2} {\\tag 7} $$ 由于方程(7)对 $\\mathbf{T}_{k, k-1}$ 是非线性的，我们用迭代高斯-牛顿法求解它。给定一个相对变换的估计 $\\hat {\\mathbf{T}}_{k, k-1}$，该估计的增量更新 $\\mathbf T(\\xi)$ 可以用一个小量 $\\xi \\in \\mathfrak{s e}(3)$ 参数化。我们使用强度残差的逆复合公式[27]，计算时间 $k−1$ 时参考图像的更新步长 $\\mathbf T(\\xi)$: $$ \\delta \\mathbf{I}\\left(\\xi, \\mathbf{u}_{i}\\right)=\\mathbf{I}_{k}\\left(\\pi\\left(\\hat{\\mathbf{T}}_{k, k-1} \\cdot \\mathbf{p}_{i}\\right)\\right)-\\mathbf{I}_{k-1}\\left(\\pi\\left(\\mathbf{T}(\\xi) \\cdot \\mathbf{p}_{i}\\right)\\right) {\\tag 8} $$ 其中 $\\mathbf{p} = \\pi^{-1}(\\mathbf{u},d_\\mathbf{u})$。然后，使用等式（3）将更新步骤的倒数应用于当前估计： $$ \\hat {\\mathbf T}_{k,k-1} \\gets \\hat {\\mathbf T}_{k,k-1} \\cdot \\mathbf T(\\xi)^{-1} {\\tag 9} $$ 注意，我们不会因为计算速度的原因而扭曲补丁。这个假设在帧到帧的小运动和小补丁大小的情况下是有效的。  为了找到最优更新步长 $\\mathbf T(\\xi)$，我们计算(7)的导数并设为零: $$ \\sum_{i \\in \\overline{\\mathcal{R}}} \\nabla \\delta \\mathbf{I}\\left(\\xi, \\mathbf{u}_{i}\\right)^{\\top} \\delta \\mathbf{I}\\left(\\xi, \\mathbf{u}_{i}\\right)=0 $$ 为了解决这个系统，我们将当前状态线性化: $$ \\delta \\mathbf{I}\\left(\\xi, \\mathbf{u}_{i}\\right) \\approx \\delta \\mathbf{I}\\left(0, \\mathbf{u}_{i}\\right)+\\nabla \\delta \\mathbf{I}\\left(0, \\mathbf{u}_{i}\\right) \\cdot \\xi $$ 其中雅克比矩阵 $\\mathbf J_i :=\\nabla \\delta \\mathbf{I}\\left(0, \\mathbf{u}_{i}\\right) $ 具有16×6维度，因为面片大小为4×4，并用链式规则计算： $$ \\frac{\\partial \\delta \\mathbf{I}\\left(\\xi, \\mathbf{u}_{i}\\right)}{\\partial \\xi}=\\left.\\left.\\left.\\frac{\\partial \\mathbf{I}_{k-1}(\\mathbf{a})}{\\partial \\mathbf{a}}\\right|_{\\mathbf{a}=\\mathbf{u}_{i}} \\cdot \\frac{\\partial \\pi(\\mathbf{b})}{\\partial \\mathbf{b}}\\right|_{\\mathbf{b}=\\mathbf{p}_{i}} \\cdot \\frac{\\partial \\mathbf{T}(\\xi)}{\\partial \\xi}\\right|_{\\xi=0} \\cdot \\mathbf{p}_{i} $$ 通过将（11）插入（10）并将雅可比矩阵叠加在矩阵 $\\mathbf J$ 中，我们得到了正规方程： $$ \\mathbf J^{\\top} \\mathbf J \\xi = - \\mathbf J^{\\top} \\delta \\mathbf I(0) {\\tag {12}} $$ 这样可解决更新 $\\xi$。注意，通过使用逆合成方法，可以预先计算雅可比矩阵，因为它在所有迭代中保持恒定(参考面片 $\\mathbf I_{k-1} (\\mathbf u_i)$ 和点 $\\mathbf p_i$ 不变)，这将导致显著的加速[27]。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:4:1","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"B. 通过特征对齐实现松弛  下一步是将相机与前一帧对齐。通过反向投影，找到的相对位姿 $\\mathbf T_{k,k−1}$ 隐式地定义了新图像中所有可见3D点的特征位置的初始猜测。由于3D点的位置不准确，因此相机姿势不准确，因此可以改进这种初始猜测。为了减少漂移，相机的位姿应该与地图对齐，而不是与前一帧对齐。  从估计的相机位姿可以将看到的地图的所有3D点都投影到图像中，从而估计出相应的2D特征位置 $\\mathbf u_i^{\\prime}$（见图3）。对于每个重投影的点，将具有最近的观察角度识别为关键帧 $r$ 。然后，特征对齐步骤通过最小化当前图像中的补丁相对于关键帧器中的参考补丁的光度误差来单独优化新图像中的所有2D特征位置 $\\mathbf u_i$： $$ \\mathbf u_i^{\\prime}=\\arg \\min _{\\mathbf u_i^{\\prime}} \\frac{1}{2}\\lVert\\mathbf{I}_{k}(\\mathbf u_i^{\\prime})-\\mathbf A_i \\cdot \\mathbf I_r(\\mathbf u_i)\\rVert^{2}, \\quad \\forall i {\\tag{13}} $$ 这种对齐是用逆合成 Lucas-Kanade 算法[27]来解决的。与前面的步骤相反，我们对参考补丁应用了一个仿射扭曲 $\\mathbf A_i$，因为最近的关键帧通常比前一张图像更远，所以使用了更大的补丁大小（8×8像素）。  这一步可以理解为一个松弛步骤，它违反了极线约束，以实现特征面片之间更高的相关性。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:4:2","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"C. 位姿与结构优化  在前一步中，我们以违反极线约束为代价，建立了具有亚像素精度的特征对应。特别是，我们生成了重投影误残差 $\\lVert \\mathbf \\delta \\mathbf u_i \\rVert = \\lVert \\mathbf u_i - \\pi(\\mathbf T_{k,w} \\enspace {}_w \\mathbf p_i )\\rVert \\neq 0$，平均为 0.3 个像素。在最后一步，我们再一次通过最小化重投影残差来优化相机位姿 $\\mathbf T_{k,w}$： $$ \\mathbf{T}_{k, w}=\\arg \\min _{\\mathbf{T}_{k, w}} \\frac{1}{2} \\sum_{i}\\left|\\mathbf{u}_{i}-\\pi\\left(\\mathbf{T}_{k, w} w \\mathbf{p}_{i}\\right)\\right|^{2} {\\tag {14}} $$ 这是一个众所周知的仅运动 BA 问题，并且可以有效地使用迭代非线性最小二乘最小化算法（如高斯牛顿）来解决。  随后，我们通过最小化重投影误差（structure-only BA）优化观察到的三维点的位置。最后，可以应用局部BA，将所有闭合关键帧的位姿和观察到的三维点共同优化。在算法的快速参数设置中，我们忽略了 BA 步骤（章节VII）。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:4:3","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"D. 讨论  算法的第一个（第IV-A部分）和最后一个（第IV-C部分）优化似乎是多余的，因为两者都优化了相机的6自由度姿态。实际上，我们可以直接从第二步开始，通过 Lucas-Kanade 跟踪所有特征补丁[27]，建立特征响应，然后进行非线性姿态细化（第IV-C节）。虽然这种方法可行，但处理时间会更长。在大距离上跟踪所有功能（例如，30像素）需要一个更大的补丁和金字塔实现。此外，一些特征可能被跟踪不准确，这将需要异常值检测。然而，在SVO中，特征对齐通过在稀疏图像对齐步骤中优化6个参数（相机姿态）来有效地初始化。稀疏图像对齐步骤隐式地满足极线约束，并确保不存在异常值。  有人可能会说，第一步（稀疏图像对齐）将足以估计相机的运动。实际上，这是最近为RGB-D相机开发的算法[10]所做的，然而，通过对齐完整的深度地图而不是稀疏的补丁。我们从经验上发现，与同时使用所有三个步骤相比，使用第一个步骤只会导致更大的漂移。精度的提高是由于新图像相对于关键帧和地图对齐，而稀疏图像对齐仅相对于前一帧对齐新帧。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:4:4","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"V. 建图  给定一幅图像及其位姿 ${ I_k,\\mathbf T_{k,w} }$，建图线程估计对应的未知三维点的二维特征的深度。特征的深度估计采用概率分布建模。每个后续观测 ${ I_k,\\mathbf T_{k,w} }$ 都用于更新贝叶斯框架中的分布（见图5），如[28]所示。当分布的方差变得足够小时，使用（2）将深度估计转换为3D点，该点插入地图并立即用于运动估计（见图1）。在下文中，我们在[28]中报告了基本结果和对原始实现的修改。  每一个深度过滤器都与一个参考关键帧 $r$ 关联。初始化滤波器在深度上为高度不确定性，并将平均值设置为参考帧中的平均场景深度。对于每个后续观测 ${ I_k,\\mathbf T_{k,w} }$，我们在新图像 $I_k$ 的极线上搜索与参考面片具有最高相关性的面片。极线可以从帧 $\\mathbf T_{r,k}$ 和穿过 $\\mathbf u_i$ 的光线之间的相对姿势计算出来。可通过三角测量找到与深度 $\\widetilde {d}_i^k$ 相关最高的相关点 $\\mathbf u^{\\prime}_i$（见图5）。  测量 $\\widetilde {d}_i^k$ 通过 Gaussian + Uniform 混合模型建模分布：一个好的测量是在真实深度周围正态分布的，而一个异常值测量是在区间 $\\lbrack d^{min}_i,d^{max}_i \\rbrack$ 内均匀分布的: $$ p\\left(\\tilde{d}_{i}^{k} \\mid d_{i}, \\rho_{i}\\right)=\\rho_{i} \\mathcal{N}\\left(\\tilde{d}_{i}^{k} \\mid d_{i}, \\tau_{i}^{2}\\right)+\\left(1-\\rho_{i}\\right) \\mathcal{U}\\left(\\tilde{d}_{i}^{k} \\mid d_{i}^{\\min }, d_{i}^{\\max }\\right) $$ 其中 $\\rho_{i}$ 是一个好的测量的概率，$\\tau_{i}^{2}$ 是一个好的测量的方差，可以通过假设图像平面中的一个像素的光度差异方差进行几何计算[29]。  在[28]中详细描述了该模型的递归贝叶斯更新步骤。与[28]相反，我们使用反向深度坐标来处理较大的场景深度。  本文提出的深度估计方法在仅搜索当前极线深度估计的一小段范围时是非常有效的；在我们的例子中，这个范围相当于当前深度估计的两倍标准差。图6 展示了只需很少的运动就可以显著降低深度的不确定性。与标准的从两个角度对点进行三角测量相比，提出的方法的主要优点是，我们观察到的离群值要少得多，因为每个滤波器都要经过许多测量直到收敛。此外，错误的测量被明确地建模，这允许深度收敛，即使在高度相似的环境。在[29]中，我们将演示如何将相同的方法用于稠密建图。 ","date":"2021-11-16","objectID":"/zh-cn/svo/:5:0","tags":["翻译","论文","SLAM"],"title":"SVO译","uri":"/zh-cn/svo/"},{"categories":["论文"],"content":"DSO论文翻译","date":"2021-10-12","objectID":"/zh-cn/dso/","tags":["翻译","论文","SLAM"],"title":"DSO","uri":"/zh-cn/dso/"},{"categories":["论文"],"content":"Direct Sparse Odometry：直接稀疏里程计 摘要：我们提出了一个新颖的直接稀疏里程计方法（DSO）。整个系统结合了一个完全直接的概率模型（最小化光度误差），并对所有的模型参数进行优化，包括在参考帧中表示为反深度的几何图形和相机运动。为了达到实时运算，算法去除了直接法添加先验的做法，取而代之的是从整个图像上均匀的采样关键点。因为我们的方法不依赖关键点检测或描述符，所以它可以自然地从所有具有强梯度的图像区域中采样像素，包括边缘或平滑的光度变化区域（大多在白色墙壁）。该模型集成了一个完整的光度校准，包括曝光时间、透镜光晕和非线性响应函数（这些基本上都是相机的参数）。我们在三个数据集，包括几个小时的视频上全面评估了我们的方法。实验表明，在各种现实环境下，该方法在跟踪精度和鲁棒性方面都显著优于最先进的直接和间接方法。 ","date":"2021-10-12","objectID":"/zh-cn/dso/:0:0","tags":["翻译","论文","SLAM"],"title":"DSO","uri":"/zh-cn/dso/"},{"categories":["论文"],"content":"1. 引言  同时定位和建图（SLAM）和视觉里程计（VO）是许多新兴技术的基本组成部分——从自动驾驶汽车和无人机到虚拟现实和增强现实。近年来，SLAM和VO的实时方法取得了重大进展。虽然长期以来该领域由基于特征的（间接）方法主导，但近年来，许多不同的方法越来越受欢迎，即直接和密集方法。 图1. 直接稀疏里程计(DSO)。围绕建筑物循环的1:40分钟视频的3D重建和跟踪轨迹（仅限单目视觉里程计）。左下角插图显示了起点和终点的特写，可视化了轨迹过程中累积的漂移。底行显示一些视频帧。 直接 vs. 间接：所有公式的基础是一个概率模型，该模型将噪声测量 Y 作为输入，并计算未知的隐藏模型参数X（3D世界模型和相机动作）的估计量 。通常使用最大似然方法，该方法找到使获得实际测量值的概率最大化的模型参数，即，$\\mathbf{X}^{*}:=\\operatorname{argmax}_{\\mathbf{X}} P(\\mathbf{Y} \\mid \\mathbf{X})$。  间接方法分两步进行。 首先，对原始传感器测量进行预处理以生成中间表示，解决整体问题的一部分，例如建立对应关系。 其次，计算出的中间值被解释为概率模型中的噪声测量 Y，以估计几何和相机运动。 请注意，第一步通常是通过提取和匹配一组稀疏的关键点来实现的——但是也存在其他选项，例如以密集、正则化的光流的形式建立对应关系。它还可以包括提取和匹配其他几何原语(如线段或曲线段)的参数表示的方法。  直接方法跳过预处理步骤，直接使用实际传感器值（在特定时间段内从特定方向接收到的光）作为概率模型中的测量值 Y。  在被动视觉的情况下，因为传感器提供光度测量，直接方法因此优化了光度误差。另一方面，间接方法优化几何误差，因为预先计算的值（点位置或流向量）是几何量。请注意，对于其他传感器模式，如深度相机或激光扫描仪（直接测量几何量），直接公式也可以优化几何误差。 稠密 vs. 稀疏：稀疏方法仅使用和重建一组选定的独立点（传统上是角点），而密集方法尝试使用和重建 2D 图像域中的所有像素。中间方法（半密集）避免重建完整的表面，但仍旨在使用和重建（主要连接且约束良好的）子集。  然而，除了使用的图像区域的范围之外，一个更基本的——也是重要的——区别在于添加了几何先验。在稀疏公式中，没有邻域的概念，并且几何参数（关键点位置）在给定相机姿势和内在函数的情况下是条件独立的。另一方面，密集（或半密集）方法利用所用图像区域的连通性来制定几何先验，通常有利于平滑。事实上，要想仅从被动视觉中观察到稠密的世界模型，就必须要有这样的先验。通常，该先验直接以附加对数似然能量项的形式表示[26,21,22]。  请注意，密集和稀疏之间的区别并不是直接和间接的同义词——事实上，所有四种组合都存在： 稀疏+间接：这是最广泛使用的形式，从一组关键点匹配来估计三维几何，从而使用几何误差而无需几何先验。例如Jin等人的[12]、monoSLAM[4]、PTAM[16]和ORB-SLAM[20]。 稠密+间接：该形式根据密集的正则化光流场或与其结合来估计 3D 几何形状，将几何误差（与流场的偏差）与几何先验（流场的平滑度）相结合，示例包括[27, 23]。 稠密+直接：该公式采用光度误差以及几何先验来估计密集或半密集几何。 示例包括DTAM[21]、其前身[26]和LSD-SLAM[5]。 稠密+直接：这是本文提出的公式。 它优化了直接在图像上定义的光度误差，而不包含几何先验。虽然我们不知道最近有任何使用该公式的工作，但Jin等人在2003年已经提出了一个稀疏和直接的公式[13]。然而，与他们基于扩展卡尔曼滤波器的工作相比，我们的方法使用非线性优化框架。 探索稀疏和直接结合的动机在下一节中阐述。 ","date":"2021-10-12","objectID":"/zh-cn/dso/:1:0","tags":["翻译","论文","SLAM"],"title":"DSO","uri":"/zh-cn/dso/"},{"categories":["论文"],"content":"1.1 动机 本文提出的单目视觉里程计的直接和稀疏公式是出于以下考虑。  （1）直接：关键点的主要好处之一是它们能够对使用现成的商品相机拍摄的图像中存在的光度和几何失真提供鲁棒性。例如自动曝光变化、非线性响应函数（伽马校正/白平衡）、镜头衰减（渐晕）、去拜耳效应，甚至是滚动快门引起的强烈几何失真。  同时，对于介绍中提到的所有用例，数以百万计的设备将（和已经）配备摄像头，仅用于为计算机视觉算法提供数据，而不是捕获图像供人类消费。这些相机应该并且将被设计为提供完整的传感器模型，并以最适合处理算法的方式捕获数据：例如，自动曝光和伽马校正不是未知的噪声源，而是提供更好图像数据的功能—— 并且可以将其合并到模型中，使获得的数据更具信息性。由于直接方法将完整的图像形成过程建模到像素强度，因此它极大地受益于更精确的传感器模型。  直接公式的主要优点之一是，它不需要点本身可以识别，因此允许更精细的几何表示（像素逆深度）。此外，我们可以从所有可用的数据中取样——包括边缘和弱强度变化——生成一个更完整的模型，并在稀疏纹理环境中提供更强的鲁棒性。 图2. 稀疏与密集 Hessian 结构。左：稀疏束平差的 Hessian 结构：由于几何几何块是对角线，因此可以使用 Schur 补集高效求解。右图：几何先验增加了（部分非结构化的）几何-几何相关性——因此得到的系统不仅要大得多，而且也变得更难解决。为简单起见，我们不显示全局相机内在参数。  （1）稀疏：添加几何先验的主要缺点是在几何参数之间引入了相关性，这使得在统计上一致的实时联合优化不可行（参见图 2）。这就是为什么现有的密集或半密集方法（a）忽略或粗略地近似几何参数（橙色）和/或几何参数和相机姿势（绿色）之间的相关性，以及（b）对密集几何采用不同的优化方法 部分，例如原始对偶公式[26, 21, 22]。  此外，当今先验的表达复杂性是有限的：虽然它们使 3D 重建更密集、局部更准确且更具视觉吸引力，但我们发现先验会引入偏差，从而减少而不是增加长期、大规模的准确性。 请注意，随着时间的推移，随着从现实世界数据中学习到的更现实、无偏见的先验的引入，这很可能会改变。 ","date":"2021-10-12","objectID":"/zh-cn/dso/:1:1","tags":["翻译","论文","SLAM"],"title":"DSO","uri":"/zh-cn/dso/"},{"categories":["论文"],"content":"1.2 贡献与概要  在本文中，我们提出了一种单目视觉里程计的稀疏直接方法。 据我们所知，它是唯一一种完全直接的方法，可以联合优化所有涉及的模型参数的完全似然性，包括相机姿势、相机内在参数和几何参数（逆深度值）。 这与 SVO[9]等混合方法形成对比，后者恢复为联合模型优化的间接形式。  优化是在滑动窗口中执行的，在滑动窗口中，旧相机姿势以及离开相机视野的点被边缘化，其方式受[17]的启发。对比已经存在的算法，我们的方法充分利用光度相机校准，包括镜头衰减、伽马校正和已知曝光时间。 这种集成的光度校准进一步提高了准确性和稳健性。  我们基于CPU的实现在笔记本电脑上实时运行。我们在三个不同的数据集（包括几个小时的视频）上进行了广泛的评估，结果表明，它在鲁棒性和准确性方面都优于其他最先进的方法（直接和间接）。通过减少设置（更少的点和活动关键帧），它甚至以 5 实时速度运行，同时仍然优于最先进的间接方法。 反过来，在高、非实时设置（更多点和活动关键帧）上，它创建的半密集模型与 LSD-SLAM 的密度相似，但更准确。  本文组织如下：第2节介绍了所提出的直接稀疏模型以及加窗优化方法。具体而言，这包括第2.1节中的几何和光度照相机校准、第2.2节中的模型公式以及第2.3节中的窗口优化。第3节描述了前端：算法中执行数据选择并为高度非凸优化后端提供足够精确的初始化的部分。我们在第4.1节中提供了与其他方法的彻底实验比较。我们还评估了重要参数和新概念的影响，如第4.2节中光度校准的使用。在第4.3节中，我们分析了增加的光度和几何噪声对数据的影响。最后，我们在第5节中提供了一个总结。 ","date":"2021-10-12","objectID":"/zh-cn/dso/:1:2","tags":["翻译","论文","SLAM"],"title":"DSO","uri":"/zh-cn/dso/"},{"categories":["论文"],"content":"2. 直接稀疏模型  我们的直接稀疏里程计基于最近帧窗口上光度误差的连续优化，同时考虑到用于图像形成的光度校准模型。与现有的直接方法相比，我们联合优化所有涉及的参数（相机内在参数、相机外在参数和逆深度值），有效地执行窗口稀疏束调整的光度等效。我们保留其他直接方法采用的几何表示，即3D点表示为参考系中的逆深度（因此具有一个自由度）。 注意. 在整篇论文中，粗体小写字母 (x) 表示向量，粗体大写字母 (H) 表示矩阵。 标量将由浅色小写字母 (t) 表示，函数（包括图像）将由浅色大写字母 (I) 表示。相机姿态表示为变换矩阵$\\mathbf{T}_{i} \\in \\mathrm{SE}(3)$，变换一个点从世界帧到相机帧。线性化的姿态增量将表示为李代数$x_i \\in \\mathfrak{s e}(3)$，这里稍微滥用符号，我们直接写成向量$x_i \\in \\mathbb{R}^{6}$。我们进一步定义了常用的操作符 ⊞ ：$\\mathfrak{s e}(3) \\times \\mathrm{SE}(3) \\rightarrow \\mathrm{SE}(3)$使用左乘公式，即： $$ \\boldsymbol{x}_i \\text { ⊞ } \\mathbf{T}_i:=e^{\\widehat{\\boldsymbol{x}_i}} \\cdot \\mathbf{T}_i $$ ","date":"2021-10-12","objectID":"/zh-cn/dso/:2:0","tags":["翻译","论文","SLAM"],"title":"DSO","uri":"/zh-cn/dso/"},{"categories":["论文"],"content":"2.1 校准  直接方法全面建模图像形成过程。 除了几何相机模型（包括将 3D 点投影到 2D 图像上的功能）之外，考虑光度相机模型也是有益的，该模型包括将传感器上的像素（辐照度）接收到的真实世界能量映射到相应的强度值的功能。请注意，对于间接方法，这没有什么好处，因此被广泛忽略，因为常见的特征提取器和描述符对光度变化是不变的（或高度鲁棒的）。 2.1.1 几何相机校准  为简单起见，我们为众所周知的针孔相机模型制定了我们的方法——在预处理步骤中去除了径向失真。虽然对于广角相机，这确实会减少视野，但它允许对仅实现有限相机型号选择的方法进行比较。在本文中，我们将投影表示为 $\\Pi_{\\mathrm{c}}: \\mathbb{R}^3 \\rightarrow \\Omega$ 和反投影 $\\Pi_{\\mathbf{c}}^{-1}: \\Omega \\times \\mathbb{R} \\rightarrow \\mathbb{R}^3$，其中 c 表示相机的固有参数（对于针孔模型，它们是焦距和主点）。请注意，类似[2]，我们的方法可以扩展到其他(可逆)相机模型，尽管这确实增加了计算需求。 2.1.2 光度相机校准  我们使用[8]中使用的图像形成模型，它解释了非线性响应函数 $G: \\mathbb{R} \\rightarrow[0,255]$，以及镜头衰减（渐晕）$V: \\Omega \\rightarrow[0,1]$。图3显示了来自 TUM monoVO 数据集的示例校准。 组合模型由下式给出： $$ I_{i}(\\mathrm{x})=G\\left(t_{i} V(\\mathrm{x}) B_{i}(\\mathrm{x})\\right){\\tag 2} $$ 其中 $B_i$ 和 $I_i$ 是第 $i$ 帧中的辐照度和观察到的像素强度，$t_i$ 是曝光时间。 该模型是通过光度校正每个视频帧作为第一步来应用的，通过计算: $$ I_{i}^{\\prime}(\\mathrm{x}):=t_{i} B_{i}(\\mathrm{x})=\\frac{G^{-1}\\left(I_{i}(\\mathrm{x})\\right)}{V(\\mathrm{x})} {\\tag 3} $$ 在本文的其余部分中，除非另有说明，否则 $I_i$ 将始终指的是经过光度校正的图像 $I^\\prime_i$ 。 图3. 光度校准。顶部：反向响应函数G-1和用于图1的相机的镜头衰减V。底部：包含室内和室外部分的序列的曝光时间t（毫秒）。请注意，它的变化系数超过500，从0.018到10.5ms。我们没有将这些量视为未知噪声源，而是在光度误差模型中明确地考虑它们 ","date":"2021-10-12","objectID":"/zh-cn/dso/:2:1","tags":["翻译","论文","SLAM"],"title":"DSO","uri":"/zh-cn/dso/"},{"categories":["论文"],"content":"2.2 模型组成 图4. 残留图案.用于残差计算的模式 Np。右下角的像素被省略以启用 SSE 优化处理。请注意，由于我们每个点有1个未知数（其逆深度），并且不使用正则化器，因此我们需要 |Np| 1，以便在仅优化两帧时对所有模型参数进行良好约束。图19显示了对这种模式如何影响跟踪精度的评估。  我们将参考帧 $I_i$ 中的点 $\\mathbf{p} \\in \\Omega_{i}$ 在目标帧 $I_j$ 中观察到的光度误差定义为小像素邻域上的加权 SSD。我们的实验表明，8 个像素以略微扩展的模式排列（见图 4），可以在评估所需的计算、运动模糊的鲁棒性和提供足够的信息之间取得良好的平衡。请注意，就包含的信息而言，在如此小的像素邻域上评估 SSD 类似于为中心像素添加一阶和二阶辐照度导数常数项（除了辐照度常数项）。即 $$ E_{\\mathbf{p} j}:=\\sum_{\\mathbf{p} \\in \\mathcal{N}_{\\mathbf{p}}} w_{\\mathbf{p}}\\left|\\left(I_{j}\\left[\\mathbf{p}^{\\prime}\\right]-b_{j}\\right)-\\frac{t_{j} e^{a_{j}}}{t_{i} e^{a_{i}}}\\left(I_{i}[\\mathbf{p}]-b_{i}\\right)\\right|_{\\gamma} $$ 其中 $\\mathcal{N}_{\\mathbf{p}}$ 是包含在 SSD 中的像素集合；$t_i$, $t_j$ 是图像 $I_i$,$I_j$ 的曝光时间；$|\\cdot|_{\\gamma}$ 是 Huber 范数。此外，$\\mathbf{p}^{\\prime}$ 代表 $\\mathbf{p}$ 的投影点位置，深度为 $d_{\\mathbf{p}}$，由下式给出: $$ \\mathbf{p}^{\\prime}=\\Pi_{\\mathbf{c}}\\left(\\mathbf{R} \\Pi_{\\mathbf{c}}^{-1}\\left(\\mathbf{p}, d_{\\mathbf{p}}\\right)+\\mathbf{t}\\right) {\\tag 5} $$ 以及 $$ \\left[\\begin{array}{cc} \\mathbf{R} \u0026 \\mathrm{t} \\newline 0 \u0026 1 \\end{array}\\right]:=\\mathrm{T}_{j} \\mathrm{~T}_{i}^{-1} {\\tag 6} $$ 为了让我们的方法在没有已知曝光时间的情况下对序列进行操作，我们包含了一个额外的仿射亮度传递函数，由下式给出$e^{-a_{i}}\\left(I_{i}-b_{i}\\right)$。请注意，与之前的大多数公式[13, 6]相比，标量因子 $e^{-a_{i}}$ 是对数参数化的。这既可以防止它变成负数，又可以避免乘法（即指数增长）漂移引起的数值问题。  除了使用鲁棒的 Huber 惩罚之外，我们还应用了一个梯度相关的加权 $w_p$，由下式给出: $$ w_{\\mathbf{p}}:=\\frac{c^{2}}{c^{2}+\\left|\\nabla I_{i}(\\mathbf{p})\\right|_{2}^{2}} {\\tag 7} $$ 它降低了具有高梯度的像素的权重。这个加权函数可以从概率上解释为在投影点位置 $\\mathbf{p}^{\\prime}$ 上添加小的、独立的几何噪声，并立即边缘化它——近似小的几何误差。总而言之，误差 $E_{pj}$ 取决于以下变量：（1）点的逆深度 $d_{\\mathbf{p}}$；（2）相机内参 $c$；（3）相关帧的姿态$\\mathbf{T}_{i}$,$ \\mathbf{T}_{j}$；（4）它们的亮度传递函数参数 $a_i$，$b_i$，$a_j$，$b_j$。  所有帧和点的完整光度误差由下式给出: $$ E_{\\text {photo }}:=\\sum_{i \\in \\mathcal{F}} \\sum_{\\mathbf{p} \\in \\mathcal{P}_{i}} \\sum_{j \\in \\text { obs }(\\mathbf{p})} E_{\\mathbf{p} j} {\\tag 8} $$ 其中 $i$ 遍历所有帧 $\\mathcal{F}$，$\\mathbf{p}$ 遍历第 $i$ 帧中的所有点 $\\mathcal{P}_{i}$，$j$ 遍历其中点 $\\mathbf{p}$ 可见的所有帧 obs($\\mathbf{p}$)。图 5 显示了生成的因子图：与经典重投影误差的唯一区别是每个残差对主帧位姿的附加依赖性，即每个项依赖于两个帧而不是一个。虽然这向 Hessian 的姿态-姿态块添加了非对角线条目，但在应用 Schur补充来边缘化点参数后，它不会影响稀疏模式。 由此产生的系统可以类似于间接公式求解。请注意，关于两帧姿势的雅可比矩阵通过它们的相对姿势的伴随线性相关。在实践中，当计算 Hessian 或其 Schur 补码时，可以将这个因素从和中提取出来，大大减少了更多变量依赖引起的额外计算。 图5.直接稀疏模型的因子图。四个关键帧和四个点的示例：一个在 KF1，两个在 KF2，一个在 KF4。每个残差项（在方程 (4) 中定义）取决于点的主框架（蓝色）、观察点的框架（红色）和点的逆深度（黑色）。此外，所有项都取决于全局相机内在向量c，其未显示。  如果曝光时间已知，我们进一步添加一个将仿射亮度传递函数拉为零的先验： $$ E_{\\text {prior }}:=\\sum_{i \\in \\mathcal{F}}\\left(\\lambda_{a} a_{i}^{2}+\\lambda_{b} b_{i}^{2}\\right) {\\tag 9} $$ 如果没有可用的光度校准，我们设置 $t_i = 1$ 和 $\\lambda_{a}=\\lambda_{b}=0$，因为在这种情况下，他们需要对相机的（未知）变化的曝光时间进行建模。作为旁注，应该提到的是，如果 $x_i$ 和 $y_i$ 都包含噪声测量值，乘法因子 $a^{*}=\\operatorname{argmax}_a \\sum_{i}\\left(a x_i-y_i\\right)^2$ 的 ML 估计量是有偏差的，在无约束的情况$\\lambda_{a} = 0$下导致 $a$ 漂移。虽然这通常对估计的姿势几乎没有影响，但如果场景只包含很少的、微弱的强度变化，它可能会引入偏差。 点维数. 在所提出的直接模型中，与间接模型中的三个未知数不同，一个点仅由一个参数（参考坐标系中的逆深度）参数化。为了理解这种差异的原因，我们首先注意到，在这两种情况下，3D 点实际上是连续、真实世界 3D 表面上任意定位的离散样本。不同之处在于定义表面上的 2D 位置的方式。在间接方法中，它被隐式定义为点，它（投影到图像中）在使用的角点响应函数中产生最大值。这意味着表面以及点在表面上的位置都是未知数，需要估计。在我们的直接方法中，点被简单地定义为源像素的光线撞击表面的点，因此只剩下一个未知数。除了减少参数的数量外，这自然能使逆深度参数化，在高斯框架中更适合于表示不确定性，而不是对极远点[3]的不确定性。 一致性. 严格来说，所提出的直接稀疏模型确实允许多次使用某些观察值（像素值），而其他的模型则根本不使用。这是因为——尽管我们的点选择策略试图通过在空间中均匀分布点来避免这种情况（参见第 3.2 节）——我们允许点观察重叠，因此依赖于相同的像素值。这尤其发生在纹理很少的场景中，其中所有点都必须从纹理图像区域的一个小子集中选择。然而，我们认为这在实践中的影响可以忽略不计，并且，如果需要，可以通过删除（或降低）使用相同像素值的观察来避免。 ","date":"2021-10-12","objectID":"/zh-cn/dso/:2:2","tags":["翻译","论文","SLAM"],"title":"DSO","uri":"/zh-cn/dso/"},{"categories":["论文"],"content":"2.3 模型组成  我们遵循Leutenegger等人[17]的方法，使用Gauss-Newton算法优化滑动窗口中的总误差（8），这在速度和灵活性之间进行了很好的权衡。  为了便于表示，我们将 (1) 中定义的算子⊞扩展到所有优化参数——对于 $\\mathrm{SE}(3)$ 以外的参数，它表示常规加法。我们将使用$\\zeta \\in \\mathrm{SE}(3)^{n} \\times \\mathbb{R}^{m}$来表示所有的优化变量，包括相机位姿、仿射亮度参数、反深度值和相机内在函数。与[17]中一样，边缘化依赖于 $\\zeta$ 中的参数的残差将固定在一个切线空间中，在这个切线空间中，有关该参数的任何未来信息(增量更新)都会累积。我们用 $\\zeta_0$ 表示这个切线空间的评估点，并用 $\\boldsymbol{x} \\in \\mathfrak{s e}(3)^{n} \\times \\mathbb{R}^{m}$ 表示积累的增量更新。因此，当前状态估计由 $\\zeta=\\boldsymbol{x}{\\text {⊞ }} \\zeta{0}$ 给出。图 6 显示了不同变量之间的关系。 高斯牛顿优化. 我们计算高斯牛顿系统为: $$ \\mathbf{H}=\\mathbf{J}^{T} \\mathbf{W} \\mathbf{J} \\quad \\text { and } \\quad \\mathbf{b}=-\\mathbf{J}^{T} \\mathbf{W} \\mathbf{r} {\\tag {10}} $$ 其中 $\\mathbf{W} \\in \\mathbb{R}^{n \\times n}$ 是包含权重的对角矩阵，$\\mathbf{r} \\in \\mathbb{R}^{n}$ 是堆叠残差向量，$\\mathbf{J} \\in \\mathbb{R}^{n \\times d}$ 是 $\\mathbf{r}$ 的雅克比矩阵。  请注意，每个点对能量贡献 $\\mathcal{N}_{\\mathbf{p}}$ = 8 个残差。为符号简单起见，我们将在下面只考虑单个残差 $r_k$， ","date":"2021-10-12","objectID":"/zh-cn/dso/:2:3","tags":["翻译","论文","SLAM"],"title":"DSO","uri":"/zh-cn/dso/"},{"categories":["论文"],"content":"ORB-SLAM论文翻译","date":"2021-09-19","objectID":"/zh-cn/orb-slam/","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"ORB-SLAM：一种通用的精确的单目SLAM系统 摘要：本文提出了ORB-SLAM，一种基于特征的单目SLAM系统，其可以实时操作在大小场景、室内室外环境。系统对复杂的剧烈运动具有鲁棒性，允许宽基线的闭环和重定位，且包含完整的自动初始化。基于最近几年的优秀算法之上，我们从头开始设计了一个新颖的系统，它对所有SLAM任务使用相同的特征：追踪、建图、重定位和闭环。合适的选择重建点和关键帧策略的存在使其具有很好的鲁棒性，并能够生成紧凑的可追踪的地图，并且只有当场景内容发生变化地图才改变，从而允许长时间操作。本文从最受欢迎的数据集中提供了27个序列的详尽评估。相对于其他最先进的单目SLAM方法，ORB-SLAM实现了前所未有的性能。为了社会的利益，我们将源代码公开。 关键字：持续建图，定位，单目视觉，识别，同时定位和制图（SLAM）。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:0:0","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"I.引言  BA提供相机定位的精确估计以及稀疏几何重建[1][2]，以及并且提供了强烈的匹配网络和良好的初始猜测。一段长的时间，这种方法被认为不符合实时性的应用，如视觉VSLAM。VSLAM系统在构建环境的同时需要估计相机的轨迹。现在，我们为了不以过高的计算成本获得准确的结果，实时SLAM算法必须向BA提供以下信息： 在候选图像帧子集中（关键帧）匹配观测的场景特征（地图云点）。 由于关键帧数量的增长，需要做筛选避免冗余。 关键帧和云点健壮的网络配置可以产生精确的结果，换言之，分布良好的关键帧集合和有明显视差、大量回环匹配的观测云点。 关键帧和云点位置的初始估计，采用非线性优化的方法。 在构建局部地图的过程中，优化的关键是获得良好的稳定性。 本系统可以实时执行快速全局优化（比如位姿图）以实现闭环回路。  BA第一次实时应用是在Mouragon等人[3]提出的视觉里程计算法中，其次是在Klein和Murray的突破性工作PTAM[4]算法中。尽管受制于小场景的应用，PTAM算法对关键帧的选择，特征匹配，点的三角化，相机位姿估计，追踪失败后的重定位非常有效。然而，由于缺少闭环检测和对遮挡场景的处理，再加上其视图不变性差，在地图初始化时需要人工干预等多个因素，使得PTAM算法的应用收到了严重的限制。  在本文中，我们基于PTAM算法的主要框架，采用Gálvez-López和Tardós提出的place recognition（场景/位置识别）算法，Strasdat等人提出的scale-aware loop closing（具备尺度感知的闭环检测）算法以及文献[7][8]中的大尺度操作中Covisibility信息的使用，重新设计了一种新的单目SLAM系统ORB-SLAM，本文的贡献主要包括： 对所有的任务采用相同的特征：追踪、地图构建、重定位和闭环检测。这使得我们的系统更有效率、简单可靠。采用的ORB特征[9]在没有GPU的情况下也有很好的实时性，且具有旋转不变性和光照不变性。 算法支持在宽阔环境中实时操作。由于covisibility graph的使用，特征点的跟踪与构图主要集中在局部共视区域，而与全局地图的大小无关。 使用Essential Graph来优化位姿实现回环检测。构建生成树，并由系统、闭环检测链接和covisibility graph的强边缘进行维护。 算法的实时相机重定位具有明显的旋转不变特性和光照不变性。这就使得点跟踪丢失后可以恢复，增强了地图的重用性。 一种新的基于模型选择的自动鲁棒初始化过程，允许创建平面和非平面场景的初始地图。 提出了一种合适的方法来选择地图点云和关键帧，通过严格删选关键帧和地图点，剔除冗余信息，使得特征点的跟踪具备了更好的稳定性，从而增强算法的可持续操作性。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:1:0","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"II.相关工作 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:2:0","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"A.位置识别  Williams等人在综述[13]中比较了几种基于景象的位置识别方法，即图像到图像的匹配，这种方法在大环境下比地图到地图或图像到地图的方法更准确。在景象匹配方法中，bags of words（词袋）[14]的使用以其效率很高而脱颖而出，比如概率方法FAB-MAP[15]算法。DBoW2方法[5]则首次使用了BRIEF描述子[16]生成的二进制词袋连同非常高效的FAST特征检测算法[17]，与SURF和SIFT相比，FAST算法的运时间减小了至少一个数量级。然而，尽管系统运行效率高、鲁棒性好，采用BRIEF描述子不具有旋转不变性和尺度不变性，系统只能运行在同一平面内（否则会造成尺度变化） ，闭环检测也只能从相似的视角中进行。在我们之前的工作[11]中，我们提出了一个使用ORB特征检测子[9]的DBoW2位置识别器。ORB特征是具有旋转不变和尺度不变特性的二进制特征，因此，用它生成的快速识别器具有较好的视角不变性。我们在4组不同的数据集上演示了位置识别功能，从10K图像数据库中提取一个候选闭合回路的运算时间少于39毫秒。在本文的工作中，我们提出了一种改进版本的位置识别方法，采用covisibility信息，在检索数据库时返回几个假设情况而不是最好的匹配。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:2:1","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"B.地图初始化  单目SLAM系统需要设计专门的策略来生成初始化地图，因为单幅图像不具备深度信息。解决这个问题的一种方法是一开始跟踪一个已知结构的对象，正如文献[20]。另一个方法是用一个具有高不确定度的逆深度参数[21]来初始化点的深度信息，理想情况下，该参数会在后期逐渐收敛到真值。最近Engel提出的半稠密方法[10]中就采用类似的方法将像素的深度信息初始化为一个随机数值。  如果是从两个视角来初始化特征，就可以采用以下方法：一种是假设局部场景在同一平面内[4]，[22]然后利用Faugeras等人论文[23]中的单应性来重构摄像头相对位姿。另一种是将场景建模为通用情况（不一定为平面），通过Nister提出的五点算法[26]来计算本征矩阵[24][25]，但该方法存在多解的问题。这两种摄像头位姿重构方法在低视差下都没有很好的约束，如果平面场景内的所有点都靠近摄像机的中心，则结果会出现双重歧义[27]。另一方面，非平面场景可以通过线性8点算法[2]来计算基础矩阵，相机的相对位姿就可以无歧义的重构出来。  针对这一问题，我们在本文的第四部分提出了一个新的基于模型选择的自动初始化方法，即对平面场景算法选择单应性矩阵，而对于非平面场景，算法选择基础矩阵。模型选择的综述方法参见Torr等人的论文[28]。基于类似的理论，我们设计了一种启发式初始化算法，算法考虑到在接近退化情况（比如：平面，近平面，或是低视差）下选择基础矩阵进行位姿估计可能存在的问题，则选择单应性计算。在平面的情况下，为了保险起见，如果最终存在双重歧义，则算法避免进行初始化，因为可能会因为错误选择而导致算法崩溃。因此，我们会延迟初始化过程，直到所选的模型在明显的视差下产生唯一的解。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:2:2","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"C.单目SLAM  单目SLAM最初采用滤波框架[20][21][29][30]来建模。在该类方法中，每一帧都通过滤波器联合估计地图特征位置和相机位姿。这样做带来的问题是在处理连续帧图像上对计算资源的浪费和线性误差的累积。而另外一种SLAM框架是基于关键帧的[3][4]，即采用少数筛选过的图像（关键帧）来构建地图，因为构图不再与帧率相关联，因此基于关键帧的SLAM方法不但节省了计算资源，还可以进行高精度的BA优化。Strasdar等人在论文[31]中证明了基于关键帧的单目SLAM方法比滤波器方法在相同的运算代价上定位结果更精确。  基于关键帧的SLAM系统最具代表性可能是由Klein和Murray等人提出的PTAM算法[4]。它第一次将相机追踪和地图构建拆分成两个并行的线程运行，并成功用于小环境的实时增强现实中。后来文献[32]引入边缘特征对PTAM算法进行了改进，在跟踪过程中增加了旋转估计步骤，实现了更好的重定位效果。由于PTAM中的地图云点通过图像区块与FAST角点匹配，因此仅适合于特征跟踪并不适合用于后期的位置识别。而实际上，PTAM算法并没有进行大闭环检测，其重定位也仅是基于关键帧低分辨率缩略图的相关性进行的，因此视角不变性较差。  Strasdat等人在文献[6]中提出了一个基于GPU实现的大尺度单目SLAM系统，其前端采用光流算法，其次用FAST特征匹配和运动BA，以及后端是基于滑动窗口BA。闭环检测通过具有相似性约束（7自由度）的位姿图优化来完成，该方法可以矫正在单目SLAM系统中出现的尺度偏移问题。在本文中，我们也将采用这种7自由度的位姿图优化方法，并将其应用到我们的Essential Graph中，更多细节将在第三部分D节里面描述。  Strasdat等人在文献[7]中采用了PTAM的前端，但其跟踪部分仅在一个从covisibility graph提取的局部图中进行。他们提出了一个双窗口优化后端，在内部窗口中连续进行BA，在有限大小的外部窗口中构建位姿图。然而， 只有当外部窗口尺寸足够大到可以包含整个闭环回路的情况下，闭环检测才能起作用。在我们的算法中，我们利用了Strasdat等人提出的基于covisibility的局部地图的优势，并且通过covisibility map来构建位姿图，同时重新设计前端和后端。另一个区别是，我们并没有用特别的特征提取方法做闭合回路检测（比如SURF方法），而是基于相同的追踪和建图的特征进行位置识别，获得具有鲁棒性的重定位和闭环检测。  在Pirker等人的论文[33]中作者提出了CD-SLAM方法，一个非常复杂的系统，包括闭环检测，重定位，大尺度操作以及对算法在动态环境运行所做的改进。但文中并没有提及地图初始化。因此不利于后期读者对算法的复现，也致使我们没法对其进行精确性、鲁棒性和大场景下的测试对比。  Song等人在论文[34]提出的视觉里程计方法中使用了ORB特征做追踪和处理BA后端滑动窗口。相比之下，我们的方法更加全面，因为他们的算法中没有涉及全局重定位，闭环回路检测，而且地图也不能重用。他们也使用了相机到地面的真实距离来限制单目SLAM算法的尺度漂移。  Lim等人在我们提交本文最初的版本[12]之后发表了论文[25]，他们也采用相同的特征进行跟踪，地图构建和闭环检测。但是，由于Lim等人的算法选择的BRIEF描述子不具备尺度不变性，因此其算法运行受限在平面轨迹上。并且他们的算法仅从上一帧关键帧开始跟踪特征点，因此访问过的地图不能重用，这样的处理方式与视觉里程计很像，存在系统无限增长的问题。我们在第三部分E小节里面与该算法进行了定性比较。  Engel等人在最近的论文[10]里提出了LSD-SLAM算法，其没有采用特征提取和BA方法，而是选择采用直接法（优化也是直接通过图像像素灰度进行）构建了大场景的半稠密地图。算法的结果让人印象深刻，其在没有GPU加速的情况下实时构建了一个半稠密地图，相比基于特征的稀疏地图SLAM系统而言，LSD-SLAM方法在机器人领域有更大的应用潜力。然而，该算法的运行仍然需要基于特征做闭环检测，且相机定位的精度也明显低于PTAM和我们的算法，相关实验结果我们将在第8部分的B小节中展示，对该结果的讨论在文章IX部分B小节进行。  Forster等人在论文[22]中提出了介于直接方式和基于特征的方法之间的半直接视觉里程计算法SVO方法。该方法不需要对每帧图像都提取特征点，且可以以很高的帧率运行，在四轴飞行器上取得了令人惊叹的定位效果。然而，SVO算法没有进行闭环检测，且目前主要基于下视摄像头运行。  最后，我们想讨论一下目前关键帧的选择方法。由于所有的视觉SLAM算法选择所有的云点和图像帧运行BA是不可行的。因此，在论文[31]中，Strasdat等人证明最合理的选择是保留尽可能多地点云和非冗余关键帧。PTAM方法非常谨慎插入关键帧避免运算量增长过大。然而，这种严格限制关键帧插入的策略在算法运行困难的情况下可能会导致追踪失败。在本文中，为了达到更好的稳定性，我们选择一种更为合适的关键帧插入策略，当算法运行困难的时候算法选择尽快的插入关键帧，然后在后期将冗余的关键帧删除以避免额外的计算成本。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:2:3","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"III.系统架构 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:3:0","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"A. 特征选择  我们系统设计的中心思想是对SLAM系统的构图、跟踪、重定位以及闭环检测等模块都采用相同的特征。这将使得我们的系统更有效率，避免了像以往文章[6][7]一样还需要额外插入一些额外的识别性强的特征以用于后期的闭环检测。我们每张图像的特征提取远少于33毫秒，远小于目前的SIFT算法（300ms）,SURF算法(300ms)，或最近提出的A-KAZE（~100ms）算法。为了使算法的位置识别能力能更加通用化，我们需要提取的特征具备旋转不变性，而BRIEF和LDB不具备这样的特性。  我们选择了我们选择了ORB[9]，其是具有256位描述符的带方向的多尺度FAST角点。他们对于计算和匹配的速度非常快，同时对视角具有旋转不变的特性。这样可以在更宽的基准线上匹配他们，增强了BA的精度。我们已经在论文[11]sup中演示了基于ORB特征的位置识别性能。需要申明的是，虽然本文的方案中采用ORB算法，但所提出的技术并不仅限于该特征。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:3:1","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"B. 三个线程：跟踪，局部地图构建以及回环检测 图1.ORB-SLAM系统框架，图中显示了算法的三个线程——跟踪、局部构图与闭环检测的所有步骤。另外还有场景识别和地图的主要组成部分。   我们的系统，如图1所示，包含三个并行的线程：跟踪，局部地图构建以及回环检测。跟踪线程负责对每帧图像的相机位置进行定位，并决定什么时候插入新的关键帧。我们首先通过与前一帧图像匹配得到初始特征点，然后采用运动BA优化摄像头位姿。如果特征跟丢（比如由于遮挡或是突然运动），则由位置识别模块进行全局重定位。一旦获得最初的相机位姿估计和特征匹配，则使用由系统维护的关键帧的`covisibility graph`提取一个局部可视化地图，如图2(a)、图2(b)所示。然后通过重投影方法搜索当前帧与局部地图点对应的匹配点，并利用所有的匹配点优化当前相机位姿。最后，跟踪线程决定是否插入新的关键帧。所有的跟踪步骤将在第5部分详细阐述。创建初始化地图的新方法将在第4部分进行说明。  局部地图构建模块负责处理新的关键帧，对周围的相机位姿进行局部BA以优化重构。在covisibility graph已连接的关键帧中搜索新的关键帧中ORB特征的匹配点，以此来三角化新的地图点。有时尽管已经创建了新的点云，但是基于跟踪过程中收集的信息，为了保证点云的高质量，可能会根据点云筛选策略临时删除一些点。局部地图构建模块也负责删除冗余的关键帧。我们将在第6章详细说明局部地图构建的步骤。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:3:2","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"C. 地图点云、关键帧及其选择标准 对每个地图点云$p_i$保存以下信息： 它在世界坐标系中的3D坐标$X_{w,i}$。 视图方向$n_i$，即所有视图方向的平均单位向量。（该方向是指连接该点云和其对应观测关键帧光心的射线方向） ORB特征描述子$D_i$，与其他所有能观测到该点云的关键帧中ORB描述子相比，该描述子的汉明距离最小。 根据ORB特征尺度不变性约束，可观测的点云的最大距离$d_{max}$和最小距离$d_{min}$。 对每个关键帧$K_i$保存以下信息： 相机位姿$T_{iw}$，从世界坐标系转换到相机坐标系下的变换矩阵。 相机内参，包括主点和焦距。 从图像帧提取的所有ORB特征，不管其是否已经关联了地图云点， 这些ORB特征点都经过畸变模型矫正过。  地图点云和关键帧的创建条件较为宽松，但是之后则会通过一个非常严格苛刻的删选机制进行挑选，该机制会检测出冗余的关键帧和匹配错误的或不可跟踪的云点进行删除。这样做的好处在于地图在构建过程中具有一定的弹性，在外界条件比较困难的情况下（比如：旋转，相机快速运动），算法仍然可以实现鲁棒的跟踪，而与此同时，当相机对同一个环境重访问时，地图的尺度大小是可控的，这就利于该系统的长期工作。另外，与PTAM算法相比，我们构建的地图中基本不包含局外点，因为秉持的原则是很苛刻的，宁缺毋滥。地图云点和关键帧的筛选过程将在第6部分B节和E节分别解释。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:3:3","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"D. Covisibility Graph和Essential Graph  关键帧之间的Covisibility信息在本文的SLAM系统中几个模块上都非常有用，像论文[7]一样，我们将其表示成一个间接的权重图。图中每个节点代表一个关键帧，如果两个关键帧都能同时观测到地图云点中至少15个点，则这两个关键帧之间用一条边线相连，我们用权重θ表示两个关键帧能共同观测到的云点数量。  为了矫正闭环回路，我们像论文[6]那样做位姿图优化，优化方法延着位姿图将闭环回路的误差进行分散。考虑到covisibility graph可能非常密集的边缘，我们提出构建一个（Essential Graph），该图中包含covisibility graph的所有节点（关键帧），但是边缘更少，仍旧保持一个强大的网络以获得精确的结果。系统从初始关键帧开始增量式地构建一个生成树，它是一个边缘数量最少的covisibility graph的子图。当插入新的关键帧时，则判断其与树上的关键帧能共同观测到多少云点，然后将其与共同观测点最多的关键帧相连反之，当一个关键帧通过筛选策略被删除时，系统会重新更新与其相关的连接。Essential Graph包含了一个生成树，一个高covisibility（$θ_{min}=100$）的covisibility graph边缘子集，以及闭环回路的边缘，这样的组合共同构建了一个强大的相机网络。图2展示了一个covisibility graph，生成树和相关的essential graph的例子。在本文第8部分第E节的实验里，当算法运行位姿图优化时，结果可以达到非常高的精度以至于即便是全局BA优化都很难达到。。essential graph的效用和θmin对算法的影响将在第8部分E节的最后讨论。 图2.对TUM RGB-D标准库[38]中fr3_long_office_household图像序列进行重构以及本文用到的各种姿态图的例子 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:3:4","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"E. 基于图像词袋模型的位置识别  为了实现闭环检测与重定位，系统嵌入了基于DBoW2[5]算法来执行闭环检测和重定位。视觉词汇（Visual words）是一个离散化的特征描述子空间，被称为视觉词典。这部视觉词典是通过从大量图像中提取ORB描述子离线创建的。如果图像的通用性强，则同一部视觉词典在不同的环境下也能获得很好的性能，正如我们之前的论文[11]那样。SLAM系统增量式地构建一个数据库，该数据库中包含了一个反向指针，用于存储每个视觉词典里的视觉单词，关键帧可以通过这个数据库查询视觉词典，从而实现高效检索。当一个关键帧通过筛选程序删除时，数据库也会相应更新。  由于关键帧之间可能会存在视图上的重叠，因此检索数据库时，可能返回的结果不止一个高分值的关键帧。原版的DBoW2认为是图像重叠的问题，就将时间上接近的图像的分值相加。但这并没有包括观测同一地点但在不同时间插入的关键帧。为了解决这一问题，我们将这些与covisibility graph相连的关键帧进行分类。另外，我们的数据库返回的是分值高于最好分值75%的所有关键帧。  用词袋模型来表示特征匹配的另外一个优势在论文[5]里有详细介绍。如果我们想计算两个ORB特征的对应关系，我们可以强制匹配视觉字典树上某一层（我们在6层里面选第2层）的相同节点（关键帧）里的特征，这可以加快搜索速度。在本文中，我们就利用这个小技巧来搜索匹配的特征点，用于三角化新的点云，闭环检测和重定位。我们还引入了方向一致性来优化匹配点，如论文[11]所示，这可以去掉无效数据，保证所有对应匹配点的旋转方向一致。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:3:5","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"IV.地图自动初始化  地图初始化的目的是计算两帧图像之间的相对位姿来三角化一组初始的地图云点。这个方法应该与场景无关（平面的一般平面），而且不需要人工干预去选择良好的双视图配置，比如两幅图应具有明显的视差。本文算法提出并行计算两个几何模型，一个是面向平面视图的单映矩阵，另一个是面向非平面视图的基础矩阵。然后，采用启发式的方法选择模型，并使用所选的模型从两图像的相对位姿中对地图点云进行重构。本文算法只有当两个视图之间的视差达到安全阈值时，才进行地图初始化。如果检测到低视差的情况或已知两视图模糊的情况（如论文[27]所示），则为了避免生成一个有缺陷的地图而推迟初始化。算法的步骤是：  1）查找初始的匹配点对：  从当前帧中提取ORB特征$F_c$（只在最好的尺度上），与在参考帧$F_r$搜索匹配点对$x_c$↔$x_r$。如果找不到足够的匹配点对，就重置参考帧。  2）并行计算两个模型：  在两个线程上并行计算单应矩阵$H_{cr}$和基础矩阵$F_{cr}$： $$ x_c=H_{cr}x_r{\\quad}{\\quad}{\\quad}{\\quad}x_c^TF_{cr}x_r=0 \\tag {1} $$ 在文献[2]中详细解释了基于RANSAC的归一化DLT算法和8点算法计算原理。为了使两个模型的计算流程尽量一样，将两个模型的迭代循环次数预先设置成一样，每次迭代的特征点数目也预先设置好，基础矩阵是8个特征点对，单映矩阵是4个特征点对。每次迭代中，我们给每个模型M（H表示单映射，F表示基本矩阵）计算一个分值SM： $$ S_{M}=\\sum_{i}\\left(\\rho_{M}\\left(d_{c r}^{2}\\left(\\mathbf{x}_{c}^{i}, \\mathbf{x}_{r}^{i}, M\\right)\\right)+\\rho_{M}\\left(d_{r c}^{2}\\left(\\mathbf{x}_{c}^{i}, \\mathbf{x}_{r}^{i}, M\\right)\\right)\\right) \\newline \\rho_{M}\\left(d^{2}\\right)=\\begin{cases} \\Gamma-d^{2} \u0026 \\text { if } \u0026 d^{2}\u003cT_{M} \\newline 0 \u0026 \\text { if } \u0026 d^{2} \\geq T_{M} \\end{cases} \\tag {2} $$ 其中，$d_{cr}^2$和$d_{rc}^2$是帧到帧之间的对称传递误差[2]。$T_M$是无效数据的排除阈值，它的依据是$\\chi^2$测试的95%（$T_H$=5.99，$T_F$=3.84，假设在测量误差上有1个像素的标准偏差）。$\\Gamma$等于$T_H$，这样两个模型在有效数据上对于同一误差$d$的分值相同，使得运算流程一致。  我们从单应矩阵和基本矩阵的计算中选择分值最高的，但如果两个模型分值都不高（没有足够的局内点），则算法流程重启，从step1开始重新计算。  3）模型选择：  如果场景是平面，近平面或存在低视差的情况，则可以通过单映矩阵来求解。同样地，我们也可以找到一个基础矩阵，但问题是基础矩阵不能够很好的约束该问题[2]，而且从基础矩阵中计算得到的运动结果是错误的。在这种情况下，我们应该选择单映矩阵才能保证地图初始的正确性，或者如果检测到低视差的情况则不进行初始化工作。另一方面，对于非平面场景且有足够的视差的情况则可以通过基础矩阵来计算，而在这种情况下单映矩阵只有基于平面点或者低视差的匹配点才能找到。因此，在这种情况下我们应该选择基础矩阵。我们利用如下强大的启发式进行计算： $$ R_{H}=\\frac{S_{H}}{S_{H}+S_{F}} \\tag{3} $$ 如果$R_H$\u003e0.45，这表示二维平面和低视差的情况，我们将选择计算单应矩阵。其他的情况，我们选择基础矩阵。 4）运动和从运动到结构的重构：  一旦选好模型，我们就可以获得相应的运动状态。如果是单应矩阵，我们按照Faugeras等人发表的论文[23]中提到的方法得到8种运动假设。该方法提出用cheriality测试来选择有效解。然而，如果在低视差的情况下，这些测试就会失效，因为云点很容易在相机的前面或后面移动，会导致选解错误。我们提出直接根据这8种解来进行三角化，然后检查是否有一种解可以使得所有的云点都位于两个相机的前面，且重投影误差较小。如果没有一个明确的解胜出，我们就不执行初始化，重新从第一步开始。这种方法使初始化程序在低视差和两个交叉的视图情况下更具鲁棒性，这也是我们整个算法体现鲁棒性的关键所在。  如果是基本矩阵，我们使用标定矩阵$K$来将其转换为本质矩阵： $$ E_{r c}={K}^{T} {F}_{r c} {K} \\tag {4} $$ 然后通过文献[4]中的奇异值分解得到4个运动解。和上文一样，通过三角化特征点，来选择一个正解。  5）Bundle adjustment：  最后我们执行一个全局BA，详细优化过程参见附录，以优化初始重构得到的点云地图。  如图3所示是对论文[39]中的室外NewCollege机器人图像序列进行地图初始化的例子，室外环境下初始化工作具有很大挑战性。从图中可以看出，PTAM算法和LSD-SLAM算法对位于同一平面上的所有点都进行了初始化，而我们的方法是当两幅图像有足够视差之后才进行初始化，并基于基础矩阵得到了正确的结果。 图3.基于NewCollege图像序列[39]进行地图初始化，最上面一行：PTAM算法,中间一行：LSD-SLAM算法，底下一行：ORB-SLAM算法。其中，PTAM算法和LSD-SLAM算法初始化了一个错误的平面地图，而我们的方法自动选择在两帧图像存在足够视差的情况下再利用基础矩阵初始化。如果人工选择关键帧，则PTAM算法也能够初始化得很好 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:4:0","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"V.跟踪  在这一部分，我们将详细介绍跟踪线程在相机每帧图像上执行的步骤。在几个步骤中都提到的相机位姿优化，包括运动BA，将在附录部分进行阐述。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:5:0","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"A. ORB特征提取  我们在8层图像金字塔上提取FAST角点，金字塔图像尺度因子为1.2。如果图像的分辨率从512*384到752*480，我们发现提取1000个角点比较合适，如果分辨率提高，如KITTI数据集[40]，则提取2000个角点。为了确保特征点均匀分布，我们将每层图像分成网格，每格提取至少5个角点。然后检测每格角点，如果角点数量不够，就调整阈值。如果某些单元格内没有角点，则其对应提取的角点数量也相应减少。最后，根据保留的FAST的角点计算方向和ORB特征描述子。ORB特征描述子将用于算法后续所有的特征匹配，而不是像PTAM算法中那样根据图像区块的相关性进行搜索。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:5:1","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"B. 通过前一帧估计初始位姿  如果上一帧图像跟踪成功，我们就用运动速率恒定模型来预测当前相机的位置，然后搜索上一帧图像中的特征点在地图中的对应云点与当前帧图像的匹配点，最后利用搜索到的匹配点对当前相机的位姿进一步优化。但是，如果没有找到足够的匹配点（比如，运动模型失效，非匀速运动），我们就加大搜索范围，搜索地图云点附近的点在当前帧图像中是否有匹配点，然后通过寻找到的对应匹配点对来优化当前时刻的相机位姿。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:5:2","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"C. 通过全局重定位开初始化位姿  如果扩大了搜索范围还是跟踪不到特征点，（那么运动模型已经失效），则计算当前帧图像的词袋（BoW）向量,并利用BoW词典选取若干关键帧作为备选匹配帧（这样可以加快匹配速度）。然后我们计算每个备选关键帧与地图云点相对应的ORB特征，如第三部分E节所描述。接着，对每个备选关键帧轮流执行PnP算法[41]计算当前帧的位姿（RANSAC迭代求解）。如果我们找到一个姿态能涵盖足够多的有效点，则搜索该关键帧对应的更多匹配云点。最后，基于找到的所有匹配点对相机位置进一步优化，如果有效数据足够多，则跟踪程序将持续执行。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:5:3","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"D. 跟踪局部地图  一旦我们获得了相机的初始位姿估计以及一组初始特征匹配点，我们就可以将更多地地图云点投影到图像上以寻找更多的匹配点。为了降低大地图的复杂性，我们只映射局部地图。该局部地图包含一组关键帧$K_1 $,它们和当前关键帧有共同的地图云点，还包括与关键帧$K_1$在covisibility graph中相邻的一组关键帧$K_2$。这个局部地图中有一个参考关键帧$K_{\\mathrm{ref}} \\in {K}_{1}$，它与当前帧具有最多共同的地图云点。现在对K1, K2中可见的每个地图云点，在当前帧中进行如下搜索：  1）计算地图点云在当前帧图像中的投影$\\mathrm{x}$，如果投影位置超出图像边缘，就将删去对应点云。  2）计算当前视图射线$v$和地图云点平均视图方向$\\mathbf{n}$的夹角。如果$\\mathbf{v} \\cdot \\mathbf{n}\u003c\\cos \\left(60^{\\circ}\\right)$，就删去对应点云。  3）计算地图点云到相机中心的距离$\\mathrm {d}$，如果它不在地图点云的尺度不变区间内，即$d \\notin\\left[d_{\\min }, d_{\\max }\\right]$，就删去该电云。  4）计算每帧图像的尺度比$d / d_{\\min }$。  5）对比地图云点的特征描述子$D$和当前帧中还未匹配的ORB特征，在预测的尺度层和靠近$\\mathrm{x}$的云点作最优匹配。  相机位姿最后通过当前帧中获得所有的地图云点进行优化。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:5:4","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"E. 新关键帧的判断  最后一步是决定当前帧是否可以作为关键帧。由于局部地图构建的过程中有一个机制去筛选冗余的关键帧，所以我们需要尽快地插入新的关键帧以保证跟踪线程对相机的运动更具鲁棒性，尤其是对旋转运动。我们根据以下要求插入新的关键帧：  1）距离上一次全局重定位后需要超过20帧图像。  2）局部地图构建处于空闲状态，或距上一个关键帧插入后，已经有超过20帧图像。  3）当前帧跟踪少于50个地图云点。  4）当前帧跟踪少于参考关键帧$K_{ref}$云点的90%。  与PTAM中用关键帧之间的距离作为判断标准不同，我们加入一个最小的视图变换，如条件4。条件1 确保一个好的重定位，条件3保证好的跟踪。如果局部地图构建处于忙状态（条件2的后半部分）的时候插入关键帧，就会发信号去暂停局部BA，这样就可以尽可能快地去处理新的关键帧。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:5:5","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"VI.局部建图  这章我们将描述根据每个新的关键帧$K_i$。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:6:0","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"A. 关键帧插入  首先更新covisibility graph，具体包括：添加一个新的关键帧节点$K_i$，检查与$K_i$有共同云点的其他关键帧，用边线连接。然后，更新生成树上与$K_i$有最多共享点的其他关键帧的链接。计算表示该关键帧的词袋，并利用三角法生成新的地图云点。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:6:1","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"B. 地图点云筛选  地图点云为了保留在地图里，必须在其创建后的头三个关键帧通过一个严格的测试，这个测试确保留下来的点云都是可以被跟踪的，而不是因为错误错误数据而被三角化创建的。一个云点必须满足如下条件： 跟踪线程必须必须在超过25%的图像中找到该特征点。 如果创建地图云点经过了多个关键帧，那么它必须至少是能够被其他3个关键帧观测到。  一旦一个地图云点通过测试，它只能在被少于3个关键帧观测到的情况下移除。这样的情况在关键帧被删除以及局部BA排除异值点的情况下发生。这个策略使得我们的地图包含很少的无效数据。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:6:2","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"C. 新地图点云创建  新的地图云点的创建是通过对covisibility graph中连接的关键帧$K_c$中的ORB特征点进行三角化实现的。对于$K_i$每个未匹配的ORB特征，我们在其他关键帧的未匹配点中进行查找，看是否有匹配上的特征点。这个匹配过程在第三部分第E节中有详细阐述，然后将那些不满足极线约束的匹配删除。ORB特征点对三角化后，需要对其在摄像头坐标系中的深度信息，视差，重投影误差和尺度一致性进行审查，通过后则将其作为新点插入地图。起初，一个地图云点通过2个关键帧观测，但它在其他关键帧中也有对应匹配点，所以它可以映射到其他相连的关键帧中，搜索算法的细则在本文第5部分D节中有讲述。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:6:3","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"D. 局部BA  局部BA主要优化当前处理的关键帧$K_i$，以及所有在covisibility graph里与$K_i$相连接的关键帧$K_c$，以及所有这些关键帧观测到的地图云点。所有其他能够观测到这些云点的关键帧，但没有连接$K_i$的会被保留在优化线程中，但保持不变。优化期间以及优化后，所有被标记为无效的观测数据都会被丢弃，附录有详细的优化细节。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:6:4","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"E. 局部关键帧筛选  为了使重构保持简洁，局部地图构建尽量检测冗余的关键帧，删除它们。这样不仅对BA过程会有很大帮助，因为随着关键帧数量的增加，BA优化的复杂度也随之增加，而且这能够在同一场景下运行下，关键帧的数量维持一个有限的数量。除非只有当场景内容改变了，关键帧的数量才会增加。这样一来，就增加了系统的可持续操作性。如果关键帧$K_c$中90%的点都可以被其他至少三个关键帧同时观测到，那认为Kc的存在是冗余的，我们则将其删除。这个尺度条件确保地图点拥有最精确测量的关键帧。这个策略受Tan等人的工作[24]的启发，在这项工作中，作者在经过一系列变化检测后即将关键帧删除。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:6:5","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"VII. 回环检测  闭环检测线程抽取$K_i$，即最后一帧局部地图关键帧，尝试用于检测以及闭合回环。具体步骤如下： ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:7:0","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"A. 候选关键帧  我们先计算$K_i$的词袋向量和它在covisibility graph中相邻图像（$θ_{min}=30$）的相似度，保留最低分值$s_{min}$。然后，我们检索图像识别数据库，丢掉那些分值低于$s_{min}$的关键帧。这和DBoW2中均值化分值的操作类似，可以获得好的鲁棒性，DBoW2中计算的是前一帧图像，而我们是使用的covisibility信息。此外，所有连接到Ki的关键帧都会从结果中删除。为了获得候选回环，我们必须检测3个一致的候选回环（covisibility graph中相连的关键帧）。如果对$K_i$来说环境样子都差不多，就可能有几个候选回环。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:7:1","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"B. 计算相似变换  单目SLAM系统有7个自由度，3个平移，3个旋转，1个尺度因子[6]。因此对于闭合回环，我们需要计算从当前关键帧$K_i$到回环关键帧$K_l$的相似变换，以获得回环的累积误差。计算相似变换也可以作为回环的几何验证。  我们先计算ORB特征关联的当前关键帧的地图云点和回环候选关键帧的对应关系，具体步骤如第3部分E节所示。此时，对每个候选回环，我们有了一个3D到3D的对应关系。我们对每个候选回环执行RANSAC迭代，通过Horn方法（如论文[42]）找到相似变换。如果我们用足够的有效数据找到相似变换$S_{il}$，我们就可以优化它，并搜索更多的对应关系。如果$S_{il}$有足够的有效数据，我们再优化它，直到$K_l$回环被接受。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:7:2","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"C. 回环融合  回环矫正的第一步是融合重复的地图云点，在covisibility graph中插入与回环相关的的新边缘。先通过相似变换$S_{il}$矫正当前关键帧位姿$T_{iw}$，这种矫正方法应用于所有与Ki相邻的关键帧，这样回环两端就可以对齐。然后，回环关键帧及其近邻能观测到的所有地图云点都映射到Ki及其近邻中，并在映射的区域附近小范围内搜索它的对应匹配点，如第5部分D节所述。所有匹配的地图云点和计算$S_{il}$过程中的有效数据进行融合。融合过程中所有的关键帧将会更新它们在covisibility graph中的边缘，创建的新边缘将用于回环检测。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:7:3","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["论文"],"content":"D. Essential Graph优化  为了有效地闭合回环，我们通过Essential Graph优化位姿图，如第三部分D节所示，这样可以将回环闭合的误差分散到图像中去。优化程序通过相似变换来校正尺度偏移，如论文[6]。误差和成本计算如附录所示。优化过后，每一个地图云点都根据关键帧的校正进行变换。 ","date":"2021-09-19","objectID":"/zh-cn/orb-slam/:7:4","tags":["翻译","论文","SLAM"],"title":"ORB-SLAM-译","uri":"/zh-cn/orb-slam/"},{"categories":["slam"],"content":"SLAM相关论文综合 ","date":"2021-09-17","objectID":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/:0:0","tags":["综述"],"title":"论文综述","uri":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/"},{"categories":["slam"],"content":"ORB-SLAM、ORB-SLAM2 目前最经典的基于特征点的视觉SLAM ","date":"2021-09-17","objectID":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/:1:0","tags":["综述"],"title":"论文综述","uri":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/"},{"categories":["slam"],"content":"DSO 目前最经典的基于直接法的视觉slam 在DSO中，提出了一种稀疏的、直接的单目视觉里程测量方法。完全直接的方法，联合优化所有涉及的模型参数的完全可能性，包括相机姿态，相机本质，和几何参数(反深度值)。与混合方法(SVO)形成对比，后者回归到联合模型优化的间接公式。 ","date":"2021-09-17","objectID":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/:2:0","tags":["综述"],"title":"论文综述","uri":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/"},{"categories":["slam"],"content":"SVO 半直接法视觉slam ","date":"2021-09-17","objectID":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/:3:0","tags":["综述"],"title":"论文综述","uri":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/"},{"categories":["slam"],"content":"LOAM 激光雷达SLAM ","date":"2021-09-17","objectID":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/:4:0","tags":["综述"],"title":"论文综述","uri":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/"},{"categories":["slam"],"content":"DEMO 同LOAM作者，视觉和激光雷达融合深度增强视觉里程计 ","date":"2021-09-17","objectID":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/:5:0","tags":["综述"],"title":"论文综述","uri":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/"},{"categories":["slam"],"content":"DVL-SLAM 视觉和激光雷达融合深度增强视觉里程计，直接法 ","date":"2021-09-17","objectID":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/:6:0","tags":["综述"],"title":"论文综述","uri":"/zh-cn/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/"},{"categories":null,"content":" Hugo 主题 FeelItHugo 主题 FeelIt \" Hugo 主题 FeelIt ","date":"2019-08-02","objectID":"/zh-cn/about/:0:0","tags":null,"title":"关于 FeelIt","uri":"/zh-cn/about/"},{"categories":null,"content":"特性 性能和 SEO  性能优化：在 Google PageSpeed Insights 中， 99/100 的移动设备得分和 100/100 的桌面设备得分  使用基于 JSON-LD 格式 的 SEO SCHEMA 文件进行 SEO 优化  支持 Google Analytics  支持 Fathom Analytics  支持 GoatCounter Analytics  支持搜索引擎的网站验证 (Google, Bind, Yandex and Baidu)  支持所有第三方库的 CDN  基于 lazysizes 自动转换图片为懒加载 外观和布局 / 响应式布局 / 浅色/深色 主题模式  全局一致的设计语言  支持分页  易用和自动展开的文章目录  支持多语言和国际化  美观的 CSS 动画 社交和评论系统  支持 Gravatar 头像  支持本地头像  支持多达 64 种社交链接  支持多达 28 种网站分享  支持 Disqus 评论系统  支持 Gitalk 评论系统  支持 Valine 评论系统  支持 Vssue 评论系统  支持 Facebook 评论系统  支持 Telegram comments 评论系统  支持 Commento 评论系统  支持 Utterances 评论系统 扩展功能  支持基于 Lunr.js 或 algolia 的搜索  支持 Twemoji  支持代码高亮  一键复制代码到剪贴板  支持基于 lightgallery.js 的图片画廊  支持 Font Awesome 图标的扩展 Markdown 语法  支持上标注释的扩展 Markdown 语法  支持分数的扩展 Markdown 语法  支持基于 $ \\KaTeX $ 的数学公式  支持基于 mermaid 的图表 shortcode  支持基于 ECharts 的交互式数据可视化 shortcode  支持基于 Mapbox GL JS 的 Mapbox shortcode  支持基于 APlayer 和 MetingJS 的音乐播放器 shortcode  支持 Bilibili 视频 shortcode  支持多种注释的 shortcode  支持自定义样式的 shortcode  支持自定义脚本的 shortcode  支持基于 TypeIt 的打字动画 shortcode  支持基于 cookieconsent 的 Cookie 许可横幅 … ","date":"2019-08-02","objectID":"/zh-cn/about/:0:1","tags":null,"title":"关于 FeelIt","uri":"/zh-cn/about/"},{"categories":null,"content":"许可协议 FeelIt 根据 MIT 许可协议授权。 更多信息请查看 LICENSE 文件。 FeelIt 主题中用到了以下项目，感谢它们的作者： modern-normalize Font Awesome Simple Icons Animate.css autocomplete.js Lunr.js algoliasearch lazysizes object-fit-images Twemoji lightgallery.js clipboard.js Sharer.js TypeIt $ \\KaTeX $ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine Vssue cookieconsent ","date":"2019-08-02","objectID":"/zh-cn/about/:0:2","tags":null,"title":"关于 FeelIt","uri":"/zh-cn/about/"}]