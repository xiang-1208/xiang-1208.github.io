<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>MOT - Xiang&#39;s Blog</title><meta name="description" content="上邪的个人博客"><meta property="og:title" content="MOT" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://xiang-1208.github.io/zh-cn/mot/" /><meta property="og:image" content="https://xiang-1208.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-08T18:19:00+08:00" />
<meta property="article:modified_time" content="2023-05-08T18:19:00+08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://xiang-1208.github.io/logo.png"/>

<meta name="twitter:title" content="MOT"/>
<meta name="twitter:description" content=""/>
<meta name="application-name" content="XIANG">
<meta name="apple-mobile-web-app-title" content="XIANG"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://xiang-1208.github.io/zh-cn/mot/" /><link rel="prev" href="https://xiang-1208.github.io/zh-cn/erasor/" /><link rel="next" href="https://xiang-1208.github.io/zh-cn/emoji-support/" /><link rel="stylesheet" href="/css/page.min.css"><link rel="stylesheet" href="/css/home.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "MOT",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/xiang-1208.github.io\/zh-cn\/mot\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/xiang-1208.github.io\/images\/Apple-Devices-Preview.webp",
                            "width":  1129 ,
                            "height":  614 
                        }],"genre": "posts","wordcount":  35725 ,
        "url": "https:\/\/xiang-1208.github.io\/zh-cn\/mot\/","datePublished": "2023-05-08T18:19:00+08:00","dateModified": "2023-05-08T18:19:00+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/xiang-1208.github.io\/images\/avatar.webp",
                    "width":  1079 ,
                    "height":  1080 
                }},"author": {
                "@type": "Person",
                "name": "xiang"
            },"description": ""
    }
    </script></head><body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('dark' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'dark' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/zh-cn/" title="Xiang&#39;s Blog"><span class="header-title-pre"><i class='fas fa-hand-holding-heart fa-fw'></i></span>XIANG</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/zh-cn/posts/"> 所有文章 </a><a class="menu-item" href="/zh-cn/tags/"> 标签 </a><a class="menu-item" href="/zh-cn/categories/"> 分类 </a><a class="menu-item" href="/zh-cn/categories/documentation/"> 文档 </a><a class="menu-item" href="/zh-cn/about/"> 关于 </a><a class="menu-item" href="https://github.com/xiang-1208/xiang-1208.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item language" title="选择语言">简体中文<i class="fas fa-chevron-right fa-fw"></i>
                        <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/zh-cn/mot/" selected>简体中文</option></select>
                    </span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="#" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="#" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a class="menu-item" href="/index.xml" title="RSS"><i class="fas fa-rss fa-fw" title="RSS"></i> </a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/zh-cn/" title="Xiang&#39;s Blog"><span class="header-title-pre"><i class='fas fa-hand-holding-heart fa-fw'></i></span>XIANG</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="#" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="#" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="#" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/zh-cn/posts/" title="">所有文章</a><a class="menu-item" href="/zh-cn/tags/" title="">标签</a><a class="menu-item" href="/zh-cn/categories/" title="">分类</a><a class="menu-item" href="/zh-cn/categories/documentation/" title="">文档</a><a class="menu-item" href="/zh-cn/about/" title="">关于</a><a class="menu-item" href="https://github.com/xiang-1208/xiang-1208.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i></a><div class="menu-item"><a href="/index.xml" title="RSS"><i class="fas fa-rss fa-fw" title="RSS"></i> </a>
                <span>&nbsp;|&nbsp;</span><a href="javascript:void(0);" class="theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div><span class="menu-item language" title="选择语言">简体中文<i class="fas fa-chevron-right fa-fw"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/zh-cn/mot/" selected>简体中文</option></select>
                </span></div>
    </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content always-active" id="toc-content-auto"></div>
        </div><article class="page single" data-toc="enable">

        

        <div class="single-card" ><h2 class="single-title animated flipInX">MOT</h2><div class="post-meta">
                <div class="post-meta-line"><span class="post-author"><a href="https://github.com/xiang-1208" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw"></i>xiang</a></span></div>
                <div class="post-meta-line"><span><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2023-05-08">2023-05-08</time></span>&nbsp;<span><i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 35725 字</span>&nbsp;
                    <span><i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 72 分钟</span>&nbsp;</div>
            </div>
            
            <hr><div class="details toc" id="toc-static"  data-kept="">
                    <div class="details-summary toc-title">
                        <span>目录</span>
                        <span><i class="details-icon fas fa-angle-right"></i></span>
                    </div>
                    <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#1introduction">1.Introduction</a></li>
    <li><a href="#2-related-work">2. Related Work</a>
      <ul>
        <li><a href="#21-3d-mot">2.1 3D MOT</a></li>
        <li><a href="#22-2d-mot">2.2 2D MOT</a></li>
      </ul>
    </li>
    <li><a href="#3-3d-mot-pipeline">3. 3D MOT Pipeline</a></li>
    <li><a href="#4-analyzing-and-improving-3d-mot">4. Analyzing and Improving 3D MOT</a>
      <ul>
        <li><a href="#41-预处理">4.1 预处理</a></li>
        <li><a href="#42-运动模型">4.2 运动模型</a></li>
        <li><a href="#43-关联">4.3 关联</a>
          <ul>
            <li><a href="#431-关联度量3d-giou">4.3.1 关联度量：3D GIoU</a></li>
          </ul>
        </li>
        <li><a href="#44-生命周期管理">4.4 生命周期管理</a></li>
        <li><a href="#45-simpletrack-的集成">4.5 SimpleTrack 的集成</a></li>
      </ul>
    </li>
    <li><a href="#5-rethinking-nuscenes">5. Rethinking nuScenes</a>
      <ul>
        <li><a href="#51-检测频率">5.1 检测频率</a></li>
        <li><a href="#52轨迹插值">5.2.轨迹插值</a></li>
      </ul>
    </li>
    <li><a href="#6-error-analyses">6. Error Analyses</a>
      <ul>
        <li><a href="#61上限实验设置">6.1.上限实验设置</a></li>
        <li><a href="#62-检测跟踪分析">6.2. “检测跟踪”分析</a></li>
      </ul>
    </li>
    <li><a href="#7-conclusions-and-future-work">7. Conclusions and Future Work</a></li>
  </ul>

  <ul>
    <li><a href="#1-introduction">1. Introduction</a></li>
    <li><a href="#2-related-work-1">2. Related work</a></li>
    <li><a href="#3-preliminaries">3. Preliminaries</a></li>
    <li><a href="#4-centerpoint">4. CenterPoint</a>
      <ul>
        <li><a href="#41两阶段中心点">4.1.两阶段中心点</a></li>
        <li><a href="#42-architecture">4.2. Architecture</a></li>
      </ul>
    </li>
    <li><a href="#5experiments">5.Experiments</a>
      <ul>
        <li><a href="#51-主要结果">5.1. 主要结果</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#1-introduction-1">1. Introduction</a>
      <ul>
        <li><a href="#11-related-work">1.1 Related Work</a></li>
        <li><a href="#12-contributions">1.2. Contributions</a></li>
      </ul>
    </li>
    <li><a href="#2-voxelnet">2. VoxelNet</a>
      <ul>
        <li><a href="#21-voxelnet架构">2.1 VoxelNet架构</a>
          <ul>
            <li><a href="#211-特征学习网络">2.1.1 特征学习网络</a></li>
            <li><a href="#212-卷积中间层">2.1.2 卷积中间层</a></li>
          </ul>
        </li>
        <li><a href="#22损失函数">2.2.损失函数</a></li>
        <li><a href="#23高效实施">2.3.高效实施</a></li>
      </ul>
    </li>
    <li><a href="#3-training-details">3. Training Details</a>
      <ul>
        <li><a href="#31网络详情">3.1.网络详情</a></li>
        <li><a href="#32-数据增强">3.2. 数据增强</a></li>
      </ul>
    </li>
    <li><a href="#4-experiments">4. Experiments</a>
      <ul>
        <li><a href="#41-kitti-验证集评估">4.1. KITTI 验证集评估</a></li>
        <li><a href="#42-kitti-测试集评估">4.2. KITTI 测试集评估</a></li>
      </ul>
    </li>
    <li><a href="#5-conclusion">5. Conclusion</a></li>
  </ul>

  <ul>
    <li><a href="#1-introduction-2">1. Introduction</a></li>
    <li><a href="#2-related-work-2">2. Related Work</a></li>
    <li><a href="#3-fully-sparse-voxel-based-network">3. Fully Sparse Voxel-based Network</a>
      <ul>
        <li><a href="#31-稀疏cnn主干自适应">3.1. 稀疏CNN主干自适应</a></li>
        <li><a href="#32-稀疏预测头">3.2. 稀疏预测头</a></li>
        <li><a href="#33-3d追踪">3.3. 3D追踪</a></li>
      </ul>
    </li>
    <li><a href="#4-experiments-1">4. Experiments</a>
      <ul>
        <li><a href="#41消融研究">4.1.消融研究</a></li>
        <li><a href="#42主要结果">4.2.主要结果</a></li>
      </ul>
    </li>
    <li><a href="#5-conclusion-and-discussion">5. Conclusion and Discussion</a></li>
  </ul>
</nav></div>
                </div><div class="content" id="content"><h1 id="simpletrack-understanding-and-rethinking-3d-multi-object-tracking">SimpleTrack: Understanding and Rethinking 3D Multi-object Tracking</h1>
<p><em>SimpleTrack：理解和重新思考 3D 多目标跟踪</em></p>
<p>近年来，3D 多目标跟踪 (MOT) 见证了许多新颖的基准和方法，尤其是那些在“检测跟踪”范式下的基准和方法。</p>
<p>尽管它们取得了进步和实用性，但尚未对其优缺点进行深入分析。在本文中，我们通过将当前的 3D MOT 方法分解为四个组成部分，将它们总结为一个统一的框架：<strong>检测预处理、关联、运动模型和生命周期管理</strong>。然后，我们将现有算法的失败案例归因于每个组件，并对其进行详细调查。基于分析，我们提出了相应的改进，从而形成了一个强大而简单的基线：SimpleTrack。 Waymo Open Dataset 和 nuScenes 的综合实验结果表明，我们的最终方法只需稍作修改即可获得最新的最新结果。</p>
<p>此外，我们采取了额外的步骤并重新思考当前的基准是否真实地反映了算法应对现实挑战的能力。我们深入研究了现有基准测试的细节并发现了一些有趣的事实。最后，我们分析了 SimpleTrack 中剩余故障的分布和原因，并提出了 3D MOT 的未来发展方向。我们的代码可在 <a href="https://github.com/TuSimple/SimpleTrack" target="_blank" rel="noopener noreffer">https://github.com/TuSimple/SimpleTrack</a> 获得</p>
<h2 id="1introduction">1.Introduction</h2>
<p>多目标跟踪 (MOT) 是计算机视觉中的一项复合任务，结合了定位和识别两个方面。鉴于其复杂性，MOT 系统通常涉及许多相互关联的部分，例如检测的选择、数据关联、对象运动的建模等。这些模块中的每一个都有其特殊的处理，并且可以显着影响整个系统的性能.因此，我们想问一下3D MOT中哪些组件起着最重要的作用，我们如何改进它们？</p>
<p>带着这样的目标，我们重新审视了当前的 3D MOT 算法 [3、10、28、37、43、44]。这些方法大多采用**“检测跟踪”范式**，它们直接从 3D 检测器中获取边界框并跨帧构建轨迹。我们首先将它们分解为四个单独的模块并检查每个模块：**输入检测的预处理、运动模型、关联和生命周期管理**。基于此模块化框架，我们将3D MOT的故障案例定位并归因于相应的组件，并发现了先前设计中被忽视的几个问题。</p>
<p>首先，我们发现<strong>不准确的输入检测可能会污染关联</strong>。然而，纯粹按分数阈值修剪它们会牺牲召回率。其次，我们发现<strong>需要仔细设计定义在两个 3D 边界框之间的相似性度量。基于距离和简单的 IoU 都效果不佳</strong>。第三，<strong>3D 空间中的对象运动比 2D 图像空间中的对象运动更可预测</strong>。因此，运动模型预测甚至不良观察（低分检测）之间的一致性可以很好地表明物体的存在。根据这些观察结果，我们提出了几个简单但重要的解决方案。对 Waymo Open Dataset [34] 和 nuScenes [8] 的评估表明，我们的最终方法“SimpleTrack”在 3D MOT 算法中具有竞争力（在表 6 和表 7 中）。</p>
<p>除了分析 3D MOT 算法外，我们还反思当前的基准。我们强调在<strong>评估中需要高频检测和正确处理输出轨迹</strong>。为了更好地理解我们方法的上限，我们根据 ID 开关和 MOTA 指标进一步分解剩余的错误。我们相信这些观察可以激发算法和基准的更好设计。</p>
<p>简而言之，我们的贡献如下：</p>
<ul>
<li>
<p>我们分解了“检测跟踪”3D MOT 框架的管道，并分析了每个组件与故障案例之间的联系。</p>
</li>
<li>
<p>我们为每个模块提出相应的处理方法，并将它们组合成一个简单的基线。结果在 Waymo 开放数据集上具有竞争力。</p>
</li>
</ul>
<h2 id="2-related-work">2. Related Work</h2>
<p>由于检测器的强大功能，大多数 3D MOT 方法 [3、10、28、37、43、44] 采用“检测跟踪”框架。我们首先总结了具有代表性的 3D MOT 工作，然后强调了 3D 和 2D MOT 之间的联系和区别。</p>
<h3 id="21-3d-mot">2.1 3D MOT</h3>
<p>许多 3D MOT 方法由手工制作的基于规则的组件组成。 AB3DMOT [37] 是使用 IoU 进行关联和使用卡尔曼滤波器作为运动模型的共同基线。其著名的追随者主要改进关联部分：Chiu 等人 [10] 和 CenterPoint [43] 用 Mahalanobis 和 L2 距离替换 IoU，这在 nuScenes [8] 上表现更好。其他一些人注意到生命周期管理的重要性，其中 CBMOT [3] 提出了一种基于分数的方法来取代“基于计数”的机制，Poschmann ¨ 等人 [28] 将 3D MOT 视为因子图上的优化问题。尽管这些改进非常有效，但仍非常需要对 3D MOT 方法进行系统研究，尤其是这些设计受到影响的地方以及如何进行进一步改进。为此，我们的论文力求达到预期。</p>
<p>与上述方法不同，许多其他方法试图以更少的手动设计来解决 3D MOT。 [2,9,15,38] 利用 RGB 图像的丰富特征进行关联和生命周期控制，Chiu 等人 [9]特别使用神经网络来处理特征融合、关联度量和轨迹初始化。最近，OGR3MOT [44] 遵循 Guillem 等人 [7] 并以端到端的方式使用图神经网络 (GNN) 解决 3D MOT，特别关注数据关联和活动轨迹的分类。</p>
<h3 id="22-2d-mot">2.2 2D MOT</h3>
<p>2D MOT 与 3D MOT 共享数据关联的共同目标。一些值得注意的尝试包括概率方法 [1、16、30、32]、动态规划 [11]、二分匹配 [6]、最小成本流 [4、46]、凸优化 [27、35、36、45]、和条件随机场 [42]。</p>
<p>随着深度学习的快速发展，许多方法 [7, 12–14, 19, 40] 学习匹配机制，而其他方法 [17, 20, 21, 24, 26] 学习关联度量。</p>
<p>与 3D MOT 类似，许多 2D 跟踪器 [5,22,33,48] 也受益于增强的检测质量并采用“检测跟踪”范式。然而，由于尺度变化，RGB 图像上的对象具有不同的大小；因此，它们对于关联和运动模型来说更难。但是 2D MOT 可以轻松利用丰富的 RGB 信息并使用外观模型 [18、19、33、39]，这是基于 LiDAR 的 3D MOT 所不具备的。总之，MOT 方法的设计应适合每种模态的特征。</p>
<h2 id="3-3d-mot-pipeline">3. 3D MOT Pipeline</h2>
<p>在本节中，我们将 3D MOT 方法分解为以下四个部分。图中有一个例子。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="D:%5chugo%5cblog%5ccontent%5cposts%5cMOT%5csimple1.bmp"
        data-srcset="D:%5chugo%5cblog%5ccontent%5cposts%5cMOT%5csimple1.bmp, D:%5chugo%5cblog%5ccontent%5cposts%5cMOT%5csimple1.bmp 1.5x, D:%5chugo%5cblog%5ccontent%5cposts%5cMOT%5csimple1.bmp 2x"
        data-sizes="auto"
        alt="D:\hugo\blog\content\posts\MOT\simple1.bmp"
        title="simple1" /></p>
<p>**输入检测的预处理：**它预处理来自检测器的边界框并选择用于跟踪的边界框。一些范例操作包括选择分数高于某个阈值的边界框。 （在“预处理”图 1 中，一些多余的边界框被删除。）</p>
<p>**运动模型：**它预测和更新对象的状态。大多数 3D MOT 方法 [3、10、37] 直接使用卡尔曼滤波器，而 CenterPoint [43] 使用检测器从多帧数据预测的速度。 （在“预测”和“运动模型更新”图 1 中。）</p>
<p>**关联：**它将检测与轨迹相关联。关联模块包括两个步骤：相似度计算和匹配。相似度测量一对检测和轨迹之间的距离，而匹配步骤根据预先计算的相似度解决对应关系。 AB3DMOT [37] 提出使用匈牙利算法的 IoU 的基线，而 Chiu 等人 [10] 使用 Mahalanobis 距离和贪婪算法，CenterPoint [43] 采用 L2 距离。 （在“关联”图 1 中。）</p>
<p>**生命周期管理：**它控制着“出生”、“死亡”和“输出”政策。 “Birth”决定一个检测边界框是否会被初始化为一个新的tracklet； “死亡”在认为已移出注意区域时移除轨迹； “Output”决定了一个tracklet是否会输出它的状态。大多数 MOT 算法采用简单的基于计数的规则 [10,37,43]，CBMOT [3] 通过修改 tracklet 置信度的逻辑来改进 birth and death。 （在“生命周期管理”图 1 中。）</p>
<h2 id="4-analyzing-and-improving-3d-mot">4. Analyzing and Improving 3D MOT</h2>
<p>在本节中，我们将分析和改进 3D MOT 流水线中的每个模块。为了更好地说明，我们通过将每个修改的影响从 SimpleTrack 的最终变体中删除来消除它。默认情况下，消融都在使用 CenterPoint [43] 检测的验证拆分上。我们还在4.5节提供了附加消融分析以及与第 1 节中其他方法的比较。</p>
<h3 id="41-预处理">4.1 预处理</h3>
<p>为了满足召回要求，当前的检测器通常输出大量边界框，其分数粗略地表明它们的质量。然而，如果这些框在 3D MOT 的关联步骤中被平等对待，质量低或重叠严重的边界框可能会偏离跟踪器以选择不准确的检测来扩展或形成轨迹（如图 2的“原始检测”所示）。检测和 MOT 任务之间的这种差距需要仔细处理。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple2.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图2.分数过滤和NMS的比较。要去除第 2 行的冗余边界框，分数过滤至少需要 0.24 的阈值，但这会消除第 1 行的检测。但是，NMS 可以通过去除第 2 行的重叠并保持第 1 行的召回率来很好地满足两者.</div>
</center>
<p>3D MOT 方法通常使用置信度分数来过滤掉低质量检测并提高 MOT 输入边界框的精度。然而，这种方法可能不利于召回，因为它们直接放弃了观察不佳的对象（图 2 中的第一行）。它对像 AMOTA 这样的指标也特别有害，它需要跟踪器使用低分边界框来满足召回要求。</p>
<p>为了在不显着降低召回率的情况下提高精度，我们的解决方案简单直接：我们对输入检测应用更严格的非最大抑制 (NMS)。如图 2 右侧所示，单独的 NMS 操作可以有效地消除重叠的低质量边界框，同时保持多样化的低质量观察，即使在稀疏点或遮挡等区域也是如此。因此，通过在预处理模块中加入NMS，我们可以大致保持召回率，但大大提高了精度和好处MOT</p>
<p>在 WOD 上，我们更严格的 NMS 操作移除了 51% 和 52% 的车辆和行人边界框，精度几乎翻了一番：车辆从 10.8% 到 21.1%，行人从 5.1% 到 9.9%。与此同时，车辆召回率从 78% 降至 74%，行人召回率从 83% 降至 79%。根据表 1 和表2，这在很大程度上有利于性能，尤其在行人（表 2 的右侧），目标检测任务更难。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple_t_1_2.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>
<h3 id="42-运动模型">4.2 运动模型</h3>
<p>运动模型描述了轨迹的运动状态。它们主要用于预测下一帧中对象的候选状态，这些状态是后续关联步骤的建议。此外，像卡尔曼滤波器这样的运动模型也可以潜在地细化物体的状态。通常，3D MOT 有两种常用的运动模型：卡尔曼滤波器 (KF)，例如AB3DMOT [37]，以及具有来自检测器的预测速度的恒速模型（CV），例如中心点 [43]。KF 的优点是它可以利用来自多个帧的信息，并在面对低质量检测时提供更平滑的结果。同时，CV 通过其明确的速度预测更好地处理突然和不可预测的运动，但其在运动平滑方面的效果有限。在表 3 和表 4，我们在WOD和nuScenes上比较了他们两个，这为我们的说法提供了明确的证据。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple_t_3_4.bmp">
</center>
<p>一般来说，这两个运动模型表现出相似的性能。在 nuScenes 上，CV 略胜 KF，而在 WOD 上则相反。 KF 在 WOD 上的优势主要来自于对边界框的细化。为了验证这一点，我们实现了“KF-PD”变体，它仅使用 KF 来提供关联之前的运动预测，并且输出都是原始检测。最终，Tab 中“CV”和“KF-PD”之间的边际差距。 表3 支持我们的主张。在 nuScenes 上，由于 nuScenes (2Hz) 的帧速率较低，CV 运动模型稍微好一些。为了证明我们的猜想，我们在 nuScenes 1</p>
<p>上应用了更高频率 10Hz 设置下的 KF 和 CV，这次 KF 在 AMOTA 中略胜 CV 0.696 和 0.693。</p>
<p>总而言之，由于更可预测的运动，卡尔曼滤波器更适合高频率情况，而恒速模型对于具有显式速度预测的低频场景更稳健。由于推断速度对于检测器而言还不常见，因此我们在不失一般性的情况下为 SimpleTrack 采用卡尔曼滤波器。</p>
<h3 id="43-关联">4.3 关联</h3>
<h4 id="431-关联度量3d-giou">4.3.1 关联度量：3D GIoU</h4>
<p>基于 IoU [37] 和基于距离的 [10,43] 关联度量是 3D MOT 中的两个普遍选择。如图 3 所示，它们具有典型但不同的故障模式。 IoU 计算边界框之间的重叠率，因此如果检测和运动预测之间的 IoU 全部为零，则它无法连接检测和运动预测，这在 tracklets 的开始或具有突然运动的物体上很常见（图 3 的左侧）。基于距离的度量的代表是 Mahalanobis [10] 和 L2 [43] 距离。使用较大的距离阈值，它们可以处理基于 IoU 的度量的失败情况，但它们可能对低质量的附近检测不够敏感。我们在图 3 的右侧解释了这种情况。在第 k 帧上，蓝色运动预测与绿色误报检测的 L2 距离较小，因此它被错误关联。通过这样的例子，我们得出结论，基于距离的度量缺乏方向区分，这正是基于 IOU 的度量的优势。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple3.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图3.关联指标的说明。左图：IoU 与 GIoU。右图：L2 距离与 GIoU。详情在4.3.1节.</div>
</center>
<p>为了充分利用两个体系，我们建议将“广义 IoU”（GIoU）[31] 推广到 3D 以进行关联。简而言之，对于任何一对 3D 边界框 B1、B2，它们的 3D GIoU 如下式（1）所示。其中I，U是B1和B2的交集和并集。C 是 U 的封闭凸包。 V 表示多边形的体积。我们将 GIoU &gt; -0.5 设置为每个类别对象的阈值在WOD和nuScenes上为这对关联进入后续的匹配步骤。
$$
\begin{aligned}
V_U &amp; =V_{B_1}+V_{B_2}-V_I \<br>
\operatorname{GIoU}\left(B_1, B_2\right) &amp; =V_I / V_U-\left(V_C-V_U\right) / V_C .
\end{aligned}
$$
如图 3 所示，GIoU 指标可以处理两种故障模式。图 4 中的定量结果还显示了 GIoU 改善 WOD 和 nuScenes 关联的能力。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple4.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图4.WOD（左和中）和 nuScenes（右）的关联指标比较。 “M-Dis”是马氏距离的缩写。最佳方法最靠近右下角，具有最低的 ID 开关和最高的 MOTA/AMOTA。</div>
</center>
<p>一般来说，检测和轨迹之间的匹配有两种方法：1）将问题表述为二分匹配问题，然后使用匈牙利算法[37]求解。 2) 通过贪心算法迭代关联最近的对[10, 43]。</p>
<p>我们发现这两种方法与关联度量严重耦合：基于 IoU 的度量对两者都很好，而基于距离的度量更喜欢贪心算法。我们假设原因是基于距离的度量范围很大，因此优化全局最优解的方法，如匈牙利算法，可能会受到异常值的不利影响。在图 5 中，我们对 WOD 上的匹配策略和关联度量之间的所有组合进行了实验。正如所证明的，IoU 和 GIoU 对于这两种策略都很好，而 Mahalanobis 和 L2 距离需要贪心算法，这也与之前工作的结论一致 [10]。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple5.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图5.WOD上的匹配策略比较。</div>
</center>
<h3 id="44-生命周期管理">4.4 生命周期管理</h3>
<p>我们分析了 WOD2 上的所有 ID-Switch，并将它们分为两组，如图 6 所示：错误关联和提前终止。与许多工作的主要重点不同，即关联，我们发现提前终止实际上是  ID-Switch的主要原因：车辆为 95%，行人为 91%。在早期终止中，许多是由点云稀疏性和空间遮挡引起的。为了缓解这个问题，我们利用免费但有效的信息：运动模型和低分检测之间的共识。这些边界框通常定位质量较低，但如果它们与运动预测一致，则它们是对象存在的有力指示。然后我们使用这些来延长 tracklet 的寿命。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple6.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图6.两种主要类型 ID-Switches的图示。</div>
</center>
<p>怀着这样的动机，我们提出了“两级关联”。具体来说，我们应用两轮具有不同分数阈值的关联：一个低 Tl 和一个高 Th（例如，WOD 上的行人为 0.1 和 0.5）。在第一阶段，我们使用与大多数当前算法 [10、37、43] 相同的过程：只有得分高于 Th 的边界框用于关联。在第二阶段，我们专注于与第一阶段检测不匹配的轨迹，并且放宽他们的匹配条件：具有大于 Tl 的分数的检测对于匹配来说是足够的。如果 tracklet 在第二阶段成功与一个边界框相关联，它仍将保持活动状态。然而，由于低分检测通常质量较差，我们不输出它们以避免误报，它们也不用于更新运动模型。相反，我们使用运动预测作为最新的轨迹状态，取代低质量检测。</p>
<p>我们在图 7 中直观地解释了我们的“Twostage Association”和传统的“One-stage Association”之间的区别。假设 T = 0.5 是过滤检测边界框的原始分数阈值，那么跟踪器将忽略分数为 0.4 和0.2 在第 3 帧和第 4 帧上，由于连续帧中缺少匹配项而死亡，这最终会导致最终的 ID 切换。相比之下，我们的两阶段关联可以保持 tracklet 的活动状态。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple7.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图7.“单阶段”和“两阶段”关联与假设示例的比较。 “Extend”的意思是“延长生命周期”，“Predict”的意思是“由于没有关联而使用运动预测”。假设 Th = 0.5 和 Tl = 0.1 是分数阈值，“one-stage”方法会因为连续缺少关联而提前终止 tracklet。第4.4节的详细信息。</div>
</center>
<p>在表 5中，我们的方法在不伤害 MOTA 的情况下大大减少了 ID-Switches。这证明SimpleTrack通过更灵活地使用检测来有效地延长生命周期。与我们的工作并行，类似的方法也被证明对 2D MOT [47] 有用。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple_t_5.bmp">
</center>
<h3 id="45-simpletrack-的集成">4.5 SimpleTrack 的集成</h3>
<p>在本节中，我们将上述技术集成到统一的 SimpleTrack 中，并演示它们如何逐步提高性能。</p>
<p>在图 8 中，我们说明了 3D MOT 跟踪器的性能如何从基线提高。在WOD上，虽然车辆和行人的属性有很大不同，但每种技术都适用。在 nuScenes 上，每项改进建议都对 AMOTA 和 ID-Switch 都有效。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple8.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图8.SimpleTrack 在 WOD（左和中）和 nuScenes（右）上的改进。我们在 WOD 上使用 AB3DMOT [37]，在 nuScenes 上使用 Chiu 等人 [10] 的公共基线。对于 nuScenes，“10Hz-Two”（使用 10Hz 检测和两级关联）的改进在5.1节中。“Pred”（输出运动模型预测）在第5.2节。修改的名称位于 x 轴上。更好的 MOTA 和 ID-Switch 值在 y 轴上更高，以实现更清晰的可视化。</div>
</center>
<p>我们还报告了测试集的性能，并与其他 3D MOT 方法进行了比较。将我们的技术结合起来会产生新的最先进的结果（在表 6 和表 7 中）。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple_t_6_7.bmp">
</center>
<h2 id="5-rethinking-nuscenes">5. Rethinking nuScenes</h2>
<p>除了上述技术外，我们还深入研究了基准测试的设计。这些基准极大地促进了研究的发展并指导了算法的设计。对比WOD和nuScenes，我们有以下发现：1）nuScenes的帧率为2Hz，而WOD为10Hz。如此低的频率给 3D MOT（第5.1 节）增加了不必要的困难。 2）nuScenes的评估需要高召回率和低分数阈值。它还通过插值预处理轨迹，这鼓励跟踪器输出反映整个轨迹质量的置信度分数，而不是帧质量（第 5.2 节）。我们希望这两项发现能够激发社区重新思考 3D 跟踪的基准和评估协议。</p>
<h3 id="51-检测频率">5.1 检测频率</h3>
<p>跟踪通常受益于更高的帧速率，因为运动在短时间间隔内更容易预测。我们在表8中比较两个基准的点云、注释和常见 MOT 帧速率的频率。在 nuScenes 上，它有 20Hz 点云但只有 2Hz 注释。这导致大多数常见的检测器和 3D MOT 算法在 2Hz 下工作，即使它们实际上利用了所有 20Hz LiDAR 数据并且运行速度快于 2Hz。因此，我们研究高频数据的影响如下。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="simple_t_8.bmp">
</center>
<p>尽管高频 (HF) 帧的信息更丰富，但合并它们并非易事，因为 nuScenes 仅对低频帧进行评估，我们称之为“评估帧”。在表9中，简单地使用所有 10Hz 帧并不能提高性能。这是因为 HF 帧上的低质量检测可能会偏离跟踪器并损害采样评估帧的性能。为了克服这个问题，我们首先在 HF 帧上应用“单阶段关联”进行探索，其中仅考虑得分大于 Th = 0.5 的边界框并将其用于运动模型更新。然后，我们采用“双阶段关联”（在第 4.4 节中描述），使用得分大于 Tl = 0.1 的框来扩展轨迹。就像在表9中一样，我们的方法显着改善了 AMOTA 和 ID 开关。我们甚至尝试将帧速率提高到 20Hz，但由于偏差问题，这几乎无法带来进一步的改进。所以 SimpleTrack 在我们最终提交给测试集时使用了 10Hz 设置。</p>
<h3 id="52轨迹插值">5.2.轨迹插值</h3>
<p>nuScenes 中使用的 AMOTA 指标计算不同召回阈值下的平均 MOTAR [37]，这需要跟踪器输出所有分数段的框。为了进一步提高召回率，我们在没有关联检测边界框的情况下输出帧和轨迹的运动模型预测，并根据经验为它们分配比任何其他检测更低的分数。在我们的例子中，它们的分数是 0.01 × SP，其中 SP 是前一帧中轨迹的置信度分数。如表10所示。这个简单的技巧提高了整体召回率和AMOTA。</p>
<p>然而，我们发现提高召回率并不是这种改进的唯一原因。除了边界框，运动模型预测的分数也做出了重要贡献。这从 nuScenes 上的评估协议开始，他们在其中插入输入轨迹以填充缺失的帧并使用其轨迹平均分数更改所有分数，如图 9 所示。在这种情况下，我们的方法可以明确地惩罚低-quality tracklets，通常包含更多由运动模型预测替换的缺失框。</p>
<p>总而言之，nuScenes 上的这种插值鼓励跟踪器整体处理轨迹质量并输出校准的质量感知分数。然而，即使对于相同的 tracklet，boxes的质量也可能在帧之间有很大差异，因此我们建议仅用一个分数来描述 tracklet 的质量是不完美的。此外，在这个插值步骤中还引入了未来信息，它改变了 tracklet 结果。这也可能引发人们对评估设置是否仍然是完全在线的担忧。</p>
<h2 id="6-error-analyses">6. Error Analyses</h2>
<p>在本节中，我们对 SimpleTrack 剩余的失败案例进行分析，并提出改进“检测跟踪”范式的潜在未来方向。不失一般性，我们以WOD为例。</p>
<h3 id="61上限实验设置">6.1.上限实验设置</h3>
<p>为了定量评估失败案例的原因，我们将 SimpleTrack 与两种不同的预言机变体进行了对比。结果总结在表11中。</p>
<p><strong>GT Output</strong> 消除了由“输出”策略引起的错误。我们在“输出”阶段计算来自 SimpleTrack 的边界框与 GT 框之间的 IoU，然后使用 IoU 来决定是否应该输出一个框而不是检测分数。</p>
<p><strong>GT All</strong> 是 CenterPoint 框跟踪性能的上限。我们贪婪地匹配从 CenterPoint 到 GT 框的检测，保持真阳性并为它们分配地面真实 ID。</p>
<h3 id="62-检测跟踪分析">6.2. “检测跟踪”分析</h3>
<p>**ID-Switches：**我们分解了ID-Switches的原因，如图 6 所示。尽管通过“两阶段关联”，车辆提前终止的比例大大降低了 86%，行人提前终止的比例降低了 70%，但它仍然占 88%，车辆和行人的 SimpleTrack 中其余ID-Switches的故障案例分别为 72%。<u>我们检查这些案例并发现它们中的大多数是由于长期遮挡或暂时看不见的物体返回。因此，除了改进关联之外，未来的潜在工作还可以开发像 2D MOT [18、19、33、39] 中的外观模型，或者在它们返回后静默维护它们的状态以重新识别这些对象。</u></p>
<p>**FP and FN：**选项卡中的“GT All”。图 11 显示了使用 CenterPoint [43] 检测的 MOT 上限，我们以车辆类别为例进行分析。即使使用“GT All”，false negatives 仍然是 0.215，这是 detection FN，在“tracking by detection”框架下很难修复。比较“GT All”和SimpleTrack，我们发现跟踪算法本身引入了0.119个漏报。我们将它们进一步分解如下。具体来说，“GT Output”和“GT ALL”之间的差异表明 0.043 个假阴性是由 NMS 和预处理中的分数阈值产生的未初始化轨迹引起的。其他来自生命周期管理。 “Initialization”在输出tracklet之前需要两帧的积累，这与AB3DMOT [37]相同。这会产生边际 0.005 的假阴性。我们的“输出”逻辑使用检测分数来决定输出与否，占误报数 0.076。基于这些分析，我们可以得出结论，差距主要是由分数和检测质量之间的不一致造成的。通过使用历史信息，与单帧检测器相比，3D MOT 可能会提供更好的分数，这最近已经引起了一些关注 [3, 44]。</p>
<h2 id="7-conclusions-and-future-work">7. Conclusions and Future Work</h2>
<p>在本文中，我们将“检测跟踪”3D MOT 算法解耦为几个组件，并分析了它们的典型故障。有了这样的见解，我们提出了使用 NMS、GIoU 和两阶段关联的相应增强，这导致了我们的 SimpleTrack。此外，我们还重新考虑了 nuScenes 中的帧率和插值预处理。我们最终指出了“通过检测跟踪”3D MOT 的几个可能的未来方向。</p>
<p>然而，除了“检测跟踪”范式之外，还有一些具有巨大潜力的分支。为了获得更好的边界框质量，3D MOT 可以使用长期信息 [25、29、41] 对其进行细化，这被证明优于仅基于局部帧的检测。未来的工作还可以将当前基于手动规则的方法转化为基于学习，例如使用基于学习的帧内机制代替NMS，使用帧间推理代替3D GIoU和生命周期管理等。</p>
<p>**致谢：**我们要感谢 Tianwei Yin 在我们将 CenterPoint 检测应用于 3D 多目标跟踪过程中的热心帮助。</p>
<h1 id="center-based-3d-object-detection-and-tracking">Center-based 3D Object Detection and Tracking</h1>
<p>基于中心的 3D 对象检测和跟踪</p>
<p>**摘要：**三维对象通常表示为点云中的 3D 框。这种表示模仿了经过充分研究的基于图像的 2D 边界框检测，但带来了额外的挑战。 <u>3D 世界中的对象不遵循任何特定方向，基于框的检测器难以枚举所有方向或将轴对齐的边界框拟合到旋转的对象。</u>在本文中，我们建议将 3D 对象表示、检测和跟踪为点。我们的框架 CenterPoint 首先使用关键点检测器检测对象的中心，然后回归到其他属性，包括 3D 大小、3D 方向和速度。在第二阶段，它使用对象上的附加点特征来改进这些估计。在 CenterPoint 中，3D 对象跟踪简化为贪心最近点匹配。由此产生的检测和跟踪算法简单、高效且有效。 CenterPoint 在 nuScenes 基准测试中实现了 3D 检测和跟踪的一流性能，单个模型的 NDS 为 65.5，AMOTA 为 63.8。在 Waymo Open Dataset 上，CenterPoint 的表现大大优于之前所有的单一模型方法，在所有仅 Lidar-only 的提交中排名第一。代码和预训练模型可在 <a href="https://github.com/tianweiy/CenterPoint" target="_blank" rel="noopener noreffer">https://github.com/tianweiy/CenterPoint</a> 获得。</p>
<h2 id="1-introduction">1. Introduction</h2>
<p>强大的 3D 感知是许多最先进的驾驶系统的核心要素 [1, 48]。与研究充分的 2D 检测问题相比，点云上的 3D 检测提出了一系列有趣的挑战：首先，点云是稀疏的，3D 对象的大部分都没有测量 [22]。其次，生成的输出是一个三维框，通常不能很好地与任何全局坐标系对齐。第三，3D 物体的尺寸、形状和纵横比范围很广，例如，在交通领域，自行车接近平面，公共汽车和豪华轿车被拉长，行人很高。2D和3D检测之间的这些显著差异使得两个领域之间的思想转移变得更加困难[43,45,58]。轴对齐的 2D 框 [16, 17] 是自由形式 3D 对象的不良代理。一种解决方案可能是为每个对象方向 [56、57] 分类不同的模板（锚点），但这不必要地增加了计算负担，并可能引入大量潜在的误报检测。我们认为，连接 2D 和 3D 域的主要潜在挑战在于对象的这种表示。</p>
<p>在本文中，我们展示了如何将物体表示为点（图1）大大简化了3D识别。我们的两阶段3D检测器CenterPoint使用关键点检测器[62]查找对象的中心及其属性，第二阶段改进所有估计。==具体来说，CenterPoint 使用标准的基于激光雷达的主干网络，即 VoxelNet [54、64] 或 PointPillars [27]，来构建输入点云的表示。==然后它将这种表示扁平化为俯视图，并使用标准的基于图像的关键点检测器来查找对象中心 [62]。对于每个检测到的中心，它会从中心位置的点特征回归到所有其他对象属性，例如 3D 大小、方向和速度。此外，我们使用轻量级第二阶段来细化对象位置。==第二阶段在估计对象 3D 边界框的每个面的 3D 中心提取点特征。它恢复了由于跨步和有限的感受野而丢失的局部几何信息，并以较小的成本带来了不错的性能提升。==</p>
<p>基于中心的表示有几个关键优势：首先，与边界框不同，点没有固有方向。这极大地减少了对象检测器的搜索空间，并允许主干学习对象的旋转不变性和等价性。其次，基于中心的表示简化了下游任务，例如跟踪。如果对象是点，那么轨迹就是空间和时间中的路径。 CenterPoint 预测连续帧之间对象的相对偏移（速度）并贪婪地链接对象。第三，基于点的特征提取使我们能够设计一个有效的两阶段细化模块，它比以前的方法快得多 [42-44]。</p>
<p>我们在两个流行的大型数据集上测试我们的模型：Waymo Open [46] 和 nuScenes [6]。我们表明，从框表示到基于中心的表示的简单切换会在不同骨干网下的 3D 检测中产生 3-4 mAP 的增加 [27、54、64、65]。两阶段细化进一步带来了额外的 2 mAP 提升，计算开销很小 (&lt; 10%)。我们最好的单一模型在 Waymo 上实现了 71.8 和 66.4 的 2 级 mAPH 车辆和行人检测，在 nuScenes 上实现了 58.0 mAP 和 65.5 NDS，在这两个数据集上的性能优于所有已发布的方法。值得注意的是，在 NeurIPS 2020 nuScenes 3D 检测挑战中，CenterPoint 构成了前 4 名获奖作品中的 3 个的基础。对于 3D 跟踪，我们的模型在 nuScenes 上的性能为 63.8 AMOTA，比之前的最新技术高出 8.8 AMOTA。在 Waymo 3D 跟踪基准测试中，我们的模型在车辆和行人跟踪方面分别达到了 59.4 和 56.6 的 2 级 MOTA，比以前的方法高出 50%。我们的端到端 3D 检测和跟踪系统几乎实时运行，在 Waymo 上为 11 FPS，在 nuScenes 上为 16 FPS。</p>
<h2 id="2-related-work-1">2. Related work</h2>
<p><strong>二维对象检测</strong>从图像输入中预测轴对齐边界框。 RCNN 家族 [16,17,20,41] 找到类别不可知的边界框候选者，然后对其进行分类和改进。 YOLO [40]、SSD [32] 和 RetinaNet [31]直接找到一个特定类别的框候选，回避后面的分类和细化。基于中心的检测器，例如CenterNet [62] 或 CenterTrack [61]，直接检测隐式对象中心点而不需要候选框。许多 3D 检测器 [19、43、45、58] 从这些 2D 检测器演变而来。我们表明基于中心的表示 [61、62] 非常适合 3D 应用。</p>
<p><strong>3D 对象检测</strong>旨在预测三维旋转边界框 [11,15,27,30,37,54,58,59]。它们不同于输入编码器上的二维检测器。 Vote3Deep [12] 利用以特征为中心的投票 [49] 来有效地处理等间距 3D 体素上的稀疏 3D 点云。 VoxelNet [64] 在每个体素内使用 PointNet [38] 来生成统一的特征表示，具有 3D 稀疏卷积 [18] 和 2D 卷积的头部从中产生检测。 SECOND [54] 简化了 VoxelNet 并加速了稀疏 3D 卷积。 PIXOR [55] 将所有点投影到具有 3D 占用和点强度信息的 2D 特征图上，以去除昂贵的 3D 卷积。PointPillars [27] 用支柱表示代替所有体素计算，每个地图位置一个高大的细长体素，提高了主干效率。 MVF [63] 和 Pillar-od [50] 结合多视图特征来学习更有效的支柱表示。我们的贡献集中在输出表示上，并且与任何 3D 编码器兼容，并且可以改进它们。</p>
<p>VoteNet [36] 通过使用点特征采样和分组的投票聚类来检测对象。相比之下，我们直接通过中心点的特征回归到 3D 边界框而不进行投票。 Wong 等人 [53] 和 Chen 等人 [8] 在对象中心区域（即点锚点）使用类似的多点表示并回归到其他属性。我们为每个对象使用一个正单元格，并使用关键点估计损失。</p>
<p>**两阶段 3D 对象检测：**最近的工作考虑将 RCNN 样式的 2D 检测器直接应用于 3D 域 [9, 42–44, 59]。他们中的大多数应用 RoIPool [41] 或 RoIAlign [20] 在 3D 空间中聚合特定于 RoI 的特征，使用基于 PointNet 的点 [43] 或体素 [42] 特征提取器。这些方法从 3D 激光雷达测量（点和体素）中提取区域特征，由于大量的点导致运行时间过长。相反，我们从中间特征图中提取 5 个表面中心点的稀疏特征。这使得我们的第二阶段非常高效并保持有效。</p>
<p>**3D 对象跟踪。**许多 2D 跟踪算法 [2、4、26、52] 很容易开箱即用地跟踪 3D 对象。然而，基于 3D 卡尔曼滤波器 [10、51] 的专用 3D 跟踪器仍然具有优势，因为它们可以更好地利用场景中的三维运动。在这里，我们采用了一种遵循 CenterTrack [61] 的更简单的方法。我们使用速度估计和基于点的检测来通过多个帧跟踪对象的中心。</p>
<h2 id="3-preliminaries">3. Preliminaries</h2>
<p><strong>2D CenterNet</strong> [62] 将目标检测改写为关键点估计。它采用输入图像并为 K 类中的每一个预测 w×h 热图 $\hat{Y} \in[0,1]^{w \times h \times K}$。输出热图中的每个局部最大值（即，其值大于其八个邻居的像素）对应于检测到的对象的中心。为了检索 2D 框，CenterNet 回归到在所有类别之间共享的大小映射$\hat{Y} \in$ ^S ∈ R w×h×2。对于每个检测对象，尺寸图将其宽度和高度存储在中心位置。 CenterNet 架构使用标准的全卷积图像主干，并在顶部添加了一个密集的预测头。在训练期间，CenterNet 学习在每个类 ci ∈ {1 的每个注释对象中心 qi 处使用渲染高斯核预测热图。 . . K}，并回归到带注释的边界框中心的对象大小 S。为了弥补骨干架构跨步引入的量化误差，CenterNet 还回归到局部偏移 ^O。</p>
<p>在测试时，检测器会生成 K 个热图和密集的类不可知回归图。热图中的每个局部最大值（峰值）对应一个对象，置信度与峰值处的热图值成正比。对于每个检测到的对象，检测器从相应峰值位置的回归图中检索所有回归值。根据应用领域，可能需要非极大值抑制 (NMS)。</p>
<p><strong>3D 检测</strong>设 P = {(x, y, z, r)i} 是 3D 位置 (x, y, z) 和反射率 r 测量的无序点云。 3D 目标检测旨在从该点云预测鸟瞰图中的一组 3D 目标边界框 B = {bk}。每个边界框 b = (u, v, d, w, l, h, α) 由相对于物体地平面的中心位置 (u, v, d) 和 3D 大小 (w, l, h) 组成, 和由偏航 α 表示的旋转。在不失一般性的情况下，我们使用以自我为中心的坐标系，传感器位于 (0, 0, 0) 且偏航 = 0。</p>
<p>现代 3D 对象检测器 [19、27、54、64] 使用 3D 编码器将点云量化为常规 bin。基于点的网络 [38] 然后提取 bin 内所有点的特征。然后 3D 编码器将这些特征合并到它的主要特征表示中。大多数计算发生在主干网络中，主干网络仅对这些量化和合并的特征表示进行操作。主干网络的输出是一个地图视图特征图 M ∈ RW×L×F，宽度为 W，长度为 L，在地图视图参考框架中有 F 个通道。宽度和高度都与单个体素箱的分辨率和骨干网络的步幅直接相关。常见的主干包括 VoxelNet [54、64] 和 PointPillars [27]。</p>
<p>使用地图视图特征图 M 检测头，最常见的是单级 [31] 或两级 [41] 边界框检测器，然后从锚定在该开销特征图上的一些预定义边界框生成对象检测。由于 3D 边界框具有各种尺寸和方向，基于锚点的 3D 检测器很难将轴对齐的 2D 框拟合到 3D 对象。此外，在训练过程中，之前基于锚点的 3D 检测器依赖 2D Box IoU 进行目标分配 [42、54]，这给为不同类别或不同数据集选择正/负阈值带来了不必要的负担。在下一节中，我们将展示如何基于点表示构建原理性 3D 对象检测和跟踪模型。我们引入了一种新颖的基于中心的检测头，但依赖于现有的 3D 主干（VoxelNet 或 PointPillars）。</p>
<h2 id="4-centerpoint">4. CenterPoint</h2>
<p>图 2 显示了 CenterPoint 模型的总体框架。令 M ∈ RW×H×F 为 3D backbone 的输出。 CenterPoint 的第一阶段预测特定类别的热图、对象大小、亚体素位置细化、旋转和速度。所有输出都是密集预测。</p>
<p><strong>中心热图头：</strong> center-head 的目标是在任何检测到的对象的中心位置产生一个热图峰值。这个头产生一个 K 通道热图 ^Y ，每个 K 类一个通道。在训练期间，它的目标是通过将带注释的边界框的 3D 中心投影到地图视图中而生成的 2D 高斯分布。我们使用焦点损失 [28, 62]。自上而下的地图视图中的对象比图像中的对象稀疏。在地图视图中，距离是绝对的，而图像视图通过透视扭曲了它们。考虑一个道路场景，在 mapview 中车辆占据的区域很小，但在 image-view 中，一些大物体可能占据了大部分屏幕。此外，透视投影中深度维度的压缩自然会使对象中心在图像视图中彼此更靠近。遵循 CenterNet [62] 的标准监督会产生非常稀疏的监督信号，其中大多数位置都被视为背景。为了抵消这一点，我们通过扩大在每个地面实况对象中心呈现的高斯峰来增加对目标热图 Y 的正监督。具体来说，我们将高斯半径设置为 σ = max(f(wl), τ )，其中 τ = 2 是允许的最小高斯半径，f 是 CornerNet [28] 中定义的半径函数。这样，CenterPoint 保持了基于中心的目标分配的简单性；该模型从附近的像素得到更密集的监督。</p>
<p>**回归头：**我们在对象的中心特征处存储了几个对象属性：子体素位置细化 o ∈ R 2 、地上高度 hg ∈ R、3D 大小 s ∈ R 3 和偏航角 (sin(α) , cos(α)) ∈ [−1, 1]2 。子体素位置细化 o 减少了骨干网络的体素化和跨步的量化误差。地上高度 hg 有助于在 3D 中定位对象并添加地图视图投影删除的缺失高程信息。方向预测使用偏航角的正弦和余弦作为连续回归目标。结合框大小，这些回归头提供了 3D 边界框的完整状态信息。每个输出都使用自己的头部。我们在地面实况中心位置使用 L1 损失来训练所有输出。我们回归到对数大小以更好地处理各种形状的box。在推理时，我们通过在每个对象的峰值位置索引到密集回归头输出来提取所有属性。</p>
<p>**速度头和跟踪：**为了通过时间跟踪对象，我们学习为每个检测到的对象预测二维速度估计 v ∈ R 2 作为附加回归输出。速度估计需要时间点云序列 [6]。在我们的实现中，我们将先前帧中的点转换并合并到当前参考帧中，并预测当前帧和过去帧之间的对象位置差异，这些差异由时间差（速度）归一化。与其他回归目标一样，速度估计也在当前时间步长的地面实况对象位置使用 L1 损失进行监督。</p>
<p>在推理时，我们使用此偏移量以贪婪的方式将当前检测与过去的检测相关联。具体来说，我们通过应用负速度估计将当前帧中的对象中心投影回前一帧，然后通过最近距离匹配将它们与跟踪对象匹配。按照 SORT [4]，我们在删除它们之前将不匹配的轨道保留最多 T = 3 帧。我们用最后已知的速度估计更新每个不匹配的轨道。详细的跟踪算法图见补充。</p>
<p>CenterPoint 将所有热图和回归损失结合到一个共同的目标中，并联合优化它们。它简化并改进了之前基于锚点的 3D 检测器（参见实验）。然而，目前所有对象属性都是从对象的中心特征推断出来的，它可能不包含足够的信息来进行准确的对象定位。例如，在自动驾驶中，传感器往往只能看到物体的侧面，而看不到它的中心。接下来，我们通过使用带有轻量级点特征提取器的第二个细化阶段来改进 CenterPoint。</p>
<h3 id="41两阶段中心点">4.1.两阶段中心点</h3>
<p>我们使用不变的中心点作为第一阶段。第二阶段从主干的输出中提取额外的点特征。我们从预测边界框的每个面的 3D 中心提取一个点特征。请注意，边界框中心、顶面和底面中心都投影到地图视图中的同一点。因此，我们只考虑四个朝外的盒子面以及预测的对象中心。对于每个点，我们使用双线性插值从骨干地图视图输出 M 中提取特征。接下来，我们连接提取的点特征并将它们传递给 MLP。第二阶段在一阶段 CenterPoint 的预测结果之上预测类别不可知的置信度分数和框细化。</p>
<p>对于类别不可知的置信度分数预测，我们遵循 [25, 29, 42, 44] 并使用由框的 3D IoU 和相应的地面实况边界框引导的分数目标 ：
$$
I=\min \left(1, \max \left(0,2 \times I o U_t-0.5\right)\right)
$$
其中 $I o U_t$是第 t 个建议框与真实值之间的 IoU。我们使用二元交叉熵损失进行训练：
$$
L_{\text {score }}=-I_t \log \left(\hat{I}_t\right)-\left(1-I_t\right) \log \left(1-\hat{I}_t\right)
$$
其中$\hat{I}_t$是预测的置信度分数。在推理过程中，我们直接使用单阶段的中心点类别预测并计算最终的置信分数$\hat{Q}_t=\sqrt{\hat{Y}_t * \hat{I}_t}$为两个分数的几何平均值，其中Qt为对象t的最终预测置信度，$\hat{Y}_t=\max _{0 \leq k \leq K} \hat{Y}_{p, k}$ 和 $\hat{I}_t$分别为对象t的第一阶段和第二阶段置信度。</p>
<p>对于盒回归，模型在第一阶段建议的基础上预测一个改进，我们用L1损失来训练模型。我们的两级CenterPoint简化并加速了之前使用昂贵的基于pointnet的特征提取器和RoIAlign操作的两级3D探测器[42,43]。</p>
<h3 id="42-architecture">4.2. Architecture</h3>
<p>所有第一阶段输出共享第一个3 × 3卷积层、批处理归一化[24]和ReLU。然后，每个输出使用由批处理范数和ReLU分隔的两个3×3卷积的自己的分支。我们的第二阶段使用了一个共享的两层MLP，其中包括批规范、ReLU和Dropout[21]，下降率为0.3，然后使用单独的三层MLP进行置信度预测和箱回归。</p>
<h2 id="5experiments">5.Experiments</h2>
<p>**Waymo开放数据集：**Waymo开放数据集[46]包含798个车辆和行人的训练序列和202个验证序列。点云包含64道激光雷达，每0.1s对应180k个点。官方的三维检测评价指标包括三维边界盒平均精度(mAP)和mAP加权航向精度(mAPH)。mAP和mAPH是基于IoU的阈值，车辆为0.7，行人为0.5。对于3D跟踪，官方指标是多目标跟踪精度(MOTA)和多目标跟踪精度(MOTP)[3]。官方评估工具包还提供了两个难度级别的性能细分:1级用于具有超过五个激光雷达点的盒子，2级用于具有至少一个激光雷达点的盒子。</p>
<p>我们的Waymo模型对X轴和Y轴的检测范围为[- 75.2,75.2]，对Z轴的检测范围为[- 2m, 4m]。CenterPoint-Voxel使用(0.1m, 0.1m, 0.15m)体素大小遵循PV-RCNN[42]，而CenterPoint-Pillar使用(0.32m, 0.32m)网格大小。</p>
<p>**nuScenes数据集：**nuScenes[6]包含1000个驱动序列，分别有700、150、150个序列用于训练、验证和测试。每个序列大约20秒长，激光雷达频率为20fps。该数据集为每个激光雷达帧提供校准的车辆姿态信息，但每10帧(0.5s)才提供框注释。nuScenes使用32车道激光雷达，每帧产生大约30k个点。总共有28k、6k、6k个带注释的框架分别用于训练、验证和测试。注释包括10个具有长尾分布的类。官方的评估指标是班级的平均水平。对于3D检测，主要指标是平均平均精度(mAP)[13]和nuScenes检测分数(NDS)。mAP采用鸟瞰图中心距离&lt; 0.5m, 1m, 2m, 4m，而不是标准的盒重叠。NDS是mAP和其他属性度量的加权平均值，包括平移、比例、方向、速度和其他方框属性[6]。在我们提交测试集之后，nuScenes团队添加了一个新的神经规划度量(PKL)[35]。PKL度量基于规划者路线(使用3D检测)和地面真实轨迹的KL散度来衡量3D目标检测对下行自动驾驶任务的影响。因此，我们还报告了在测试集上评估的所有方法的PKL度量。</p>
<p>对于3D跟踪，nuScenes使用AMOTA[51]，它惩罚ID开关，假阳性和假阴性在各种召回阈值上的平均值。</p>
<p>对于nuScenes的实验，我们将X轴和Y轴的检测距离设置为[- 51.2m, 51.2m]， Z轴的检测距离设置为[- 5m, 3m]。CenterPoint-Voxel使用(0.1m, 0.1m, 0.2m)体素大小，CenterPoint-Pillars使用(0.2m, 0.2m)网格。</p>
<p>**训练和推理：**我们使用与先前工作相同的网络设计和训练时间表[42,65]。详细的超参数请参见附录。在两阶段CenterPoint的训练过程中，我们从第一阶段预测中随机抽取128个正负比为1:1的盒子[41]。如果提案与至少0.55 IoU的基础真值注释重叠，则提案是积极的[42]。在推理过程中，我们对非最大值抑制(NMS)之后的前500个预测运行第二阶段。推理时间是在Intel酷睿i7 CPU和Titan RTX GPU上测量的。</p>
<h3 id="51-主要结果">5.1. 主要结果</h3>
<p>**3D检测：**我们首先在Waymo和nuScenes的测试集上展示我们的3D检测结果。这两个结果都使用单个CenterPoint-Voxel模型。表1和表2总结了我们的结果。在Waymo的测试集上，我们的模型在车辆检测方面达到了71.8的2级mAPH，在行人检测方面达到了66.4的2级mAPH，比以前的方法高出了7.1%的车辆mAPH和10.6%的行人mAPH。在nuScenes上(表2)，我们的模型在多尺度输入和多模型集成的情况下比去年的挑战赛冠军CBGS[65]高出5.2%的mAP和2.2%的NDS。我们的模型也快得多，如后面所示。补充材料中有分类说明。我们的模型在所有类别中都显示出一致的性能改进，并且在小类别(交通锥的+5.6 mAP)和极端纵横比类别(自行车的+6.4 mAP和施工车辆的+7.0 mAP)中显示出更显著的收益。。更重要的是，我们的模型在神经平面指标(PKL)下的表现明显优于其他所有提交的作品，PKL是组织者在我们提交排行榜后评估的隐藏指标。这突出了我们框架的泛化能力。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="center_t_1.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表1.Waymo测试集上最先进的3D检测比较。我们展示了第1级和第2级基准测试的mAP和mAPH。</div>
</center>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="center_t_2.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表2.nuScenes测试集上3D检测的最新比较。我们展示了nuScenes检测分数(NDS)和平均平均精度(mAP)。</div>
</center>
<p>**3D跟踪：**表3显示了CenterPoint在Waymo测试集上的跟踪性能。我们在第4节中描述的基于速度的最接近距离匹配明显优于Waymo论文[46]中的官方跟踪基线，后者使用基于卡尔曼滤波的跟踪器[51]。我们观察到车辆和行人跟踪的MOTA分别提高了19.4和18.9。在nuScenes(表4)上，我们的框架优于上次挑战的获胜者Chiu etal。[10]由8.8 AMOTA。值得注意的是，我们的跟踪不需要单独的运动模型，并且在可以忽略不计的时间内运行，比检测时间多1ms。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="center_t_3.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表3.Waymo测试集上最先进的3D跟踪比较。我们展示了MOTA和MOTP。↑越高越好，↓越低越好。</div>
</center>
 <center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="center_t_4.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表4.在nuScenes测试集上最先进的3D跟踪比较。我们展示了AMOTA、假阳性(FP)、假阴性(FN)、id开关(IDS)的数量和每个类别的AMOTA。↑越高越好，↓越低越好。</div>
</center>
<p>5.2. 消融实验</p>
<p>**基于中心vs基于锚点：**我们首先将基于中心的一级检测器与基于锚点的一级检测器进行比较[27,54,65]。在Waymo上，我们遵循最先进的PV-RCNN[42]来设置锚超参数:我们在每个位置使用两个锚，分别为0°和90°，车辆的正/负IoU阈值为0.55/0.4，行人的正/负IoU阈值为0.5/0.35。在nuScenes上，我们遵循上次挑战获胜者CBGS的主播分配策略[65]。我们也与VoteNet[36]、PointRCNN[43]和PIXOR[55]中使用的基于网格点的表示进行了比较，后者将地面真值框内的所有点都赋值为正。对于这个实验，我们保持所有其他参数与我们的CenterPoint模型相同。</p>
<p>如表5所示，在Waymo数据集上，简单地从锚点切换到我们的中心，VoxelNet和PointPillars编码器的mAPH分别提高了4.3和4.5。在nuScenes上(表6)，CenterPoint在不同骨干网上提高了基于锚点的对应3.8-4.1 mAP和1.1-1.8 NDS。与基于网格点的表示(3.2-3.3 mAP和1.4-2.0 NDS改进)相比，结果相似。为了理解这种改进的来源，我们进一步展示了基于Waymo验证集上的对象大小和方向角度在不同子集上的性能分解。</p>
 <center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="center_t_5.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表5.基于锚点和基于中心的Waymo验证3D检测方法比较。我们显示了每个班级和平均LEVEL 2 mAPH。</div>
</center>
 <center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="center_t_6.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表6.基于锚点、基于网格点和基于中心的nuScenes验证3D检测方法比较我们给出了平均精度(mAP)和nuScenes检测分数(NDS)。</div>
</center>
<p>我们首先根据它们的航向角度将ground truth实例分为三个bin: 0°到15°、15°到30°和30°到45°。这种划分测试了探测器检测严重旋转箱子的性能，这对自动驾驶的安全部署至关重要。我们还将数据集分为三个部分:小、中、大，每个部分包含13个整体的地面真值框。</p>
<p>表7和表8总结了结果。当盒子旋转或偏离平均盒子尺寸时，我们的基于中心的检测器比基于锚点的基线表现得更好，这证明了模型在检测物体时捕捉旋转和尺寸不变性的能力。这些结果令人信服地突出了使用基于点的3D对象表示的优势。</p>
 <center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="center_t_7.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表7.基于锚点和基于中心的不同航向角目标检测方法的比较。第2行和第3行列出了旋转角度的范围及其对应的对象部分。我们在Waymo验证中展示了两种方法的LEVEL 2 mAPH。</div>
</center>
 <center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="center_t_8.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表8.对象大小对基于锚点和基于中心方法性能的影响。我们展示了不同大小范围对象的每个类LEVEL 2 mAPH:小33%，中33%和大33%。</div>
</center>
**一阶段vs两阶段：**在表9中，我们展示了在Waymo验证中使用二维CNN特征的单阶段和两阶段CenterPoint模型的比较。具有多个中心功能的两阶段细化为两个3D编码器提供了小开销(6ms-7ms)的大精度提升。我们还与RoIAlign进行了比较，后者在RoI中密集采样6 × 6个点[42,44]，我们基于中心的特征聚合取得了相当的性能，但更快更简单。体素量化限制了两阶段CenterPoint对PointPillars行人检测的改进，因为行人通常只存在于模型输入的1个像素中。在我们的实验中，两阶段的细化并没有带来nuScenes上单阶段中心点模型的改进。这在一定程度上是由于nuScenes中的点云更稀疏。nuScenes使用32车道激光雷达，每帧产生约30k个激光雷达点，约为Waymo数据集中点数的16个。这限制了两阶段细化的可用信息和潜在改进。在PointRCNN[43]和PV-RCNN[42]等之前的两阶段方法中也观察到了类似的结果。
 <center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="center_t_9.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表9.在Waymo验证中，比较VoxelNet和PointPillars编码器的3D LEVEL 2 mAPH使用单阶段、两阶段具有3D中心特征，以及两阶段具有3D中心和表面中心特征。</div>
</center>
<p>**不同特征组件的影响：**在我们的两阶段CenterPoint模型中，我们只使用来自2D CNN特征图的特征。然而，以前的方法也提出利用体素特征进行第二阶段的细化[42,44]。在这里，我们比较了两种体素特征提取基线:</p>
<p>**Voxel-Set提取：**PV-RCNN[42]提出了体素集抽象(voxel - set Abstraction, VSA)模块，该模块扩展了pointnet++[39]的集合抽象层，在一个固定半径的球中聚合体素特征。</p>
<p>**径向基函数(RBF)插值：**PointNet++[39]和SA-SSD[19]使用径向基函数从最近的三个非空三维特征体中聚合网格点特征。</p>
<p>对于这两个基线，我们使用它们的官方实现将鸟瞰特征与体素特征结合起来。表10总结了结果。它表明鸟瞰特征足以获得良好的性能，并且与文献中使用的体素特征相比效率更高[19,39,42]。</p>
<p>为了与之前没有对Waymo测试进行评估的工作进行比较，我们还在表11中报告了Waymo验证分割的结果。我们的模型在很大程度上优于所有已发布的方法，特别是对于具有挑战性的2级数据集的行人类别(+18.6 mAPH)，其中框只包含一个激光雷达点。</p>
<p>**3D跟踪：**表12为nuScenes验证上3D跟踪的消融实验。我们与去年的挑战赛冠军Chiu等人[10]进行了比较，他们使用基于马氏距离的卡尔曼滤波来关联CBGS的检测结果[65]。我们将评估分解为检测器和跟踪器，以进行严格的比较。给定相同的检测对象，使用我们简单的基于速度的最近点距离匹配比基于卡尔曼滤波的Mahalanobis距离匹配[10]高出3.7 AMOTA(行1 vs.行3，行2 vs.行4)。改进有两个来源:1)我们用学习到的点速度来建模物体运动，而不是用卡尔曼滤波器来建模三维边界框;2)我们通过中心点距离来匹配物体，而不是盒态的马氏距离或三维边界盒IoU。更重要的是，我们的跟踪是一个简单的最近邻匹配，没有任何隐藏状态计算。这节省了3D卡尔曼滤波器的计算开销(73ms vs. 1ms)。</p>
<p>**结论：**提出了一种基于中心的激光雷达点云三维目标同步检测与跟踪框架。我们的方法使用一个标准的3D点云编码器，在头部有几个卷积层，以产生鸟瞰热图和其他密集的回归输出。检测是一个简单的局部峰值提取与细化，跟踪是一个最接近的距离匹配。CenterPoint简单，接近实时，在Waymo和nuScenes基准测试中实现了最先进的性能。</p>
<h1 id="voxelnet-end-to-end-learning-for-point-cloud-based-3d-object-detection">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</h1>
<p>VoxelNet：基于点云的 3D 对象检测的端到端学习</p>
<p>准确检测 3D 点云中的对象是许多应用程序的核心问题，例如自主导航、家政机器人和增强/虚拟现实。为了将高度稀疏的 LiDAR 点云与区域建议网络 (RPN) 连接起来，大多数现有工作都集中在手工制作的特征表示上，例如，鸟瞰图投影。在这项工作中，我们消除了对 3D 点云进行手动特征工程的需要，并提出了 VoxelNet，这是一种通用的 3D 检测网络，它将特征提取和边界框预测统一到一个阶段、端到端的可训练深度网络中。==具体来说，VoxelNet 将点云划分为等间距的 3D 体素，并通过新引入的体素特征编码 (VFE) 层将每个体素内的一组点转换为统一的特征表示。通过这种方式，点云被编码为描述性的体积表示，然后连接到 RPN 以生成检测。==在 KITTI 汽车检测基准上的实验表明，VoxelNet 的性能大大优于最先进的基于 LiDAR 的 3D 检测方法。此外，我们的网络学习了具有各种几何形状的物体的有效判别表示，从而在仅基于 LiDAR 的行人和骑自行车者的 3D 检测中取得了令人鼓舞的结果。</p>
<h2 id="1-introduction-1">1. Introduction</h2>
<p>基于点云的 3D 对象检测是各种现实世界应用的重要组成部分，例如自主导航 [11、14]、家政机器人 [28] 和增强/虚拟现实 [29]。与基于图像的检测相比，LiDAR 提供可靠的深度信息，可用于准确定位对象并表征其形状 [21、5]。然而，与图像不同的是，由于 3D 空间的非均匀采样、传感器的有效范围、遮挡和相对位姿等因素，LiDAR 点云是稀疏的并且具有高度可变的点密度。为了应对这些挑战，许多方法手动为点云制作特征表示，并对其进行3D对象检测。几种方法将点云投影到透视图中并应用基于图像的特征提取技术 [30、15、22]。其他方法将点云光栅化为 3D 体素网格，并使用手工制作的特征对每个体素进行编码 [43、9、39、40、21、5]。然而，这些手动设计选择引入了信息瓶颈，阻碍了这些方法有效地利用 3D 形状信息和检测任务所需的不变性。图像识别 [20] 和检测 [13] 任务的重大突破是由于从手工制作的特征转向机器学习的特征。</p>
<p>最近，Qi 等人[31] 提出了 PointNet，这是一种端到端的深度神经网络，可以直接从点云中学习逐点特征。这种方法在 3D 对象识别、3D 对象部分分割和逐点语义分割任务上展示了令人印象深刻的结果。在 [32] 中，引入了 PointNet 的改进版本，使网络能够学习不同尺度的局部结构。为了取得令人满意的结果，这两种方法在所有输入点（~1k 点）上训练特征变换器网络。由于使用激光雷达获得的典型点云包含约100k个点，因此像[31,32]中那样训练架构会导致高计算和内存需求。将 3D 特征学习网络扩展到更多点和 3D 检测任务是我们在本文中解决的主要挑战。</p>
<p>区域提议网络 (RPN) [34] 是一种高度优化的高效目标检测算法 [17、5、33、24]。然而，这种方法要求数据密集并以张量结构（例如图像、视频）组织，而典型的 LiDAR 点云并非如此。在本文中，我们缩小了点集特征学习和 RPN 之间在 3D 检测任务中的差距。</p>
<p>我们展示了 VoxelNet，这是一种通用的 3D 检测框架，它以端到端的方式同时从点云中学习判别特征表示并预测准确的 3D 边界框，如图 2 所示。我们设计了一种新颖的体素特征编码（VFE ）层，通过将逐点特征与局部聚合特征相结合，实现体素内的点间交互。堆叠多个 VFE 层允许学习复杂的特征以表征局部 3D 形状信息。具体来说，VoxelNet 将点云划分为等间距的 3D 体素，通过堆叠的 VFE 层对每个体素进行编码，然后 3D 卷积进一步聚合局部体素特征，将点云转换为高维体积表示。最后，RPN 使用体积表示并产生检测结果。这种高效的算法受益于稀疏点结构和体素网格上的高效并行处理。</p>
<p>我们评估了VoxelNet在鸟瞰图检测上的性能以及由KITTI提供的完整3D探测任务基准[11]。实验结果表明，VoxelNet在很大程度上优于最先进的基于激光雷达的3D检测方法。我们还证明了VoxelNet在从LiDAR点云检测行人和骑自行车的人方面取得了非常令人鼓舞的结果。</p>
<h3 id="11-related-work">1.1 Related Work</h3>
<p>3D 传感器技术的快速发展促使研究人员开发有效的表示来检测和定位点云中的对象。一些较早的特征表示方法是 [41, 8, 7, 19, 42, 35, 6, 27, 1, 36, 2, 25, 26]。当丰富和详细的 3D 形状信息可用时，这些手工制作的特征会产生令人满意的结果。然而，它们无法适应更复杂的形状和场景，也无法从数据中学习所需的不变性，导致在自主导航等不受控制的场景中取得的成功有限。</p>
<p>鉴于图像提供了详细的纹理信息，许多算法从 2D 图像推断出 3D 边界框 [4、3、44、45、46、38]。然而，基于图像的 3D 检测方法的准确性受深度估计准确性的限制。</p>
<p>几种基于激光雷达的 3D 对象检测技术利用体素网格表示。 [43, 9] 使用从体素中包含的所有点派生的 6 个统计量对每个非空体素进行编码。 [39]融合了多个局部统计数据来表示每个体素。 [40] 计算体素网格上的截断符号距离。[21] 对 3D 体素网格使用二进制编码。 [5] 通过计算鸟瞰视图中的多通道特征图和正面视图中的圆柱坐标，引入了 LiDAR 点云的多视图表示。其他几项研究将点云投影到透视图上，然后使用基于图像的特征编码方案 [30、15、22]。</p>
<h3 id="12-contributions">1.2. Contributions</h3>
<ul>
<li>
<p>我们提出了一种用于基于点云的3D 检测的新型端到端可训练深度架构VoxelNet，它直接在稀疏3D 点上运行并避免手动特征工程引入的信息瓶颈。</p>
</li>
<li>
<p>我们提出了一种实现VoxelNet 的有效方法，它受益于稀疏点结构和体素网格上的高效并行处理。</p>
</li>
<li>
<p>我们在 KITTI 基准测试中进行实验，表明 VoxelNet 在基于 LiDAR 的汽车、行人和骑车人检测基准测试中产生了最先进的结果。</p>
</li>
</ul>
<h2 id="2-voxelnet">2. VoxelNet</h2>
<p>在本节中，我们将解释 VoxelNet 的架构、用于训练的损失函数以及实现网络的高效算法。</p>
<h3 id="21-voxelnet架构">2.1 VoxelNet架构</h3>
<p>所提出的 VoxelNet 由三个功能块组成：(1) 特征学习网络，(2) 卷积中间层，以及 (3) 区域提议网络 [34]，如图 2 所示。我们在下面详细介绍了 VoxelNet部分。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="VoxelNet2.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图2.体素网络架构。特征学习网络将原始点云作为输入，将空间划分为体素，并将每个体素内的点转换为表征形状信息的向量表示。该空间表示为稀疏的 4D 张量。卷积中间层处理 4D 张量以聚合空间上下文。最后，RPN 生成 3D 检测。</div>
</center>
<h4 id="211-特征学习网络">2.1.1 特征学习网络</h4>
<p>**体素分区：**给定一个点云，我们将 3D 空间细分为等间距的体素，如图 2 所示。假设点云包含范围分别为 D、H、W 沿 Z、Y、X 轴的 3D 空间。我们相应地定义大小为 vD、vH 和 vW 的每个体素。生成的 3D 体素网格的大小为 D′ = D/vD，H′ = H/vH，W′ = W/vW。这里，为简单起见，我们假设 D、H、W 是 vD、vH、vW 的倍数。</p>
<p>**分组：**我们根据点所在的体素对点进行分组。由于距离、遮挡、物体相对位姿、非均匀采样等因素。雷达点云是稀疏的，并且在整个空间中具有高度可变的点密度。因此，分组后，体素将包含可变数量的点。如图 2 所示，其中 Voxel-1 的点明显多于 Voxel-2 和 Voxel-4，而 Voxel-3 不包含任何点。</p>
<p>**随机采样：**通常高清 LiDAR 点云由约 100k 个点组成。直接处理所有点不仅会增加计算平台的内存/效率负担，而且整个空间中高度可变的点密度可能会使检测产生偏差。为此，我们从包含超过 T 个点的体素中随机抽取固定数量 T 个点。这种采样策略有两个目的，(1) 节省计算量（详见第 2.3 节）； (2) 减少体素之间点的不平衡，从而减少采样偏差，并为训练增加更多变化。</p>
<p>**堆叠体素特征编码：**关键创新是 VFE 层链。为简单起见，图 2 说明了一个体素的分层特征编码过程。不失一般性，我们使用 VFE Layer-1 在以下段落中描述细节。图 3 显示了 VFE 第 1 层的架构。</p>
<center>
    <img style="width:100% border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="VoxelNet3.bmp">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图3.体素特征编码层。</div>
</center>
<p>将 V = {pi = [xi , yi , zi , ri ] T ∈ R 4}i=1&hellip;t 表示为包含 t ≤ T LiDAR 点的非空体素，其中 pi 包含第 i 个的 XYZ 坐标点，ri 是接收到的反射率。我们首先计算局部均值作为 V 中所有点的质心，表示为 (vx, vy, vz)。然后我们用相对偏移 w.r.t 增加每个点 pi。质心并获得输入特征集 Vin = {p^i = [xi , yi , zi , ri , xi −vx, yi −vy, zi −vz] T ∈ R 7}i=1&hellip;t。接下来，每个 p^i 通过全连接网络 (FCN) 转换为特征空间，我们可以在其中聚合来自点特征 fi ∈ R m 的信息，以编码体素内包含的表面形状。 FCN 由线性层、批量归一化 (BN) 层和修正线性单元 (ReLU) 层组成。在获得逐点特征表示后，我们在与 V 相关联的所有 fi 上使用逐元素最大池化来获得 V 的局部聚合特征 ~f ∈ R m。最后，我们增加每个 fi 与 ∼f 形成逐点连接的特征 f out i = [f T i , ∼f T ] T ∈ R 2m。这样我们就得到了输出特征集Vout = {f out i }i&hellip;t。所有非空体素都以相同的方式编码，并且它们在 FCN 中共享相同的参数集。</p>
<p>我们使用 VFE-i(cin, cout) 来表示将维度 cin 的输入特征转换为维度 cout 的输出特征的第 i 个 VFE 层。线性层学习大小为 cin ×(cout/2) 的矩阵，逐点级联得到维度 cout 的输出</p>
<p>由于输出特征结合了逐点特征和局部聚合特征，堆叠 VFE 层对体素内的点交互进行编码，并使最终特征表示能够学习描述性形状信息。 voxel-wise 特征是通过 FCN 将 VFE-n 的输出转换为 R C 并应用 element-wise Maxpool 获得的，其中 C 是 voxel-wise 特征的维度，如图 2 所示。</p>
<p>**稀疏张量表示：**通过仅处理非空体素，我们获得了体素特征列表，每个特征都唯一地关联到特定非空体素的空间坐标。获得的体素特征列表可以表示为一个稀疏的 4D 张量，大小为 C × D' × H' × W'，如图 2 所示。虽然点云包含约 100k 个点，但超过 90% 的体素通常是空的。将非空体素特征表示为稀疏张量大大减少了反向传播过程中的内存使用和计算成本，这是我们高效实施的关键步骤。</p>
<h4 id="212-卷积中间层">2.1.2 卷积中间层</h4>
<p>我们用ConvMD(cin, cout, k, s, p)表示一个M维卷积算子，其中cin和cout是输入输出通道数，k, s, p是核大小对应的M维向量，分别是步幅大小和填充大小。当 M 维度的大小相同时，我们使用标量来表示大小，例如k 为 k = (k, k, k)。</p>
<p>每个卷积中间层应用 3D 卷积，BN层，ReLU层依次。卷积中间层在逐渐扩大的感受野内聚合体素特征，为形状描述添加更多上下文。卷积中间层中过滤器的详细尺寸在第 3 节中解释。</p>
<p>2.1.3 区域建议网络</p>
<p>最近，区域建议网络 [34] 已成为性能最佳的对象检测框架 [40、5、23] 的重要组成部分。在这项工作中，我们对 [34] 中提出的 RPN 架构进行了几个关键修改，并将其与特征学习网络和卷积中间层相结合，形成端到端的可训练管道。</p>
<p>我们的 RPN 的输入是由卷积中间层提供的特征图。该网络的架构如图 4 所示。该网络具有三个全卷积层块。每个块的第一层通过步长为 2 的卷积将特征图下采样一半，然后是一系列步长为 1 的卷积（×q 表示滤波器的 q 次应用）。在每个卷积层之后，应用 BN 和 ReLU 操作。然后我们将每个块的输出上采样到固定大小并连接以构建高分辨率特征图。最后，这个特征图被映射到所需的学习目标：(1) 概率得分图和 (2) 回归图。</p>
<h3 id="22损失函数">2.2.损失函数</h3>
<p>设 {a pos i }i=1&hellip;Npos 是 Npos 正锚的集合，{a neg j }j=1&hellip;Nneg 是 Nneg 负锚的集合。我们将一个 3D ground truth box 参数化为 (x g c , yg c , zg c , lg , wg , hg , θg )，其中 x g c , yg c , zg c 表示中心位置，l g , wg , hg 分别是长、宽、高盒子的角度，θ g 是绕 Z 轴的偏航角。为了从参数化为 (x a c , ya c , za c , la , wa , ha , θa ) 的匹配正锚中检索地面真值框，我们定义了包含对应中心位置Δx, Δy, Δz、尺寸mensions和旋转角∆θ， 7 个回归目标的残差向量 u ∗ ∈ R 7 ，它们被定义为：
$$
\begin{aligned}
&amp; \Delta x=\frac{x_c^g-x_c^a}{d^a}, \Delta y=\frac{y_c^g-y_c^a}{d^a}, \Delta z=\frac{z_c^g-z_c^a}{h^a} \<br>
&amp; \Delta l=\log \left(\frac{l^g}{l^a}\right), \Delta w=\log \left(\frac{w^g}{w^a}\right), \Delta h=\log \left(\frac{h^g}{h^a}\right), \<br>
&amp; \Delta \theta=\theta^g-\theta^a
\end{aligned}
$$
其中$d^a=\sqrt{\left(l^a\right)^2+\left(w^a\right)^2}$是锚框底部的对角线。在这里，我们的目标是直接估计定向 3D 框并使用对角线 d a 均匀地归一化 Δx 和 Δy，这不同于 [34, 40, 22, 21, 4, 3, 5]。我们定义损失函数如下：
$$
\begin{aligned}
L &amp; =\alpha \frac{1}{N_{\mathrm{pos}}} \sum_i L_{\mathrm{cls}}\left(p_i^{\mathrm{pos}}, 1\right)+\beta \frac{1}{N_{\mathrm{neg}}} \sum_j L_{\mathrm{cls}}\left(p_j^{\mathrm{neg}}, 0\right) \<br>
&amp; +\frac{1}{N_{\mathrm{pos}}} \sum_i L_{\mathrm{reg}}\left(\mathbf{u}_i, \mathbf{u}_i^*\right)
\end{aligned}
$$
其中 p pos i 和 p neg j 分别表示正锚点 a pos i 和负锚点 a neg j 的 softmax 输出，而 ui ∈ R 7 和 u ∗ i ∈ R 7 是正锚点 a pos 的回归输出和ground truth我 。前两项是 {a pos i }i=1&hellip;Npos 和 {a neg j }j=1&hellip;Nneg 的归一化分类损失，其中 Lcls 代表二元交叉熵损失，α、β 是平衡相对重要性的正常数。最后一项 Lreg 是回归损失，我们在这里使用 SmoothL1 函数 [12, 34]。</p>
<h3 id="23高效实施">2.3.高效实施</h3>
<p>GPU 针对处理密集张量结构进行了优化。直接使用点云的问题是点在空间中分布稀疏，每个体素都有可变数量的点。我们设计了一种将点云转换为密集张量结构的方法，其中可以跨点和体素并行处理堆叠的 VFE 操作。</p>
<p>该方法总结在图 5 中。我们初始化一个 K × T × 7 维张量结构来存储体素输入特征缓冲区，其中 K 是非空体素的最大数量，T 是每个体素的最大点数，7是每个点的输入编码维度。这些点在处理之前是随机的。对于点云中的每个点，我们检查相应的体素是否已经存在。此查找操作使用哈希表在 O(1) 中高效完成，其中体素坐标用作哈希键。如果体素已经初始化，如果点少于 T，我们将点插入到体素位置，否则忽略该点。如果体素未初始化，我们初始化一个新的体素，将其坐标存储在体素坐标缓冲区中，并将点插入到该体素位置。体素输入特征和坐标缓冲区可以通过单次遍历点列表来构建，因此其复杂度为 O(n)。为了进一步提高内存/计算效率，可以仅存储有限数量的体素 (K) 并忽略来自具有少量点的体素的点。</p>
<p>构建体素输入缓冲区后，堆叠式 VFE 仅涉及点级和体素级密集操作，可以在 GPU 上并行计算。请注意，在 VFE 中进行串联操作后，我们将对应于空点的特征重置为零，这样它们就不会影响计算出的体素特征。最后，使用存储的坐标缓冲区，我们将计算出的稀疏体素结构重组为密集体素网格。以下卷积中间层和 RPN 操作在密集的体素网格上工作，可以在 GPU 上有效地实现。</p>
<h2 id="3-training-details">3. Training Details</h2>
<p>在本节中，我们将解释 VoxelNet 的实现细节和训练过程。</p>
<h3 id="31网络详情">3.1.网络详情</h3>
<p>我们的实验设置基于 KITTI 数据集 [11] 的 LiDAR 规范。</p>
<p><strong>汽车检测:</strong> 对于此任务，我们分别考虑沿 Z、Y、X 轴 [−3, 1] × [−40, 40] × [0, 70.4] 米范围内的点云。投影到图像边界之外的点被删除 [5]。我们选择 vD = 0.4，vH = 0.2，vW = 0.2 米的体素大小，这导致 D' = 10，H' = 400，W' = 352。我们将 T = 35 设置为随机采样点的最大数量在每个非空体素中。我们使用两个 VFE 层 VFE-1(7, 32) 和 VFE-2(32, 128)。最终的 FCN 将 VFE-2 输出映射到 $\mathbb{R}^{128}$。因此，我们的特征学习网络生成了一个形状为 128 × 10 × 400 × 352 的稀疏张量。为了聚合体素特征，我们依次使用卷积中间层，分别为 Conv3DD(128, 64, 3,(2,1,1), (1,1,1))， Conv3D(64, 64, 3, (1,1,1), (0,1,1))和Conv3D(64, 64, 3, (2,1,1), (1,1,1))，产生大小为 64 × 2 × 400 × 352 的 4D 张量。整形后，RPN 的输入是一个大小为 128×400×352 的特征图，其中维度对应于 3D 张量的通道、高度和宽度。图 4 说明了此任务的详细网络架构。与 [5] 不同，我们仅使用一个锚尺寸，la = 3.9，wa = 1.6，ha = 1.56 米，以 z a c = −1.0 米为中心，有两个旋转，0 度和 90 度。我们的 anchor 匹配标准如下：如果一个 anchor 与 ground truth 具有最高的 Intersection over Union (IoU) 或者其与 ground truth 的 IoU 高于 0.6（在鸟瞰图中），则该锚被认为是正的。如果 anchor 与所有 ground truth boxes 之间的 IoU 小于 0.45，则该 anchor 被认为是负的。我们将锚点视为不关心它们是否具有 0.45 ≤ IoU ≤ 0.6 与任何基本事实。我们在等式中设置 α = 1.5 和 β = 1.2。</p>
<p>**行人和骑车人检测：**输入范围 1 分别为 [−3, 1] × [−20, 20] × [0, 48] 米，沿 Z、Y、X 轴。我们使用与汽车检测相同的体素大小，得到 D = 10，H = 200，W = 240。我们设置 T = 45 以获得更多的 LiDAR 点以更好地捕获形状信息。特征学习网络和卷积中间层与汽车检测任务中使用的网络相同。对于 RPN，我们通过将第一个 2D 卷积中的步幅大小从 2 更改为 1 对图 4 中的块 1 进行了修改。这允许在锚点匹配中获得更精细的分辨率，这对于检测行人和骑自行车的人是必要的。我们使用锚尺寸 l a = 0.8, wa = 0.6, ha = 1.73 米，以 z a c = −0.6 米为中心，旋转 0 度和 90 度进行行人检测，并使用锚尺寸 l a = 1.76, wa = 0.6, ha = 1.73 米，以 z a c = −0.6 米为中心z a c = −0.6，旋转 0 度和 90 度以检测骑车人。具体的anchor匹配标准如下：如果anchor与ground truth的IoU最高，或者其与ground truth的IoU大于0.5，我们将其指定为positive。如果一个锚点与每个基本事实的 IoU 小于 0.35，则该锚点被认为是负的。对于具有 0.35 ≤ IoU ≤ 0.5 且具有任何基本事实的锚点，我们将它们视为无关紧要。</p>
<p>在训练期间，我们在前 150 个时期使用学习率为 0.01 的随机梯度下降 (SGD)，在最后 10 个时期将学习率降低至 0.001。我们使用 16 个点云的批量大小。</p>
<h3 id="32-数据增强">3.2. 数据增强</h3>
<p>如果训练点云少于 4000 个，从头开始训练我们的网络将不可避免地出现过度拟合。为了减少这个问题，我们引入了三种不同形式的数据增强。增强训练数据是即时生成的，无需存储在磁盘上 [20]。</p>
<p>定义集合 M = {pi = [xi , yi , zi , ri ] T ∈ R 4}i=1,&hellip;,N 为整个点云，由 N 个点组成。我们将一个 3D 边界框 bi 参数化为 (xc, yc, zc, l, w, h, θ)，其中 xc, yc, zc 是中心位置，l, w, h 是长、宽、高，θ 是绕 Z 轴的偏航旋转。我们定义 Ωi = {p|x ∈ [xc − l/2, xc + l/2], y ∈ [yc − w/2, yc + w/2], z ∈ [zc − h/2, zc + h/2], p ∈ M} 作为包含 bi 内所有 LiDAR 点的集合，其中 p = [x, y, z, r] 表示整个集合 M 中的特定 LiDAR 点。</p>
<p>第一种数据增强形式独立地将扰动应用于每个地面实况 3D 边界框以及框中的那些 LiDAR 点。具体来说，我们围绕 Z 轴旋转 bi 和相关的 Ωi 相对于 (xc, yc, zc) 通过均匀分布的随机变量 ∆θ ∈ [−π/10, +π/10]。然后我们将平移 (Δx, Δy, Δz) 添加到 bi 的 XYZ 分量和 Ωi 中的每个点，其中 Δx, Δy, Δz 独立于具有均值零和标准差的高斯分布绘制1.0。为了避免物理上不可能的结果，我们在扰动后对任意两个框进行碰撞测试，如果检测到碰撞则恢复到原始状态。由于扰动独立应用于每个地面真值框和相关的 LiDAR 点，因此网络能够从比原始训练数据更多的变化中学习。</p>
<p>其次，我们对所有地面真值框 bi 和整个点云 M 应用全局缩放。具体来说，我们将 XYZ 坐标和每个 bi 的三个维度以及 M 中所有点的 XYZ 坐标与从中提取的随机变量相乘均匀分布 [0.95, 1.05]。如基于图像的分类 [37、18] 和检测任务 [12、17] 所示，引入全局尺度增强提高了网络检测具有各种大小和距离的对象的鲁棒性。</p>
<p>最后，我们对所有地面真值框 bi 和整个点云 M 应用全局旋转。旋转沿 Z 轴和围绕 (0, 0, 0) 应用。全局旋转偏移由均匀分布 [−π/4, +π/4] 采样确定。通过旋转整个点云，我们模拟车辆转弯。</p>
<h2 id="4-experiments">4. Experiments</h2>
<p>我们在 KITTI 3D 对象检测基准 [11] 上评估 VoxelNet，其中包含 7,481 个训练图像/点云和 7,518 个测试图像/点云，涵盖三个类别：汽车、行人和骑自行车的人。对于每个类别，检测结果根据三个难度级别进行评估：简单、中等和困难，这三个级别根据对象大小、遮挡状态和截断级别确定。由于测试集的基本事实不可用并且对测试服务器的访问受到限制，我们使用 [4, 3, 5] 中描述的协议进行综合评估，并将训练数据细分为训练集和验证集，这将产生 3,712 个用于训练的数据样本和 3,769 个用于验证的数据样本。拆分避免了来自同一序列的样本同时包含在训练和验证集中 [3]。最后我们还展示了使用 KITTI 服务器的测试结果。</p>
<p>对于汽车类别，我们将所提出的方法与几种性能最佳的算法进行比较，包括基于图像的方法：Mono3D [3] 和 3DOP [4]；基于 LiDAR 的方法：VeloFCN [22] 和 3D-FCN [21]；和多模式方法 MV [5]。 Mono3D [3]、3DOP [4] 和 MV [5] 使用预训练模型进行初始化，而我们仅使用 KITTI 中提供的 LiDAR 数据从头开始训练 VoxelNet。</p>
<p>为了分析端到端学习的重要性，我们实施了一个强大的基线，该基线源自 VoxelNet 架构，但使用手工制作的特征而不是建议的特征学习网络。我们称这个模型为手工制作的基线（HC-baseline）。 HC-baseline 使用 [5] 中描述的鸟瞰图特征，这些特征是在 0.1m 分辨率下计算的。与 [5] 不同的是，我们将高度通道的数量从 4 个增加到 16 个，以捕获更详细的形状信息——进一步增加高度通道的数量并没有带来性能提升。我们用相似大小的二维卷积层替换 VoxelNet 的中间卷积层，分别是 Conv2D(16, 32, 3, 1, 1), Conv2D(32, 64, 3, 2, 1), Conv2D(64, 128, 3) , 1, 1).最后，RPN 在 VoxelNet 和 HC-baseline 中是相同的。 HC-baseline 和 VoxelNet 中的参数总数非常相似。我们使用第 3 节中描述的相同训练过程和数据增强来训练 HC 基线。</p>
<h3 id="41-kitti-验证集评估">4.1. KITTI 验证集评估</h3>
<p>**指标：**我们遵循官方的 KITTI 评估协议，其中汽车类的 IoU 阈值为 0.7，行人和自行车类的 IoU 阈值为 0.5。鸟瞰图和全 3D 评估的 IoU 阈值相同。我们使用平均精度 (AP) 指标比较这些方法。</p>
<p>**鸟瞰图评估：**评估结果如表 1 所示。VoxelNet 在所有三个难度级别上始终优于所有竞争方法。与最先进的 [5] 相比，HC-baseline 也取得了令人满意的性能，这表明我们的基区域建议网络 (RPN) 是有效的。对于鸟瞰图中的行人和骑车人检测任务，我们将提出的 VoxelNet 与 HC-baseline 进行了比较。对于这些更具挑战性的类别，VoxelNet 产生的 AP 远高于 HC 基线，这表明端到端学习对于基于点云的检测至关重要。</p>
<p>我们要注意的是，[21] 报告的简单、中等和困难级别分别为 88.9%、77.3% 和 72.7%，但这些结果是基于 6,000 个训练框架和～1,500 个验证框架的不同拆分获得的，并且它们不能直接与表 1 中的算法进行比较。因此，我们不将这些结果包含在表中。</p>
<p>**3D 评估：**与只需要在 2D 平面中精确定位物体的鸟瞰图检测相比，3D 检测是一项更具挑战性的任务，因为它需要在 3D 空间中更精细地定位形状。表 2 总结了比较。对于汽车类，VoxelNet 在所有难度级别上都明显优于 AP 中的所有其他方法。具体来说，仅使用 LiDAR，VoxelNet 明显优于最先进的方法 MV (BV+FV+RGB) [5]，基于LiDAR+RGB，在简单、中等和困难级别分别提高了 10.68%、2.78% 和 6.29%。 HC-baseline 达到了与 MV [5] 方法相似的精度。</p>
<p>与鸟瞰图评估一样，我们还将 VoxelNet 与 HC-baseline 在 3D 行人和骑车人检测方面进行了比较。由于 3D 姿势和形状的高度变化，成功检测这两个类别需要更好的 3D 形状表示。如表 2 所示，针对更具挑战性的 3D 检测任务强调了 VoxelNet 的改进性能（从鸟瞰图提高约 8% 到 3D 检测提高约 12%），这表明 VoxelNet 在捕获 3D 形状信息方面比手工制作的功能。</p>
<h3 id="42-kitti-测试集评估">4.2. KITTI 测试集评估</h3>
<p>我们通过将检测结果提交给官方服务器，在 KITTI 测试集上对 VoxelNet 进行了评估。结果总结在表 3 中。VoxelNet 在所有任务（鸟瞰图和 3D 检测）和所有困难方面都明显优于先前发布的最新技术 [5]。我们要注意的是，KITTI 基准测试中列出的许多其他主要方法同时使用 RGB 图像和 LiDAR 点云，而 VoxelNet 仅使用 LiDAR。</p>
<p>我们在图 6 中展示了几个 3D 检测示例。为了更好地可视化，将使用 LiDAR 检测到的 3D 框投影到 RGB 图像上。如图所示，VoxelNet 在所有类别中都提供了高度准确的 3D 边界框。</p>
<p>VoxelNet 的推理时间为 33 毫秒：体素输入特征计算需要 5 毫秒，特征学习网络需要 16 毫秒，卷积中间层需要 1 毫秒，区域建议网络在 TitanX GPU 和1.7GHz 中央处理器上需要 11 毫秒。</p>
<h2 id="5-conclusion">5. Conclusion</h2>
<p>基于 LiDAR 的 3D 检测中的大多数现有方法都依赖于手工制作的特征表示，例如，鸟瞰图投影。在本文中，我们消除了手动特征工程的瓶颈，并提出了 VoxelNet，这是一种用于基于点云的 3D 检测的新型端到端可训练深度架构。我们的方法可以直接在稀疏 3D 点上操作并有效地捕获 3D 形状信息。我们还展示了一种有效的 VoxelNet 实现，它受益于点云稀疏性和体素网格上的并行处理。我们在 KITTI 汽车检测任务上的实验表明，VoxelNet 大大优于最先进的基于 LiDAR 的 3D 检测方法。在更具挑战性的任务中，例如行人和骑自行车者的 3D 检测，VoxelNet 还展示了令人鼓舞的结果，表明它提供了更好的 3D 表示。未来的工作包括扩展 VoxelNet 以用于联合 LiDAR 和基于图像的端到端 3D 检测，以进一步提高检测和定位精度。</p>
<h1 id="voxelnext-fully-sparse-voxelnet-for-3d-object-detection-and-tracking">VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking</h1>
<p>VoxelNeXt:用于3D对象检测和跟踪的完全稀疏的VoxelNet</p>
<p>**摘要：**3D物体检测器通常依赖于手工制作的代理，例如锚点或中心，并将经过充分研究的2D框架转换为3D。因此，稀疏体素特征需要通过密集预测头进行密集化和处理，这不可避免地需要额外的计算成本。在本文中，我们提出了VoxelNext来进行完全稀疏的3D物体检测。我们的核心见解是直接基于稀疏体素特征来预测对象，而不依赖于手工制作的代理。我们强大的稀疏卷积网络VoxelNeXt完全通过体素特征检测和跟踪3D物体。它是一个优雅而高效的框架，不需要稀疏到密集的转换或NMS后处理。我们的方法在nuScenes数据集上实现了比其他主机检测器更好的速度-精度权衡。我们首次证明了一个完全稀疏的基于体素的表示可以很好地用于LIDAR 3D目标检测和跟踪。在nuScenes、Waymo和Argoverse2基准测试上进行的大量实验验证了我们方法的有效性。我们的模型在nuScenes跟踪测试基准上优于所有现有的LIDAR方法。代码和模型可在github.com/dvlab-research/VoxelNeXt上获得。</p>
<h2 id="1-introduction-2">1. Introduction</h2>
<p>3D感知是自动驾驶系统的基本组成部分。三维检测网络以稀疏点云或体素为输入，对三维物体进行定位和分类。最近的3D目标检测器[12,41,57]由于其效率，通常采用稀疏卷积网络(sparse cnn)[53]进行特征提取。受2D目标检测框架[14,39]的启发，锚点[12,53]或中心[57]，即CenterPoint[57]中的密集点锚点，通常用于预测。它们都是手工制作的，并作为3D对象的中间代理。</p>
<p>锚点和中心首先是为规则和网格结构的图像数据设计的，没有考虑3D数据的稀疏性和不规则性。为了使用这些代理表示，主流的检测器[12,41,57]将三维稀疏特征转换为二维密集特征，从而为有序锚点或中心构建密集检测头。尽管有用，但这种密集的封头传统会导致其他限制，包括效率低下和复杂的管道，如下所述。</p>
<p>在图1中，我们将CenterPoint中的热图可视化[57]。很明显，很大一部分空间的预测分数几乎为零。由于固有的稀疏性和背景点多，只有少数点有响应，即nuScenes验证集中Car类的平均响应不到1%。然而，密集预测头根据密集卷积计算的要求对特征映射中的所有位置进行计算。它们不仅浪费了大量的计算，而且由于冗余的预测而使检测管道复杂化。它需要使用非最大抑制(NMS)，如后处理来删除重复检测，从而使检测器不美观。这些限制促使我们寻求替代的稀疏检测解决方案。</p>
<p>在本文中，我们提出了VoxelNeXt。它是一个简单，高效，无后处理的3D物体检测器。我们设计的核心是一个体素到对象的方案，它通过一个强大的全稀疏卷积网络，直接从体素特征预测3D对象。关键的优点是我们的方法可以摆脱锚代理，稀疏到密集的转换，区域建议网络，和其他复杂组件。我们在图2中说明了主流3D探测器和我们的管道。</p>
<p>高推理效率是由于我们的体素到目标方案避免了密集的特征映射。它只对稀疏和必要的位置进行预测，如表1所示，与CenterPoint[57]进行比较。这种表示也使得VoxelNeXt很容易扩展到使用离线跟踪器的3D跟踪。先前的工作[57]只跟踪预测的对象中心，这可能涉及到对其位置的预测偏差。在VoxelNeXt中，查询体素，即用于框预测的体素，也可以被跟踪以进行关联。</p>
<p><u>最近，FSD[16]利用了全稀疏框架。在VoteNet[37]的激励下，它对对象中心进行投票，并采用迭代优化。由于3D稀疏数据通常分散在物体表面，因此这种投票过程不可避免地会引入偏见或错误。因此，需要细化，例如迭代组校正，以确保最终的准确性。该系统由于过于相信物体中心而变得复杂。FSD[16]在大范围Argoverse2上很有前景，但其效率不如我们，如图3所示。</u></p>
<p>为了证明VoxelNeXt的有效性，我们在nuScenes[3]、Waymo[45]、Argoverse2[52]数据集的三个大规模基准上评估了我们的模型。在这两个基准测试中，VoxelNeXt在3D物体检测方面都实现了高效的领先性能。它还产生了最先进的3D跟踪性能。在nuScenes跟踪测试分裂中，它在所有LIDAR-only条目中排名第一[3]。</p>
<h2 id="2-related-work-2">2. Related Work</h2>
<p>**雷达检测器：**3D探测器的工作原理通常与2D探测器相似，如R-CNN系列[12,34,41,54]和CenterPoint系列[14,57,60]。三维检测由于数据分布的稀疏性而区别于二维任务。但是许多方法[12,53,57,61]仍然寻求二维密集卷积头作为解决方案。</p>
<p>VoxelNet[61]使用PointNet[38]进行体素特征编码，然后使用密集区域建议网络和头进行预测。SECOND[53]通过使用密集的锚定头进行高效的稀疏卷积来改进VoxelNet。其他最先进的方法，包括PV-RCNN[41]、Voxel R-CNN[12]和VoTr[35]仍然保留了稀疏到密集的方案来扩大感受野。</p>
<p>在2D CenterNet[14]的激励下，CenterPoint[57]被应用于三维检测和跟踪。它将骨干网络的稀疏输出转换为地图-视图密集特征图，并基于密集特征预测物体中心位置的密集热图。这种基于密集中心的预测已被几种密集头方法所采用[30,33]。在本文中，我们采取了一个新的方向，并令人惊讶地证明了一个简单而强的稀疏CNN足以进行直接预测。值得注意的发现是，密集的head并不总是必要的。</p>
<p><strong>稀疏检测器：</strong>[16,46,47]的方法避免了密集的检测头，而是引入了其他复杂的管道。RSN[47]对距离图像进行前景分割，然后在剩余的稀疏数据上检测3D物体。SWFormer[46]提出了一种带有精细窗口分割和带有特征金字塔的多个头的稀疏变压器。在VoteNet[37]的激励下，FSD[16]利用点聚类和群校正来解决中心特征缺失问题。这些探测器进行稀疏的预测，但以不同的方式使检测管道复杂化。在我们的工作中，这个中心缺失问题也可以简单地跳过具有大接受域的稀疏网络。我们对常用的稀疏cnn做最小的调整来实现完全稀疏的检测器。</p>
<p>稀疏卷积网络：稀疏卷积网络因其高效性成为3D深度学习的主干网络[10,11,23,41]。人们普遍认为，它的表示能力在预测方面是有限的。为了解决这个问题，[12,41,49,53]的3D检测器依赖于密集卷积头进行特征增强。最近的方法[6,32]对稀疏cnn进行卷积修改。[21,35]的方法甚至用变压器代替大的感受野。与所有这些解决方案相反，我们证明可以通过额外的下采样层简单地解决接收场不足的瓶颈，而无需任何其他复杂的设计。</p>
<p>**3D对象跟踪：**基于多帧激光雷达的三维目标跟踪模型。以往的大多数方法[2,9,51]对检测结果直接使用卡尔曼滤波，如AB3DMOT[51]。在CenterTrack[60]之后，CenterPoint[57]预测了通过多帧关联对象中心的速度。本文引入查询体素进行关联，有效缓解了目标中心的预测偏差。</p>
<h2 id="3-fully-sparse-voxel-based-network">3. Fully Sparse Voxel-based Network</h2>
<p>点云或体素是不规则分布的，通常分散在3D物体的表面，而不是在中心或内部。这促使我们沿着一个新的方向研究，直接基于体素来预测3D盒子，而不是手工制作的锚点或中心。</p>
<p>为此，我们的目标是进行最小的修改，使普通的3D稀疏CNN网络适应直接体素预测。在下面，我们将介绍骨干自适应(第3.1节)，稀疏头部设计(第3.2节)，以及扩展到3D目标跟踪(第3.3节)。</p>
<h3 id="31-稀疏cnn主干自适应">3.1. 稀疏CNN主干自适应</h3>
<p>**额外的下采样：**为了确保对稀疏体素特征进行直接和正确的预测，必须具有足够的接受域的强特征表示。尽管朴素的稀疏CNN骨干网已被广泛应用于3D目标检测器中[12,41,57]，但最近的研究发现了它的弱点，并提出了各种方法来增强稀疏骨干网，如精心设计的卷积[7]、大核[8]和变压器[25,26,35]。</p>
<p>与所有这些方法不同，我们尽可能少地修改来实现这一点，只使用额外的下采样层。缺省情况下，朴素稀疏CNN骨干网有4个阶段，特征步长为{1,2,4,8}。我们将输出的稀疏特征分别命名为{F1, F2, F3, F4}。这种设置不能直接预测，特别是对于大型物体。为了增强其能力，我们简单地包括两个额外的下采样层，以获得{F5, F6}的步长为{16,32}的特征。</p>
<p>这个小小的变化直接对感受野的扩大产生了显著的影响。我们将最后三个阶段{F4, F5, F6}的稀疏特征组合到Fc。它们的空间分辨率都与F4对齐。对于阶段i, Fi是一组单独的特征fp。p∈Pi是三维空间中的一个位置，坐标为(xp, yp, zp)。该过程如图4所示。值得注意的是，这种简单的稀疏连接不需要其他参数化层。稀疏特征Fc及其位置Pc为：
$$
\begin{aligned}
&amp; F_c=F_4 \cup\left(F_5 \cup F_6\right), \<br>
&amp; P_6^{\prime}=\left{\left(x_p \times 2^2, y_p \times 2^2, z_p \times 2^2\right) \mid p \in P_6\right} \<br>
&amp; P_5^{\prime}=\left{\left(x_p \times 2^1, y_p \times 2^1, z_p \times 2^1\right) \mid p \in P_5\right} \<br>
&amp; P_c=P_4 \cup\left(P_5^{\prime} \cup P_6^{\prime}\right) .
\end{aligned}
$$
我们在图5中可视化了有效的感受野(erf)。有了额外的下采样层，erf更大，预测框更准确。它足够有效，并且花费很少的额外计算，如表2所示。因此，我们采用这种简单的设计作为骨干网。</p>
<p><strong>稀疏高度压缩：</strong>[12,41,57]的3D目标检测器通过将稀疏特征转换为密集特征，然后将深度(沿z轴)合并为通道维度，将3D体素特征压缩为密集的2D地图。这些操作需要占用内存和计算。</p>
<p>在VoxelNet中，我们发现二维稀疏特征对于预测是有效的。高度压缩在VoxelNeXt是完全稀疏的。我们简单地把所有体素放在地面上，并在相同的位置总结特征。它的花费不超过1毫秒。我们发现，利用压缩后的二维稀疏特征进行预测的成本要低于利用三维稀疏特征进行预测的成本，如表5所示。压缩后的稀疏特征¯Fc及其位置¯Pc为:</p>
<p>$$
\begin{aligned}
\bar{P}<em>c &amp; =\left{\left(x_p, y_p\right) \mid p \in P_c\right} \<br>
\bar{F}<em>c &amp; =\left{\sum</em>{p \in S</em>{\bar{p}}} f_p, \mid \bar{p} \in \bar{P}_c\right}
\end{aligned}
$$
其中Sp¯= {p | xp = xp¯，yp = yp¯，p∈Pc}，包含放置在相同2D位置p。</p>
<p>**空间体素修剪：**我们的网络完全基于体素。3D场景通常包含大量冗余的背景点，这些背景点对预测几乎没有好处。我们沿着下采样层逐渐修剪不相关的体素。根据SPS-Conv[32]，我们抑制了小特征量体素的扩张，如图6所示。将抑制比设为0.5，我们只对特征幅度|fp|(在通道维度上的平均值)排在所有体素的前一半的体素进行放大。体素修剪在不影响性能的情况下大大节省了计算量，如表3所示。</p>
<h3 id="32-稀疏预测头">3.2. 稀疏预测头</h3>
<p>**体素选择：**VoxelNeXt模型的详细框架如图4所示。我们不依赖于密集特征映射M，而是直接基于3D CNN骨干网络V∈R N×F的稀疏输出进行对象预测。我们首先预测K类的体素分数，s∈R N×K。在训练过程中，我们将最接近每个标注的边界框中心的体素分配为正样本。我们使用焦点丢失[31]进行监督。我们注意到，在推理查询过程中，体素通常不在对象中心。它们甚至不一定在边界框内，例如图9中行人的边界框。我们统计了nuScenes验证集上查询体素的分布，见表7。</p>
<p>在推理过程中，我们通过使用稀疏最大池化来避免NMS后处理，因为特征足够稀疏。类似于子流形稀疏卷积[19]，它只作用于非空位置。这是基于预测分数，并针对每个类单独进行的。我们采用稀疏最大池化来选择具有空间局部最大值的体素。移除的体素将被排除在框预测中，从而节省了头部的计算。</p>
<p>**框回归：**Bounding boxes直接从正的或选择的稀疏体素特征v∈Rn×F中回归。按照 CenterPoint [57] 中的协议，我们回归位置 (Δx, Δy) ∈ R 2 、高度 h ∈ R、3D 尺寸 s ∈ R 3 和旋转角度 (sin(α), cos(α)) ∈ R 2 。对于 nuScenes 数据集或跟踪，我们根据任务定义对速度 v ∈ R 2 进行回归。这些预测在训练过程中受到 L1 损失函数的监督。对于 Waymo 数据集，我们还预测 IoU 并使用 IoU 损失进行训练以提高性能 [22]。我们简单地使用全连接层或核大小为 3 的 3×3 子流形稀疏卷积层进行预测，没有其他复杂的设计。我们发现 3 × 3 稀疏卷积产生比全连接层更好的结果，负担有限，如表6.</p>
<h3 id="33-3d追踪">3.3. 3D追踪</h3>
<p>我们的框架很自然地扩展到 3D 跟踪。CenterPoint [57] 通过二维速度 v ∈ R 2 跟踪预测的对象中心，该速度也受 L1 损失的监督。我们将此设计扩展到 VoxelNeXt。我们的方法是使用体素关联来包含更多与查询体素位置匹配的轨迹。</p>
<p>如图 8 所示，我们记录了用于预测每个框的体素的位置。与中心关联类似，我们计算用于匹配的 L2 距离。查询位置是通过将它们的索引回溯到原始输入体素而不是 stride-8 位置来选择的。跟踪体素存在于输入数据中，其偏差小于预测中心。此外，相邻帧之间的查询体素与框共享相似的相对位置。我们凭经验表明，体素关联改善了 Tab 中的跟踪。 11.</p>
<h2 id="4-experiments-1">4. Experiments</h2>
<h3 id="41消融研究">4.1.消融研究</h3>
<p>**额外的下采样层：**我们消除了 VoxelNeXt 中下采样层的影响。我们将其扩展到变体 Ds。 s 表示下采样的次数。例如，D3 与基础模型具有相同的网络步幅（3 倍）。我们的修改不会改变检测头的分辨率。这些模型的结果显示在表2中。 没有密集的头部，D3 性能下降严重，尤其是在 Truck 和 Bus 的大型物体上。从D3到D5，性能逐渐提升。额外的下采样层补偿感受野。额外的下采样层补偿感受野。为了验证这一点，我们添加了一个变体，D 5×5×5 3 ，这将所有阶段的稀疏卷积的内核大小增加到 5 × 5 × 5。大内核在一定程度上提高了性能但降低了效率。因此，我们使用额外的下采样作为简单的解决方案。</p>
<p><strong>空间体素修剪：</strong> VoxelNeXt 根据特征大小逐渐删除冗余体素。我们在表3中取消了此设置。我们控制掉落比例从0.1到0.9，间隔0.2。当比率不大于 0.5 时，性能几乎不会衰减。因此，我们在实验中将下降率设置为 0.5 作为默认设置。我们还在表4中消融了体素修剪的阶段。我们默认在前3个阶段使用它。</p>
<p>**稀疏高度压缩：**我们在 2D 和 3D 的稀疏 CNN 类型上进行消融，在 VoxelNeXt 的骨干和头部，在表5中，naive 设计是backbone 和head 都应用了3D sparse CNN，导致高延迟。通过稀疏高度压缩，我们结合了 3D 骨干和 2D 稀疏预测头。它以良好的性能实现了更高的效率。我们将其用作 VoxelNeXt 的默认设置。当我们使用 2D 稀疏 CNN 作为骨干网络时，它具有与 3D 相同的层数和双通道。它实现了最佳效率，但性能有所下降。由于其高效率，我们将其命名为 VoxelNeXt-2D。</p>
<p>**稀疏预测头中的层类型：**我们使用全连接层或子流形稀疏卷积消除效果，以预测稀疏头中的框，如表6所示。全连接 (FC) 头的性能不如 3 × 3 稀疏卷积对应物，但效率更高。我们在 VoxelNeXt 中用 K3 表示后者。</p>
<p>**体素和预测框之间的相对位置：**在 VoxelNeXt 中，框预测的体素不需要在框内，更不用说像 表7中的中心了。我们计算它们生成的 3D 边界框内的体素的相关性。根据体素与框的相对位置，我们将体素分为近中心、近边界和外框 3 种区域类型。平均而言，大多数框是从内部的体素预测的，可能不在中心附近。统计上，只有少数框（总共不到 10%）是基于对象中心附近的体素预测的。这一发现表明边界体素也有资格进行预测，而对象中心并不总是必要的。</p>
<p>另一个观察结果是不同类别的比率之间存在很大差距。对于汽车和拖车，大多数框都是在内部体素上预测的。相比之下，对于卡车、交通锥和行人，大约一半的框是从外部体素预测的。我们在图 9 中说明了示例对。由于不同类别中的对象大小不同和空间稀疏性，预测体素符合数据分布，而不是像锚点或中心这样的代理。</p>
<p>**误差分析中与 CenterPoint 的比较：**我们将 VoxelNeXt 与表8中具有代表性的密集头方法 CenterPoint [57] 进行了比较。 在 1/4 nuScenes 训练集上训练并在完整的验证拆分上进行评估，VoxelNeXt 实现了 0.9% mAP 和 1.0% NDS 改进。在进一步分析中，CenterPoint 和 VoxelNeXt 在位置、大小和速度方面存在类似的错误。但是，在其他错误类型中存在较大差距，尤其是在定向方面。值得注意的是，VoxelNext 的方向误差比 CenterPoint 少 4.9%。我们假设这是由于稀疏的体素特征可能对方向差异更敏感。</p>
<p>**骨干网络效率统计：**我们在表9中计算了稀疏 CNN 主干网络的效率相关统计数据。由于将最后 3 个阶段的特征相加进行高度压缩，因此它们共享相同的通道号 128。由于第 5-6 阶段的高下采样率，它们的体素数与前几个阶段相比要小得多。因此，阶段 5-6 中引入的计算成本限制为 6 和 3 毫秒内的 6.1G 和 2.8G FLOP。它不超过整个骨干网的1/3，却对性能提升有显着影响。</p>
<p>**稀疏最大池化：**我们消除了表10中稀疏最大池化和 NMS 的影响。与常用的 NMS 相比，max-pool 的 mAP 相当，56.0% v.s.56.2%。 VoxelNeXt 可以灵活地与 NMS 或稀疏最大池化一起使用。 Max-pool 是一个优雅的解决方案，避免了一些不必要的预测计算。</p>
<p>**3D 跟踪体素关联：**表11 显示了 nuScenes 验证中 3D 跟踪的消融。除了跟踪预测框中心外，我们还包括预测匹配框的体素。体素关联引入了 1.1% AMOTA 的显着改进。</p>
<h3 id="42主要结果">4.2.主要结果</h3>
<p>**3D 对象检测：**表12中，我们在测试拆分上评估我们的检测模型，并将它们与 nuScenes 测试集上的其他基于 LIDAR 的方法进行比较。表示为 † [13,28,40] 的结果报告了双翻转测试增强 [57]。两条线的结果都比之前的要好。我们在 Tab 中的 Waymo 验证拆分上将 VoxelNeXt 与其他 3D 对象检测器进行了比较。 15 和表中的 Argoverse2 [52]。 16. 我们在 Tab 中展示了延迟比较。如图 12 和图 3 所示。VoxelNeXt 在这些方法中以高效率实现了领先的性能。</p>
<p>**3D 多目标跟踪：**表13 和选项卡。 14，我们将 VoxelNeXt 的跟踪性能与 nuScenes 测试和验证拆分中的其他方法进行了比较。 VoxelNeXt 在所有基于激光雷达的方法中实现了最好的 AMOTA。此外，结合表中的双翻转测试结果。 12，在表中表示为†。 13日，VoxelNeXt进一步达到71.0%的AMOTA，在nuScenes 3D激光雷达跟踪基准测试中排名第一。</p>
<h2 id="5-conclusion-and-discussion">5. Conclusion and Discussion</h2>
<p>在本文中，我们提出了一个用于 3D 对象检测和跟踪的完全稀疏和基于体素的框架。它采用简单的技术，运行速度快，没有太多额外成本，并且以优雅的方式工作，无需 NMS 后处理。我们首次表明基于<strong>直接体素的预测是可行且有效的</strong>。因此，基于规则的方案，例如锚点或中心，以及密集的头部在我们的方案中变得不必要。 VoxelNeXt 在包括 nuScenes [3]、Waymo [45] 和 Argoverse2 [52] 在内的大规模数据集上展示了有希望的结果。凭借高效率，它在 3D 对象检测方面取得了领先的性能，并在 nuScenes 3D 跟踪 LIDAR 基准测试中排名第一。</p>
<p>**局限性：**理论 FLOP 和实际推理速度之间存在差距。与 CenterPoint [57] 的 186.6G 相比，VoxelNeXt 的 FLOPs 小得多，只有 38.7G。实际延迟减少很明显，但没有表1中的 FLOPs 大，因为它高度依赖于实现和设备。</p></div><div class="post-footer" id="post-footer">
    <div class="post-info"><div class="post-info-line"><div class="post-info-mod">
                <span>更新于 2023-05-08</span>
            </div><div class="post-info-mod"><span>
                            <a class="link-to-markdown" href="/zh-cn/mot/index.md" target="_blank">阅读原始文档</a>
                        </span><span>
                        &nbsp;|&nbsp;
                        <a class="link-to-markdown" href="https://github.com/khusika/FeelIt/edit/main/exampleSite/content/posts%5cMOT%5cindex.zh-cn.md" target="_blank">Improve Article</a>
                    </span></div>
        </div><div class="post-info-share">
            <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://xiang-1208.github.io/zh-cn/mot/" data-title="MOT"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://xiang-1208.github.io/zh-cn/mot/"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://xiang-1208.github.io/zh-cn/mot/" data-title="MOT"><i class="fab fa-hacker-news fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://xiang-1208.github.io/zh-cn/mot/" data-title="MOT"><i class="fab fa-line fa-fw"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://xiang-1208.github.io/zh-cn/mot/" data-title="MOT"><i class="fab fa-weibo fa-fw"></i></a></span>
        </div></div><div class="post-nav"><a href="/zh-cn/erasor/" class="prev" rel="prev" title="ERASOR-译"><i class="fas fa-angle-left fa-fw"></i>Previous Post</a>
            <a href="/zh-cn/emoji-support/" class="next" rel="next" title="Emoji 支持">Next Post<i class="fas fa-angle-right fa-fw"></i></a></div></div>
</div><div id="comments" class="single-card"><div id="vssue"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://github.com/meteorlxy/vssue">Vssue</a>.
            </noscript></div></article></div>
            </main>
            <footer class="footer"><div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.88.1">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/khusika/FeelIt" target="_blank" rel="noopener noreffer" title="FeelIt 1.0.1"><i class="fas fa-hand-holding-heart fa-fw"></i> FeelIt</a>
        </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2021 - 2023</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://github.com/xiang-1208">xiang</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
</div>
<script>
if ('serviceWorker' in navigator) {
    navigator.serviceWorker
        .register('/sw.min.js?version=1.0.1', { scope: '/' })
        .then(() => {
            console.info('Xiang\u0027s Blog\u00A0Service Worker Registered');
        }, err => console.error('Xiang\u0027s Blog\u00A0Service Worker registration failed: ', err));

    navigator.serviceWorker
        .ready
        .then(() => {
            console.info('Xiang\u0027s Blog\u00A0Service Worker Ready');
        });
}
</script>
</footer>
        </div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-chevron-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment-alt fa-fw"></i>
            </a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/vssue@1.4.8/dist/vssue.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/copy-tex.min.css"><script src="https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.runtime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vssue@1.4.8/dist/vssue.github.min.js"></script><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.0/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.10.3/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.1/sharer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/copy-tex.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/mhchem.min.js"></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{"vssue":{"clientID":"292ffebe5a600878b1dd","clientSecret":"4687b21cb4e24dac3ba567603b471e161fa90109","owner":"xiang-1208","repo":"xiang-1208.github.io","title":"MOT"}},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"654CMGADD6","algoliaIndex":"index.zh-cn","algoliaSearchKey":"d37e5f35ed35ec6a7ddc94bb8e7a727e","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"}};</script><script src="/js/theme.min.js"></script></body></html>
